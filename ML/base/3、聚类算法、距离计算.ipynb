{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>目录<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#聚类\" data-toc-modified-id=\"聚类-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>聚类</a></span></li><li><span><a href=\"#性能度量\" data-toc-modified-id=\"性能度量-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>性能度量</a></span><ul class=\"toc-item\"><li><span><a href=\"#示例\" data-toc-modified-id=\"示例-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>示例</a></span></li><li><span><a href=\"#外部度量指标\" data-toc-modified-id=\"外部度量指标-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>外部度量指标</a></span><ul class=\"toc-item\"><li><span><a href=\"#Jaccard-系数\" data-toc-modified-id=\"Jaccard-系数-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>Jaccard 系数</a></span></li><li><span><a href=\"#FM指数\" data-toc-modified-id=\"FM指数-2.2.2\"><span class=\"toc-item-num\">2.2.2&nbsp;&nbsp;</span>FM指数</a></span></li><li><span><a href=\"#Rand--指数\" data-toc-modified-id=\"Rand--指数-2.2.3\"><span class=\"toc-item-num\">2.2.3&nbsp;&nbsp;</span>Rand  指数</a></span></li></ul></li><li><span><a href=\"#内部度量指标\" data-toc-modified-id=\"内部度量指标-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>内部度量指标</a></span><ul class=\"toc-item\"><li><span><a href=\"#DB-指数\" data-toc-modified-id=\"DB-指数-2.3.1\"><span class=\"toc-item-num\">2.3.1&nbsp;&nbsp;</span>DB 指数</a></span></li></ul></li><li><span><a href=\"#Dunn-指数\" data-toc-modified-id=\"Dunn-指数-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Dunn 指数</a></span></li></ul></li><li><span><a href=\"#距离计算\" data-toc-modified-id=\"距离计算-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>距离计算</a></span><ul class=\"toc-item\"><li><span><a href=\"#闵可夫斯基距离\" data-toc-modified-id=\"闵可夫斯基距离-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>闵可夫斯基距离</a></span></li><li><span><a href=\"#欧氏距离\" data-toc-modified-id=\"欧氏距离-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>欧氏距离</a></span></li><li><span><a href=\"#曼哈顿距离\" data-toc-modified-id=\"曼哈顿距离-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>曼哈顿距离</a></span></li><li><span><a href=\"#三种距离比较\" data-toc-modified-id=\"三种距离比较-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>三种距离比较</a></span></li><li><span><a href=\"#距离计算的优缺点\" data-toc-modified-id=\"距离计算的优缺点-3.5\"><span class=\"toc-item-num\">3.5&nbsp;&nbsp;</span>距离计算的优缺点</a></span></li><li><span><a href=\"#离散与连续属性\" data-toc-modified-id=\"离散与连续属性-3.6\"><span class=\"toc-item-num\">3.6&nbsp;&nbsp;</span>离散与连续属性</a></span><ul class=\"toc-item\"><li><span><a href=\"#离散距离衡量-VDM\" data-toc-modified-id=\"离散距离衡量-VDM-3.6.1\"><span class=\"toc-item-num\">3.6.1&nbsp;&nbsp;</span>离散距离衡量-VDM</a></span></li><li><span><a href=\"#VDM示例\" data-toc-modified-id=\"VDM示例-3.6.2\"><span class=\"toc-item-num\">3.6.2&nbsp;&nbsp;</span>VDM示例</a></span></li><li><span><a href=\"#混合处理\" data-toc-modified-id=\"混合处理-3.6.3\"><span class=\"toc-item-num\">3.6.3&nbsp;&nbsp;</span>混合处理</a></span></li><li><span><a href=\"#非度量距离\" data-toc-modified-id=\"非度量距离-3.6.4\"><span class=\"toc-item-num\">3.6.4&nbsp;&nbsp;</span>非度量距离</a></span></li></ul></li></ul></li><li><span><a href=\"#原型聚类\" data-toc-modified-id=\"原型聚类-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>原型聚类</a></span><ul class=\"toc-item\"><li><span><a href=\"#k-means-算法\" data-toc-modified-id=\"k-means-算法-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>k-means 算法</a></span></li><li><span><a href=\"#算法流程\" data-toc-modified-id=\"算法流程-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>算法流程</a></span></li><li><span><a href=\"#k-means-示例\" data-toc-modified-id=\"k-means-示例-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>k-means 示例</a></span></li><li><span><a href=\"#k-means-的优缺点\" data-toc-modified-id=\"k-means-的优缺点-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>k-means 的优缺点</a></span></li></ul></li><li><span><a href=\"#K-means改进\" data-toc-modified-id=\"K-means改进-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>K-means改进</a></span><ul class=\"toc-item\"><li><span><a href=\"#k-means-++\" data-toc-modified-id=\"k-means-++-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>k-means ++</a></span></li><li><span><a href=\"#ISODATA算法\" data-toc-modified-id=\"ISODATA算法-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>ISODATA算法</a></span></li><li><span><a href=\"#MiniBatch\" data-toc-modified-id=\"MiniBatch-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>MiniBatch</a></span></li></ul></li><li><span><a href=\"#其他聚类方法\" data-toc-modified-id=\"其他聚类方法-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>其他聚类方法</a></span><ul class=\"toc-item\"><li><span><a href=\"#k-medoids\" data-toc-modified-id=\"k-medoids-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>k-medoids</a></span></li><li><span><a href=\"#密度聚类法\" data-toc-modified-id=\"密度聚类法-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>密度聚类法</a></span></li><li><span><a href=\"#层次聚类\" data-toc-modified-id=\"层次聚类-6.3\"><span class=\"toc-item-num\">6.3&nbsp;&nbsp;</span>层次聚类</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 聚类\n",
    "**概念：无监督学习中（unsupervised learning）、聚类（clustering）**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在**“无监督学习（unsupervised learning）”** 中，训练样本的标记信息是未知的，目标是通过对无标记的样本的学习来揭示数据的内在性质及其规律，为进一步的数据分析做准备。此类任务中研究使用最多的就是**聚类（clustering）**。\n",
    "\n",
    "聚类试图通过将数据集中的样本划分为若干个不相交的子集，每个子集成为一个**簇（cluster）**。通过这样划分，每个簇可能对应一些潜在的**概念（类别）**，如浅色瓜、深色瓜、无子瓜等。但是，这概念对于聚类算法而言事先是未知的，聚类过程仅能自动形成簇结构，簇所对应的概念语义需要使用者来把握和命名。\n",
    "\n",
    "聚类既能作为一个单独的过程，用于寻找数据的内部结构，也可以为分类等其他学习任务的前驱过程。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 性能度量\n",
    "**概念：簇内相似度（intra-cluster similarity）、簇间相似度（inter-cluster similarity）、参考模型（reference model）、外部指标（external index）、内部指标（internal index）**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "聚类的性能度量亦称为**有效性指标（validity index）**，即对聚类结果的好坏的衡量。\n",
    "\n",
    "聚类是将样本集 D 划分为若干个互不相交的子集，即样本簇。直观上看，我们希望“物以类聚”，即同一簇的样本极可能的彼此相似，不同的簇的样本尽可能的不同。换言之就是，**簇内相似度（intra-cluster similarity）**高，且**簇间相似度（inter-cluster similarity）**低。\n",
    "\n",
    "聚类性能度量大致有两类\n",
    "- 一类将聚类结果与某个**参考模型（reference model）**进行比较，称为**外部指标（external index）**\n",
    "- 另一类是直接考察聚类结果而不利用任何参考模型，称为**内部指标（internal index）**\n",
    "\n",
    "对于数据集$D = \\{\\boldsymbol{x}_1,...,\\boldsymbol{x}_m\\}$，假定通过聚类给出的簇划分为$C=\\{C_1,...C_k\\}$，参考模型划分结果为$C^*=\\{C_1^*,...C_k^*\\}$。相应的，令$\\lambda$与$\\lambda^*$分别表示与 C 与$C^*$对应的簇标记向量。将样本两两配对考虑，定义：\n",
    "$$\n",
    "\\large{\n",
    "\\begin{split}\n",
    "a &= |SS|,\\; SS=\\{(\\boldsymbol{x}_i,\\boldsymbol{x}_j)|\\; \\lambda_i=\\lambda_j, \\lambda_i^*=\\lambda_j^*,i<j\\} \\qquad(1) \\\\\n",
    "b &= |SD|,\\; SD=\\{(\\boldsymbol{x}_i,\\boldsymbol{x}_j)|\\; \\lambda_i=\\lambda_j, \\lambda_i^* \\neq \\lambda_j^*,i<j\\}\\qquad(2) \\\\\n",
    "c &= |DS|,\\; DS=\\{(\\boldsymbol{x}_i,\\boldsymbol{x}_j)|\\; \\lambda_i \\neq \\lambda_j, \\lambda_i^*=\\lambda_j^*,i<j\\}\\qquad(3) \\\\\n",
    "d &= |DD|,\\; DD=\\{(\\boldsymbol{x}_i,\\boldsymbol{x}_j)|\\; \\lambda_i \\neq \\lambda_j, \\lambda_i^* \\neq \\lambda_j^*,i<j\\}\\qquad(4) \\\\\n",
    "\\end{split}\n",
    "}\n",
    "$$\n",
    "\n",
    "其中集合 SS 表示包含了 C 中隶属于相同簇，且在 $C^*$ 中也隶属于相同簇的样本对，集合 SD 包含了在 C 中属于相同的簇但在 $C^*$ 中属于不同的簇的样本对，由于对样本对$(\\boldsymbol{x}_i,\\boldsymbol{x}_j)(i<j)$仅能出现在一个集合中，因此有 a+b+c+d=m(m-1)/2。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 示例\n",
    "例如，样本集合$D=\\{d_1,d_2,d_3,d_4,d_5,d_6\\}$，通过聚类划分为两个簇其中：\n",
    "- $C_1=\\{d_1,d_3,d_5\\}$\n",
    "- $C_2=\\{d_2,d_4,d_6\\}$\n",
    "\n",
    "参考模型划分是三个类别：\n",
    "- $C_1^* =\\{d_1,d_2\\}$\n",
    "- $C_2^* =\\{d_4,d_6\\}$\n",
    "- $C_3^* =\\{d_3,d_5\\}$\n",
    "\n",
    "可以知道 m=6，那么一共有$(\\boldsymbol{x}_i,\\boldsymbol{x}_j)(i<j)$ 15个，分别为:$P=\\{(d_1,d_2),(d_1,d_3),(d_1,d_4),(d_1,d_5),(d_1,d_6),(d_2,d_3),\\\\\\quad(d_2,d_4),(d_2,d_5),(d_2,d_6),(d_3,d_4),(d_3,d_5),(d_3,d_6),(d_4,d_5),(d_4,d_6),(d_5,d_6)\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到 $a=2，SS=\\{(d_3,d_5),(d_4,d_6)\\}$，即$(d_3,d_5)$同时在$C_1$中，在参考模型划分中也同在$C_3^*$中；$(d_4,d_6)$亦是。同理可知：\n",
    "- $b=4，SD=\\{(d_1,d_3),(d_1,d_5),(d_2,d_4),(d_2,d_6)\\}$\n",
    "- $c=1，DS=\\{(d_1,d_2)\\}$\n",
    "- $d=8，DD=\\{(d_1,d_4),(d_1,d_6),(d_2,d_3),(d_2,d_5),(d_3,d_4),(d_3,d_6),(d_4,d_5),(d_5,d_6)\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 外部度量指标\n",
    "【参考】\n",
    "- [sklearn - Clustering performance evaluation](https://scikit-learn.org/stable/modules/clustering.html#clustering-performance-evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jaccard 系数\n",
    "$$\n",
    "\\large{\n",
    "\\begin{split}\n",
    "\\text{JC} = \\frac{a}{a+b+c}\n",
    "\\end{split}\n",
    "}\\tag{A1}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FM指数\n",
    "- [Fowlkes-Mallows scores](https://scikit-learn.org/stable/modules/clustering.html#fowlkes-mallows-scores)\n",
    "\n",
    "FM 指数即 Fowlkes and Mallows Index，简称 FMI\n",
    "$$\n",
    "\\large{\n",
    "\\begin{split}\n",
    "\\text{FMI} = \\sqrt{\\frac{a}{a+b}\\cdot \\frac{a}{a+c} } \\;或\\; \\text{FMI} = \\frac{\\text{TP}}{\\sqrt{(\\text{TP} + \\text{FP}) (\\text{TP} + \\text{FN})}}\n",
    "\\end{split}\n",
    "}\\tag{A2}\n",
    "$$\n",
    "\n",
    "分数的值在 0到1 之间，值越高表明两个聚类之间越相似。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rand  指数\n",
    "Rand 指数，即 Rand Index 简称 RI\n",
    "$$\n",
    "\\large{\n",
    "\\begin{split}\n",
    "\\text{RI} = \\frac{2(a+d)}{m(m-1)} \\;或\\; \\text{RI} = \\frac{a + d}{C_2^{m}}\n",
    "\\end{split}\n",
    "}\\tag{A3}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上述性能指标的值都在[0,1]之间，显然值越大说明聚类的效果越好。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 内部度量指标"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "考虑聚类划分的结果为：$C=\\{C_1,..,C_k\\}$，令 $|C_i|$ 表示第 i 个簇中的样本数，$dist(\\cdot,\\cdot)$表示两个样本之间的距离，$\\mu_i$表示第 i 个簇的中心点$\\mu=\\frac{1}{|C_i|}\\sum_{1\\le i \\le |C_i|}\\boldsymbol{x}_i$。定义：\n",
    "\n",
    "- 簇内平均距离\n",
    "$$\n",
    "\\large{\n",
    "\\begin{split}\n",
    "avg(C_i) &= \\frac{2}{|C_i|(|C_i|-1)}\\sum_{1\\le i \\lt j \\le |C_i|} dist(\\boldsymbol{x}_i,\\boldsymbol{x}_j)\n",
    "\\end{split}\n",
    "}\\tag{B1}\n",
    "$$\n",
    "\n",
    "- 簇内样本间的最大距离\n",
    "$$\n",
    "\\large{\n",
    "\\begin{split}\n",
    "diam(C_i) &= \\max_{1\\le i \\lt j \\le |C_i|}dist(\\boldsymbol{x}_i,\\boldsymbol{x}_j)\n",
    "\\end{split}\n",
    "}\\tag{B2}\n",
    "$$\n",
    "\n",
    "- 簇间样本点的最近距离\n",
    "$$\n",
    "\\large{\n",
    "\\begin{split}\n",
    "d_{min}(C_i,C_j) =  \\min_{\\boldsymbol{x}_i \\in C_i,\\boldsymbol{x}_j \\in C_j}dist(\\boldsymbol{x}_i,\\boldsymbol{x}_j)\n",
    "\\end{split}\n",
    "}\\tag{B3}\n",
    "$$\n",
    "\n",
    "- 簇间中心点的距离\n",
    "$$\n",
    "\\large{\n",
    "\\begin{split}\n",
    "d_{cen}(C_i,C_j) = dist(\\boldsymbol{\\mu}_i,\\boldsymbol{\\mu}_j)\n",
    "\\end{split}\n",
    "}\\tag{B4}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如下图所示：\n",
    "![image](https://ws4.sinaimg.cn/large/69d4185bly1fwp77ldeplj20bk09dgm1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DB 指数\n",
    "DB 指数，即 Davies-Bouldin Index，简称 DBI\n",
    "$$\n",
    "\\large{\n",
    "\\begin{split}\n",
    "\\text{DBI} = \\frac{1}{k}\\sum_{i=1}^{k}\\max_{j \\neq i}\\left(\\frac{avg(C_i) + avg(C_j)}{d_{cen}(\\boldsymbol{\\mu}_i,\\boldsymbol{\\mu}_j)}\\right)\n",
    "\\end{split}\n",
    "}\\tag{B5}\n",
    "$$\n",
    "\n",
    "可以看到 DB I数越小越好。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dunn 指数\n",
    "即 Dunn Index ，简称 DI\n",
    "$$\n",
    "\\large{\n",
    "\\begin{split}\n",
    "\\text{DI} = \\min_{1\\le i \\le k}\\left\\{\\min_{j\\neq i}\\left(\\frac{d_{min}(C_i,C_j)}{\\,max_{1 \\le l \\le k }diam(C_l)}\\right)\\right\\}\n",
    "\\end{split}\n",
    "}\\tag{B6}\n",
    "$$\n",
    "\n",
    "可以看到 DI 越大越好。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 距离计算\n",
    "**概念：距离度量（distance measure）、**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于函数dist(·,·)，若他是一个**距离度量（distance measure）**，则满足一些基本的性质：\n",
    "- 非负性：$dist(\\boldsymbol{x}_i,\\boldsymbol{x}_j) \\ge 0$;\n",
    "- 同一性：$dist(\\boldsymbol{x}_i,\\boldsymbol{x}_j) = 0$，当仅当 $\\boldsymbol{x}_i=\\boldsymbol{x}_j$\n",
    "- 对称性：$dist(\\boldsymbol{x}_i,\\boldsymbol{x}_j) = dist(\\boldsymbol{x}_j,\\boldsymbol{x}_i)$\n",
    "- 直递性：$dist(\\boldsymbol{x}_i,\\boldsymbol{x}_j) \\le dist(\\boldsymbol{x}_i,\\boldsymbol{x}_k) + dist(\\boldsymbol{x}_k,\\boldsymbol{x}_j)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 闵可夫斯基距离"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "给定样本 $\\boldsymbol{x}_i = (x_{i1},x_{i2},..,x_{in})$ 与 $\\boldsymbol{x}_j = (x_{j1},x_{j2},..,x_{jn})$，两个样本的闵可夫斯基距离定义为：\n",
    "$$\n",
    "\\large{\n",
    "\\begin{split}\n",
    "dist_{mk}(\\boldsymbol{x}_i, \\boldsymbol{x}_j) = \\left( \\sum_{u=1}^{n}\\lvert x_{iu} - x_{ju}\\rvert^{p} \\right)^{\\frac{1}{p}}\n",
    "\\end{split}\n",
    "}\\tag{C1}\n",
    "$$\n",
    "\n",
    "可以看到闵可夫斯基距离满足上面的距离度量的基本性质。\n",
    "\n",
    "当 p 趋向于 ∞  时就是切比雪夫距离。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 欧氏距离"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当 p=2 时，闵可夫斯基距离就是**欧氏距离（Euclidean distance）**：\n",
    "$$\n",
    "\\large{\n",
    "\\begin{split}\n",
    "dist_{ed}(\\boldsymbol{x}_i, \\boldsymbol{x}_j) = \\Arrowvert \\boldsymbol{x}_i - \\boldsymbol{x}_j \\Arrowvert_{2} = \\sqrt{\\sum_{u=1}^{n}\\lvert x_{iu} - x_{ju}\\rvert^{2}}\n",
    "\\end{split}\n",
    "}\\tag{C2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 曼哈顿距离"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当 p=1 时闵可夫斯基距离就是**曼哈顿距离（Manhattan distance）**：\n",
    "$$\n",
    "\\large{\n",
    "\\begin{split}\n",
    "dist_{man}(\\boldsymbol{x}_i, \\boldsymbol{x}_j) = \\Arrowvert \\boldsymbol{x}_i - \\boldsymbol{x}_j \\Arrowvert_{1} = \\sum_{u=1}^{n}\\lvert x_{iu} - x_{ju}\\rvert\n",
    "\\end{split}\n",
    "}\\tag{C3}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三种距离比较\n",
    "\n",
    "【参考】\n",
    "- [csdn - 最小距离分类法介绍](https://blog.csdn.net/liangdas/article/details/17039583)\n",
    "- [coolshell - K-MEANS 算法](https://coolshell.cn/articles/7779.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了更一步理解这些距离的差别，我们做下面一个实验：选取一个点作为中心点，用上述的三个距离公式分别计算周围的每一个点到这个中心点的距离，将距离相同的点用同一种颜色表示，那么得到下面的三个图：\n",
    "![image](https://wx4.sinaimg.cn/large/69d4185bly1fwq5lnho8kj20cy052dgm.jpg)\n",
    "\n",
    "从三个图中可以看出，三个不同的距离公式，对中心的逼近方式也不一样：\n",
    "- 欧式距离是以同心圆的方式向中心靠近；\n",
    "- 曼哈顿距离是以倾斜45度角的正方形的方式向中心靠近；\n",
    "- 闵可夫斯基距离(p=10)则是以四个角度光滑的四边形的方式向中心靠近。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们继续做下面一个分类实验：在维数为2维的平面空间中（只有横轴方向x和纵轴方向y两维空间），有一块128*128的区域，在这个区域中有已知的A(110, 18)，B(30, 18)，C(110, 88)，D(90, 58)，E(25, 123)五个点，以这五个点为5个类别的中心，我们剩下要做的事:将这个128*128的区域里面的所有的点，用上面的三个不同的距离计算方法，对它们进行最小距离分类，最后得到的分类结结果图如下：\n",
    "![image](https://wx1.sinaimg.cn/large/69d4185bly1fwq5s9xd5hj20ir06j74r.jpg)\n",
    "\n",
    "图中，每一个类别中心在图中以字母标示出了，不同的颜色表示的是不同的类别，同一种类别在一起组成一个多边形，多边形内的点是到其类别中心距离最近的点，多边形边界上的点是临界点，它们到相邻的两个类别中心的距离相等。第一张图中的多边形其实就是我们通常所说的泰森多边形。从三个图中可以看出来，不同距离算法，类别中心对周围点的作用域是不相同的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 距离计算的优缺点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最小距离分类法原理简单，容易理解，计算速度快，但是因为其只考虑每一类样本的均值，而不用管类别内部的方差（每一类样本的分布），也不用考虑类别之间的协方差（类别和类别之间的相关关系），所以分类精度不高，因此，一般不用它作为我们分类对精度有高要求的分类，但它可以在快速浏览分类概况中使用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 离散与连续属性\n",
    "**概念：有序属性（ordinal attribute）、无序属性（non-ordinal attribute）、VDM（Value Difference Metric）**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们常常将属性划分为**连续属性（continues attribute）**和**离散属性（categorical attribute）**。前者在定义域上有无穷多个可能的取值，后者在定义域上是有限个取值。  \n",
    "然而，讨论距离计算的时候，属性上是否定义了**序**关系更为重要。\n",
    "\n",
    "例如定义域为{1，2，3}的离散属性和连续属性的性质更为接近一些，能直接在属性上计算距离：1与2更接近，与3比较远，这样的属性称为**有序属性（ordinal attribute）**\n",
    "\n",
    "定义域为{飞机，火车，轮船}这样的离散属性不能直接计算距离，称为**无序属性（non-ordinal attribute）**。\n",
    "\n",
    "显然闵可夫斯基距离可用于计算有序属性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 离散距离衡量-VDM\n",
    "对于无序属性可以使用 **VDM（Value Difference Metric）**。令 $m_{u,a}$ 表示在属性 u 上取值为 a 的样本的个数，$m_{u,a,i}$表示在第 i 个样本簇中在属性 u 上取值为 a 的样本数，k 为样本簇数，则属性 u 上两个离散值 a 与 b 之间的 VDM 距离为：\n",
    "$$\n",
    "\\large{\n",
    "\\begin{split}\n",
    "VDM_{p}(a,b) = \\sum_{i=1}^{k}\\left| \\frac{m_{u,a,i}}{m_{u,a}} - \\frac{m_{u,b,i}}{m_{u,b}}\\right|^p\n",
    "\\end{split}\n",
    "}\\tag{C4}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VDM示例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以西瓜数据集为例：\n",
    "![img](http://wx3.sinaimg.cn/large/69d4185bly1fwfxkfhcjej20k70ajgo4.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设数据集被分为了两簇$C_1=\\{1,2,3,4,5,6,7,8\\}$和$C_1=\\{9,10,11,12,13,14,15,16，17\\}$，可知 k=2。以属性色泽为例，并计算青绿与浅白两个属性之间的距离，那么就有：\n",
    "- $m_{色泽,青绿} = \\{1,4,6,10,13,17\\} = 6,\\; m_{色泽,浅白} = \\{5,11,12,14,16\\} = 5$\n",
    "- $m_{色泽,青绿,1} = \\{1,4,6\\} = 3,\\; m_{色泽,浅白,1} = \\{5\\} = 1$\n",
    "- $m_{色泽,青绿,1} = \\{10,13,17\\} = 3,\\; m_{色泽,浅白,2} = \\{11,12,14,16\\} = 4$\n",
    "\n",
    "$$\n",
    "\\large{\n",
    "\\begin{split}\n",
    "VDM_{p}(青绿,浅白) = \\left| \\frac{3}{6} - \\frac{1}{5}\\right|^p + \\left| \\frac{3}{6} - \\frac{4}{5}\\right|^p\n",
    "\\end{split}\n",
    "}\n",
    "$$\n",
    "\n",
    "可以看到相同的属性的 VDM 是为 0 的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 混合处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将闵可夫斯基距离与 VDM 结合就可以处理混合属性。假定有$n_c$个有序属性，$n-n_c$个无序属性，不是一般性，令有序属性排列在无序属性之前，那么有：\n",
    "$$\n",
    "\\large{\n",
    "\\begin{split}\n",
    "MinkovDM_{p}(\\boldsymbol{x}_i,\\boldsymbol{x}_j) = \\left(\\sum_{u=1}^{n_c}\\left|x_{iu} - x_{ju}\\right|^{p} + \\sum_{u=n_c + 1}^{n}VDM_{p}(x_{iu},x_{ju})\\right)^{\\frac{1}{p}}\n",
    "\\end{split}\n",
    "}\\tag{C5}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "还以上面的西瓜数据集为例，计算样本编号为1和5的距离，有序属性为：密度和含糖率，无序属性为：色泽、根蒂、敲声、纹理、脐部和触感。有序属性按照闵可夫斯基距离计算，VMD 距离计算可参考 VDM 示例，那么就有：\n",
    "$$\n",
    "\\large{\n",
    "\\begin{split}\n",
    "MinkovDM_{p}(\\boldsymbol{x}_1,\\boldsymbol{x}_5) \n",
    "= \\left(\\left|0.697 - 0.556\\right|^{p} + \\left|0.460 - 0.215\\right|^{p} \\\\ \n",
    " + VDM_{p}(青绿,浅白) + VDM_{p}(蜷缩,蜷缩) + VDM_{p}(浊响,浊响) \\\\ \n",
    " + VDM_{p}(清晰,清晰) + VDM_{p}(凹陷,凹陷) + VDM_{p}(硬滑,硬滑)\\right)^{\\frac{1}{p}}\n",
    "\\end{split}\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当样本空间的不同属性重要性不同时，可使用**加权距离（weighted distance）**，以闵可夫斯基距离为例：\n",
    "$$\n",
    "\\large{\n",
    "\\begin{split}\n",
    "dist_{wmk}(\\boldsymbol{x}_i, \\boldsymbol{x}_j) = \\left(\\omega_1\\cdot\\lvert x_{i1} - x_{j1}\\rvert^p + ... + \\omega_n\\cdot\\lvert x_{in} - x_{jn}\\rvert^p\\right)^{\\frac{1}{p}}\n",
    "\\end{split}\n",
    "}\\tag{C6}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 非度量距离\n",
    "**概念：相似度度量（similarity measure）、**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通常我们是基于某种形式的距离定义**相似度度量（similarity measure）**，距离越大，相似度越小。然而用于相似度度量的距离未必一定要满足距离度量的所有性质，尤其是**直递性**。\n",
    "\n",
    "例如在某些任务中我们可能希望有这样的度量：“人“、\"马\"分别与\"人马\"相似，但“人”与“马”很不相似；要达到这个目的，可以令“人”“马”与“人马”之间的距离较小，但人与马这件的距离很大。此时距离不满足直递性，这样的距离称为**非度量距离（non-metric distance）**。我们上面的距离计算都是事先定义好的，但在不少现实任务中，有必要基于数据来确定合适的距离计算式，这可通过**距离度量学习（distance metric learning）**来实现。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 原型聚类\n",
    "**概念：**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "原型聚类亦称为**基于原型的聚类（prototype-based clustering）**，**原型是指样本空间中具有代表性的点**，此类算法那假设聚类结构能够通过一组原型进行刻画，在现实聚类任务中极为常用。\n",
    "\n",
    "通常情况下，算法先对原型进行初始化操作，然后对原型进行迭代更新求解，采用不同的原型表示，不同的求解方式，将产生不同的算法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-means 算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "给定样本集$D=\\{\\boldsymbol{x}_1,...,\\boldsymbol{x}_m\\}$，k-means 算法针对聚类所得簇划分$C=\\{C_1,...,C_k\\}$最小化平法误差：\n",
    "$$\n",
    "\\large{\n",
    "\\begin{split}\n",
    "E &= \\sum_{i=1}^{k}\\sum_{\\boldsymbol{x} \\in C_i}\\Arrowvert\\boldsymbol{x} - \\boldsymbol{\\mu}_i\\Arrowvert_2^2 \\\\\n",
    "其中，\\;\\boldsymbol{\\mu}_i &= \\frac{1}{|C_i|}\\sum_{\\boldsymbol{x} \\in C_i}\\boldsymbol{x} \\;是 C_i 的均值向量\n",
    "\\end{split}\n",
    "}\\tag{D1}\n",
    "$$\n",
    "\n",
    "直观上看上式刻画了簇内样本围绕均值向量的紧密程度，E 值越小则簇内样本相似度越高。\n",
    "\n",
    "但上式的最优解并不是特别容易找到，因此通常采用**贪心策略**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 算法流程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**输入：**样本集$D=\\{\\boldsymbol{x}_1,...,\\boldsymbol{x}_m\\}$  \n",
    "&emsp;&emsp;&emsp;聚类簇数 k\n",
    "\n",
    "**过程:**  \n",
    "从 D 中随机选择 k 个样本作为初始均值向量 $\\{\\boldsymbol{\\mu}_1,...,\\boldsymbol{\\mu}_k\\}$  \n",
    "\n",
    "**repeat**  \n",
    "&emsp;&emsp; 令 $C_i = \\phi \\;(1 \\le i \\le k)$  \n",
    "&emsp;&emsp; **for j=1,2...,m do**  \n",
    "&emsp;&emsp;&emsp;&emsp; 计算样本 $\\boldsymbol{x}_j$ 与各个均值向量$\\boldsymbol{\\mu}_i$的距离；$d_{ji}=\\Arrowvert\\boldsymbol{x}_j - \\boldsymbol{\\mu}_i\\Arrowvert_2$;  \n",
    "&emsp;&emsp;&emsp;&emsp; 根据距离最近的均值向量确定 $\\boldsymbol{x}_j$ 的簇标记：$\\lambda_j = argmin_{i \\in \\{1,2,...,k\\}}d_{ji}$;  \n",
    "&emsp;&emsp;&emsp;&emsp; 将样本 $\\boldsymbol{x}_j$ 划入相应的簇：$C_{\\lambda_j} = C_{\\lambda_j} \\bigcup \\{\\boldsymbol{x}_j\\}$  \n",
    "&emsp;&emsp; **end for**  \n",
    "&emsp;&emsp; **for i=1,2...,k do**  \n",
    "&emsp;&emsp;&emsp;&emsp; 计算新的均值向量：$\\boldsymbol{\\mu}_i{'} = \\frac{1}{|C_i|}\\sum_{\\boldsymbol{x} \\in C_i}\\boldsymbol{x}$;  \n",
    "&emsp;&emsp;&emsp;&emsp; **if $\\boldsymbol{\\mu}_i{'} \\neq \\boldsymbol{\\mu}_i$**  \n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; 将当前均值向量$\\boldsymbol{\\mu}_i$新为$\\boldsymbol{\\mu}_i{'}$  \n",
    "&emsp;&emsp;&emsp;&emsp; **else**  \n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; 保持当前均值向量不变  \n",
    "&emsp;&emsp;&emsp;&emsp; **end if**  \n",
    "&emsp;&emsp; **end for** \n",
    "**until** 当前均值向量均为更新  \n",
    "\n",
    "**输出：**簇划分$C=\\{C_1,...,C_k\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-means 示例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下列西瓜数据集为例：\n",
    "![image](https://wx3.sinaimg.cn/large/69d4185bly1fwq2u7lks0j20pb06fmyo.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假定 k=3 ，算法开始随机选择三个样本$\\pmb{x}_6,\\pmb{x}_{12},\\pmb{x}_{24}$ 作为初始均值向量：\n",
    "$$\n",
    "\\large{\n",
    "\\begin{split}\n",
    "\\pmb{\\mu}_1 = (0.403, 0.237),\\pmb{\\mu}_2 = (0.343,0.099),\\pmb{\\mu}_3=(0.478,0.437)\n",
    "\\end{split}\n",
    "}\n",
    "$$\n",
    "\n",
    "考察样本 $\\pmb{x}_1=(0.697;0.460)$，他与当前均值向量$\\pmb{\\mu}_1, \\pmb{\\mu}_2, \\pmb{\\mu}_3$的距离分别为0.369，0.506，0.220，因此将$\\pmb{x}_1$ 划入到簇$C_3$中，类似的，对数据集中的所有样本考察一遍后，可以得到当前簇的划分：\n",
    "$$\n",
    "\\large{\n",
    "\\begin{split}\n",
    "C_1 &= \\{\\pmb{x}_{ 3}, \\pmb{x}_{ 5}, \\pmb{x}_{ 6}, \\pmb{x}_{ 7}, \\pmb{x}_{ 8}, \\pmb{x}_{ 9}, \\pmb{x}_{10}, \\pmb{x}_{13}, \\pmb{x}_{14}, \\pmb{x}_{17}, \\pmb{x}_{18}, \\pmb{x}_{19}, \\pmb{x}_{20}, \\pmb{x}_{23}\\}  \\\\\n",
    "C_2 &= \\{\\pmb{x}_{ 113}, \\pmb{x}_{12}, \\pmb{x}_{16}\\} \\\\\n",
    "C_3 &= \\{\\pmb{x}_{ 1}, \\pmb{x}_{ 2}, \\pmb{x}_{ 4}, \\pmb{x}_{15}, \\pmb{x}_{21}, \\pmb{x}_{22}, \\pmb{x}_{24}, \\pmb{x}_{25}, \\pmb{x}_{26}, \\pmb{x}_{27}, \\pmb{x}_{28}, \\pmb{x}_{29}, \\pmb{x}_{30}\\}\n",
    "\\end{split}\n",
    "}\n",
    "$$\n",
    "\n",
    "于是得到新的均值向量：\n",
    "$$\n",
    "\\large{\n",
    "\\begin{split}\n",
    "\\pmb{\\mu}_1' = (0.493, 0.207),\\pmb{\\mu}_2' = (0.393,0.066),\\pmb{\\mu}_3'=(0.602,0.396)\n",
    "\\end{split}\n",
    "}\n",
    "$$\n",
    "\n",
    "更新向量后，不断重复上面过程，在第五轮基本和第四轮相同，于是算法停止，最终结果如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://ws1.sinaimg.cn/large/69d4185bly1fwq3gevaplj20ka0cydhy.jpg)\n",
    "\n",
    "'·'表示样本点，'×'表示均值向量，倒三角表示下一次均值向量将要更新的位置。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "结合 k-means 与 Vector Quantization（VQ）可以对图片像素进行聚类，大大降低图片的存储容量。实际使用过程可以参考 [《漫谈 Clustering (番外篇): Vector Quantization》](http://blog.pluskid.org/?p=57)\n",
    "![image](https://ws4.sinaimg.cn/large/69d4185bly1fxfwn0dd4dj20e10aldh5.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-means 的优缺点\n",
    "【参考】\n",
    "- [csdn - k-means 的原理，优缺点以及改进](https://blog.csdn.net/u014465639/article/details/71342072)\n",
    "- [知乎 - k-means聚类算法优缺点？](https://www.zhihu.com/question/31296149)\n",
    "- [sklearn - K-means](https://scikit-learn.org/stable/modules/clustering.html#k-means)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "聚类假设数据集是凸的切实各项同性（isotropic），但大多数情况下并不是这样，如下图的对比：\n",
    "![image](https://wx3.sinaimg.cn/large/69d4185bly1fx8m08967bj20qw0r9gpw.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-Means的主要优点有：\n",
    "- 1）原理比较简单，实现也是很容易，收敛速度快。\n",
    "- 2）聚类效果较优。\n",
    "- 3）算法的可解释度比较强。\n",
    "- 4）主要需要调参的参数仅仅是簇数k。\n",
    "\n",
    "K-Means的主要缺点有：\n",
    "- 1）K值的选取不好把握\n",
    "- 2）对于不是凸的数据集比较难收敛\n",
    "- 3）如果各隐含类别的数据不平衡，比如各隐含类别的数据量严重失衡，或者各隐含类别的方差不同，则聚类效果不佳。\n",
    "- 4） 采用迭代方法，得到的结果只是局部最优。\n",
    "- 5） 对噪音和异常点比较的敏感\n",
    "- 6）初始聚类中心的选择"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-means改进"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k-means 有主要两大缺点，且都和初始值有关：\n",
    "- K 是事先给定的，这个 K 值的选定是非常难以估计的。很多时候，事先并不知道给定的数据集应该分成多少个类别才最合适。（ ISODATA 算法通过类的自动合并和分裂，得到较为合理的类型数目 K）\n",
    "- K-Means算法需要用初始随机种子点来搞，这个随机种子点太重要，不同的随机种子点会有得到完全不同的结果。（K-Means++算法可以用来解决这个问题，其可以有效地选择初始点）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-means ++\n",
    "【参考】\n",
    "- [cnblogs - K-means聚类算法的三种改进(K-means++,ISODATA,Kernel K-means)介绍与对比](https://www.cnblogs.com/yixuan-xu/p/6272208.html)\n",
    "- [github - k-means、k-means++以及k-means||算法分析](https://github.com/endymecy/spark-ml-source-analysis/blob/master/%E8%81%9A%E7%B1%BB/k-means/k-means.md)\n",
    "- [cnblogs - K-Means++算法](https://www.cnblogs.com/shelocks/archive/2012/12/20/2826787.html)\n",
    "- [oschina - 学习笔记：聚类算法Kmeans/K-均值算法](https://my.oschina.net/keyven/blog/518670)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k-means ++ 主要流程如下：\n",
    "- 1.从数据集中随机选择一个样本作为初始的聚类中心$C_1$\n",
    "- 2.对于每个样本点，计算与当前已知聚类中心的最短距离（即与最近的一个聚类中心的距离） D(x) ，这个值越大，表示被选取作为聚类中心的概率较大；\n",
    "- 3.重复2知道找到 k 个聚类中心\n",
    "\n",
    "假设数据集中有 m 个样本点，那么第二步计算将得到 m 个 D(x) 都成的集合 D 。在D中，为了避免噪声，不能直接选取值**最大**的元素，应该选择值**较大**的元素，然后将其对应的数据点作为种子点。 较大值选择操作如下：\n",
    "- 首先求 D 中所有距离的和 $Sum(D(x))$\n",
    "- 选取一个随机值 random ，使得 random 的值在 $Sum(D(x))$内，即 $0 < random < Sum(D(x))$.\n",
    "- 然后循环距离构成的集合 D，计算 $random -= D(x)$，直到 Random 小于 0，那么这个点就作为下一个聚类中心点。\n",
    "\n",
    "如下例：  \n",
    "有8个样本点，随机选择一个点作为聚类中心，计算得到距离集合D = [2,6,10,5,23,18,9,0]，可知 $Sum(D(x))= 73$，然后取 $random = 24$，然后循环 D 计算与 random 的差值：\n",
    "- 第一次：$r = 24 - 2 = 22$ 不小于0\n",
    "- 第二次：$r = 22 - 6 = 16$ 不小于0\n",
    "- 第三次：$r = 16 - 10 = 6$ 不小于0\n",
    "- 第四次：$r = 6 - 5 = 1$ 不小于0\n",
    "- 第五次：$r = 1 - 23 = -22$ 小于0 \n",
    "\n",
    "此时结果小于0，因此停止计算，选择第五个样本作为下次聚类的中心点。之后不断重复上面过程，直到找到 k 个簇为止。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ISODATA算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "但根本上还是 k 值保持不变，需要人来决定 k 的值。而ISODATA算法在运行过程中能够根据各个类别的实际情况进行两种操作来调整聚类中心数K：(1)**分裂操作**，对应着增加聚类中心数；(2)**合并操作**，对应着减少聚类中心数。\n",
    "\n",
    "ISODATA 的输入：\n",
    "- **预期的聚类中心数目$K_o$**：虽然在ISODATA运行过程中聚类中心数目是可变的，但还是需要由用户指定一个参考标准。事实上，该算法的聚类中心数目变动范围也由$K_o$决定。具体地，最终输出的聚类中心数目范围是 [$K_o/2$, $2K_o$]。\n",
    "- **每个类所要求的最少样本数目$N_{min}$**：用于判断当某个类别所包含样本分散程度较大时是否可以进行分裂操作。如果分裂后会导致某个子类别所包含样本数目小于$N_{min}$，就不会对该类别进行分裂操作。\n",
    "- **最大方差$Sigma$**：用于衡量某个类别中样本的分散程度。当样本的分散程度超过这个值时，则有可能进行分裂操作（注意同时需要满足[2]中所述的条件）。\n",
    "-  **两个类别对应聚类中心之间所允许最小距离$d_{min}$**：如果两个类别靠得非常近（即这两个类别对应聚类中心之间的距离非常小），则需要对这两个类别进行合并操作。是否进行合并的阈值就是由 $d_{min}$ 决定。\n",
    "\n",
    "ISODATA算法的原理非常直观，不过由于它和其他两个方法相比需要额外指定较多的参数，并且某些参数同样很难准确指定出一个较合理的值，因此ISODATA算法在实际过程中并没有K-means++受欢迎。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ISODATA 算法流程：\n",
    "1. 从数据集中随机选择 $K_o$ 个样本点作为聚类中心$C = \\{C_1,...,C_{k_o}\\}$\n",
    "2. 对数据集中的每个样本 $\\boldsymbol{x}_i$，计算他到 $K_o$ 个聚类中心的距离，并将其分到距离最近的聚类中心所在的簇\n",
    "3. 判断上面每个类别中的元素数据是否小于 $N_{min}$ ，如果小于则将该类别丢弃，并令 K=K - 1，并将该类别中的样本重新分配给剩下类中距离最小的类\n",
    "4. 针对每个类别$C_i$，重新计算聚类中心 $C_i = \\frac{1}{|C_i|}\\sum_{\\boldsymbol{x} \\in C_i}\\boldsymbol{x}$\n",
    "5. 如果 $K \\le K_o/2$ 则进行**分裂操作**\n",
    "    - 计算每个类别下所有样本在每个维度下的方差\n",
    "    - 针对每个类别的所有方差挑选出最大的方差$\\sigma_{max}$\n",
    "    - 如果这个类别的 $\\sigma_{max} > Sigma$，并且该类包含的样本数量$ni \\ge 2n_{min}$ ，则可以进行分类操作，然后执行下一步的操作。如果不满足上述条件则退出分裂操作\n",
    "    - 将满足上面步奏的类分列成两个类，并令 K= K + 1，$m_{i}^{(+)} = m_i + \\sigma_{max}, m_{i}^{(-)} = m_i - \\sigma_{max}$\n",
    "6. 如果$K \\ge 2K_o$，则进行**合并操作**\n",
    "    - 计算当前所有类别聚类中心两两之间的距离，用 D 表示，其中 D(i,i) = 0\n",
    "    - 对于 $D(i,i) < d_{min}$ 的两个类别进行合并操作，变成一个新的类，该类的新的中心位置为：\n",
    "$$\n",
    "\\large{\n",
    "\\begin{split}\n",
    "m_{new} = \\frac{1}{n_i + n_j}(n_im_i + n_jm_j)\n",
    "\\end{split}\n",
    "}\n",
    "$$\n",
    "其中$n_i,n_j$表示这两个类别中的样本个数，新的聚类中心可以看做事对这个两个类别进行加权求和。如果其中一个类样本个数较多，所合成的新类更偏向于他\n",
    "7. 如果迭代次数达到了最大迭代次数则终止，否则回到第二步继续执行。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "针对ISODATA算法总结一下：**该算法能够在聚类过程中根据各个类所包含样本的实际情况动态调整聚类中心的数目。如果某个类中样本分散程度较大（通过方差进行衡量）并且样本数量较大，则对其进行分裂操作；如果某两个类别靠得比较近（通过聚类中心的距离衡量），则对它们进行合并操作。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MiniBatch\n",
    "- [sklearn - Mini Batch K-Means](https://scikit-learn.org/stable/modules/clustering.html#mini-batch-k-means)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mini batch 是指在计算的时候不适用全都的数据，而是从数据集中采集一定数量的样本记性计算，这样可以加速计算。当然这会最后的结果不如使用全部样本精确，但是精度损失的并不是特别多。两种方法的对比如下，使用的均是 k-mean++ 算法：\n",
    "![img](https://ws2.sinaimg.cn/large/69d4185bly1fx8st4vb36j20m808cq4t.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 其他聚类方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-medoids\n",
    "【参考】\n",
    "- [coursera - The K-Medoids Clustering Method](https://www.coursera.org/lecture/cluster-analysis/3-4-the-k-medoids-clustering-method-nJ0Sb)\n",
    "- [csdn - 机器学习：K-means和K-medoids对比[4]](https://blog.csdn.net/databatman/article/details/50445561)\n",
    "- [pluskids - 漫谈 Clustering (2): k-medoids](http://blog.pluskid.org/?p=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "虽然 k-means 聚类可以得到一个很好的结果，但是对于属于离散值而不是连续值的数据就不是那么好了。比如我们划分不同鸟的类型，在使用 k-means 计算聚类中心时，我们使用的是在当前类别的欧式空间计算所有样本点的均值，显然对于离散的值比如麻雀、和喜鹊就没有办法使用平均值来计算，算出来也没有什么意义。\n",
    "\n",
    "这时候就需要用到 k-medoids 算法，算法与 k-means 类似，不同的在于我们把k-means目标函数的都是距离修改为任意的相似度衡量函数即可。k-medoids 的算法如下：\n",
    "- 在样本中随机选取 K 个样本作为初始的聚类中心\n",
    "- 计算各个样本点到每个聚类中心的相似度，样本点与哪个聚类中心相似度最大，就划分到哪个类别中\n",
    "- 根据分好的类，更新聚类中心\n",
    "     - 计算聚类内每个样本点到其中一个样本点的距离，\n",
    "     - 找到距离和最小的样本点作为新的聚类中心\n",
    "- 重复第2、3步知道满足迭代次数或指定的误差值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，k-means的质心是各个样本点的平均，可能是样本点中不存在的点。k-medoids的质心一定是某个样本点的值。如下图：\n",
    "![img](https://wx4.sinaimg.cn/large/69d4185bly1fxfvrz4m0rj20fu0aa3z6.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由以上描述可以知道，k-medios 需要计算两两样本之间的距离，在样本量不大的情况下还好，只要样本量一大计算速度会非常慢，时间复杂度为 $O(N^2)$，而 k-means 的复杂度为 $O(N)$，所有实践中 k-means 的应用要比 k-medoids 多。但 k-medoids 对噪声的鲁棒性比较好。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 密度聚类法\n",
    "\n",
    "最主要的方法时 DBSCAN 方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 层次聚类\n",
    "\n",
    "最主要的是 AGNES。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "目录",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "279px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
