{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>目录<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#决策树（DT）\" data-toc-modified-id=\"决策树（DT）-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>决策树（DT）</a></span></li><li><span><a href=\"#划分选择\" data-toc-modified-id=\"划分选择-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>划分选择</a></span><ul class=\"toc-item\"><li><span><a href=\"#信息增益(ID3)\" data-toc-modified-id=\"信息增益(ID3)-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>信息增益(ID3)</a></span></li><li><span><a href=\"#西瓜书的例子\" data-toc-modified-id=\"西瓜书的例子-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>西瓜书的例子</a></span></li><li><span><a href=\"#增益率(C4.5)\" data-toc-modified-id=\"增益率(C4.5)-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>增益率(C4.5)</a></span></li><li><span><a href=\"#基尼指数(CART)\" data-toc-modified-id=\"基尼指数(CART)-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>基尼指数(CART)</a></span></li></ul></li><li><span><a href=\"#剪枝处理\" data-toc-modified-id=\"剪枝处理-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>剪枝处理</a></span><ul class=\"toc-item\"><li><span><a href=\"#预剪枝\" data-toc-modified-id=\"预剪枝-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>预剪枝</a></span></li><li><span><a href=\"#后剪枝\" data-toc-modified-id=\"后剪枝-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>后剪枝</a></span></li></ul></li><li><span><a href=\"#连续值和缺失值\" data-toc-modified-id=\"连续值和缺失值-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>连续值和缺失值</a></span><ul class=\"toc-item\"><li><span><a href=\"#连续值的处理\" data-toc-modified-id=\"连续值的处理-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>连续值的处理</a></span></li><li><span><a href=\"#缺失值处理\" data-toc-modified-id=\"缺失值处理-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>缺失值处理</a></span></li></ul></li><li><span><a href=\"#多变量决策树\" data-toc-modified-id=\"多变量决策树-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>多变量决策树</a></span></li><li><span><a href=\"#一些技巧\" data-toc-modified-id=\"一些技巧-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>一些技巧</a></span><ul class=\"toc-item\"><li><span><a href=\"#停止条件\" data-toc-modified-id=\"停止条件-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>停止条件</a></span></li><li><span><a href=\"#过拟合\" data-toc-modified-id=\"过拟合-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>过拟合</a></span></li><li><span><a href=\"#KF验证\" data-toc-modified-id=\"KF验证-6.3\"><span class=\"toc-item-num\">6.3&nbsp;&nbsp;</span>KF验证</a></span></li><li><span><a href=\"#RF-随机森林\" data-toc-modified-id=\"RF-随机森林-6.4\"><span class=\"toc-item-num\">6.4&nbsp;&nbsp;</span>RF 随机森林</a></span></li></ul></li><li><span><a href=\"#总结\" data-toc-modified-id=\"总结-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>总结</a></span><ul class=\"toc-item\"><li><span><a href=\"#优缺点\" data-toc-modified-id=\"优缺点-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>优缺点</a></span></li><li><span><a href=\"#实践中的建议\" data-toc-modified-id=\"实践中的建议-7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;</span>实践中的建议</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 决策树（DT）\n",
    "【参考】\n",
    "\n",
    "- [csdn - 机器学习方法(四)：决策树Decision Tree原理与实现技巧](https://blog.csdn.net/xbinworld/article/details/44660339)\n",
    "- [csdn - 从决策树学习谈到贝叶斯分类算法、EM、HMM](https://blog.csdn.net/v_july_v/article/details/7577684) 含有代码实现\n",
    "\n",
    "决策树（decision tree）是一类常见的机器学习算法，顾名思义，他是基于树结构进行决策的，和人脑的处理机制很类似。\n",
    "\n",
    "一般的，一棵决策树包含一个根节点，若干个内部结点和若干个叶节点；叶节点对应于决策结果，其他每个节点对应于一个属性测试；每个节点包含的样本集合根据属性测试的结果被划分到子节点中；根节点包含样本全集。从根节点到叶节点的路径对应于一个**判定测试序列**。决策树的目的就在于**产生一个泛化能力强，即处理未见示例能力强的决策树。**其基本流程遵循简单而直接的**分而治之（divide-and-conquer）**策略。\n",
    "\n",
    "如下面的例子：\n",
    "![image](https://wx3.sinaimg.cn/large/69d4185bly1fweir2imifj20g006hdgk.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "决策树的训练数据往往就是这样的表格形式，表中的前三列（ID不算）是数据样本的属性，最后一列是决策树需要做的分类结果。通过该数据，构建的决策树如下： \n",
    "![img](https://ws3.sinaimg.cn/large/69d4185bly1fweirsaxcoj20g407yq3f.jpg)\n",
    "\n",
    "决策树的基本算法：\n",
    "\n",
    "输入数据集$D = \\{ (\\boldsymbol{x}_1,y_1), (\\boldsymbol{x}_2,y_2),\\cdots,(\\boldsymbol{x}_m,y_m)\\}$\n",
    "\n",
    "属性集 $A=\\{ a_1,a_2,\\cdots,a_d \\}$， 即在每个 x 中含有 d 个属性。\n",
    "\n",
    "-------------------------------------\n",
    "流程：函数TreeGenerate(D, A):\n",
    "\n",
    "生成节点 Node\n",
    "\n",
    "**if** D 中样本全属于同一类别 C **then**  （如样本在性别属于中全部属于男性）  \n",
    "    &emsp;&emsp; 将 node 标记为 C 类叶节点；** return **  \n",
    "**end if**\n",
    "\n",
    "**if** A=φ **OR** D 中样本在 A 上取值相同 **then**（表示已经没有属性可以划分，或者在属性上取值相同如在性别上去不为男性）  \n",
    "    &emsp;&emsp;将 node 标记为叶节点，其类别标记为 D 中样本数最多的类； **return**  \n",
    "**end if**\n",
    "\n",
    "$\\large{\\color{red}{从 A 中选择最优的划分属性 a_*}}$ (如上图中属性拥有房产)\n",
    "\n",
    "**for** $a_*$ 的每一个值 $a_*^{v}$ **do**:（如拥有房产属性中有两个值：是、否）  \n",
    "    &emsp;&emsp;为 node 生成一个分支；令$D_v$表示 D 中在 $a_*$ 上取值为 $a_*^{v}$ 的样本子集；（如$a_*^{v}$为是，$D_v$就表示拥有房产的样本）  \n",
    "    &emsp;&emsp; **if** $D_v$ = φ **then**  \n",
    "    &emsp;&emsp;&emsp;&emsp;将分支节点标记为叶节点，其类别标记为 D 中样本最多的类；**return**  \n",
    "    &emsp;&emsp; **else**  \n",
    "    &emsp;&emsp;&emsp;&emsp; 以 TreeGenerate($D_v$, A\\\\{$a_*$})，为分支点（A\\\\{$a_*$} 表示从 A 中去除 $a_*$ 属性）  \n",
    "    &emsp;&emsp; **end if**  \n",
    "**end for**\n",
    "\n",
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有了这棵树，我们就可以对新来的用户数据进行是否可以偿还的预测了。\n",
    "\n",
    "决策树最重要的是决策树的构造。所谓决策树的构造就是进行属性选择度量确定各个特征属性之间的拓扑结构。构造决策树的关键步骤是分裂属性。所谓分裂属性就是在某个节点处按照某一特征属性的不同划分构造不同的分支，其目标是让各个分裂子集尽可能地“纯”。尽可能“纯”就是尽量让一个分裂子集中待分类项属于同一类别。分裂属性分为三种不同的情况:\n",
    "- 1、属性是离散值且不要求生成二叉决策树。此时用属性的每一个划分作为一个分支。 （如婚姻属性，就可以划分为三个分支：单身、已婚、离婚）\n",
    "- 2、属性是离散值且要求生成二叉决策树。此时使用属性划分的一个子集进行测试，按照“属于此子集”和“不属于此子集”分成两个分支。 （如婚姻属性按照是否已婚可以划分为：已婚和未婚，未婚中就包含了单身和离婚这两个属性值？）\n",
    "- 3、属性是连续值。此时确定一个值作为**分裂点**split_point，按照>split_point和<=split_point生成两个分支。（如年收入）\n",
    "\n",
    "决策树的属性分裂选择是[**贪心算法**](https://baike.baidu.com/item/%E8%B4%AA%E5%BF%83%E7%AE%97%E6%B3%95)，也就是没有回溯的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 划分选择\n",
    "**概念：纯度（purity）、信息熵(information entropy)、**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "决策树的学习关键就在于**从属性集中找到最后的划分属性**。一般而言，随着划分的进行，我们希望决策树的分支节点所包含的样本尽可能的属于同一类别，即节点的**纯度（purity）**越来越高。那我们就需要一个来衡量节点的纯度高低的指标，而**信息熵(information entropy)**就是用来衡量样本集合纯度的指标。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 信息增益(ID3)\n",
    "假设当前样本集合 D 中第 k 类所占的比例为$p_k(k=1,2,\\cdots,|y|)$ ，这里的 y 与 $p_k$ 的含义如下：\n",
    " - y 指的是要把样本分成几类，如上面的列子，我们要把样本分成有能力偿还债务与不能偿还债务的两种，那么 y 就是2；以及西瓜书中把西瓜分为分为好瓜与坏瓜两类，y 也是等于2\n",
    " - $p_k$ 表示类别所占的比列，如上面的例子，样本总数为 10 个，y 的值为2，那么$p_1$表示能偿还债务的人（即无法偿还债务属性为否的样本）数量为 7 ，因此 $p_1 = \\frac{7}{10} = 0.7$；同理$p_2$无法偿还债务的人，样本数量为 3，$p_2 = \\frac{3}{10} = 0.3$\n",
    "\n",
    "那么 D 的信息熵定义为：\n",
    "$$\n",
    "\\large{\n",
    "Ent(D) = -\\sum_{k=1}^{|y|}p_k\\log_{2}p_k\n",
    "} \\tag{1}\n",
    "$$\n",
    "\n",
    "Ent(D) 越小，D 的纯度越高。\n",
    "\n",
    "因此可以计算上面例子的信息熵值（根节点）为$Ent(D) = -(0.7 * \\log_20.7 + 0.3 * \\log_20.3)= 0.88$\n",
    "\n",
    "假设离散属性 a 有 V 个可能的取值 {$a^{1},a^{2},\\cdots,a^{V}$}（如上例的婚姻属性有三个可能取值，V=3，$a^{1}$=单身、$a^{2}$=已婚、$a^{3}$=离婚），若使用 a 对样本数据集 D 进行划分，则会产生 V 个分支节点，其中第 v 个分支节点包含了 D 中所有在属性 a 上取值为$a^{v}$的样本，记为 $D^{v}$，(如$D^{1}$就代表了婚姻属性为单身的所有样本)。\n",
    "\n",
    "根据式子（1）可以计算出$D^{v}$信息熵，在考虑到不同的分支节点所包含的样本数的不同，给分支节点赋予权重$\\frac{|D^{v}|}{|D|}$，这样样本越多的分支节点的影响越大，于是可以计算出用属性 a 对样本集 D 进行划分所获得的**信息增益（information gain）**：\n",
    "$$\n",
    "\\large{\n",
    "Gain(D,a) = Ent(D) - \\sum_{v=1}^{V}\\frac{|D^{v}|}{|D|} Ent(D^{v})\n",
    "}\\tag{2}\n",
    "$$\n",
    "\n",
    "一般来说，**信息增益越大，则意味着使用属性 a 来进行划分所获得的纯度提升越大**，因此在基本算法中(红字部分)就是选择信息增益最大的属性，即$\\large{\\color{red}{a_* = arg\\max_{a \\in A}Gain(D,a)}}$，著名的 **$\\large{\\color{red}{ID3}}$**决策树算法就是以信息增益为准侧进行划分属性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如上例，我们已经计算出了信息熵 Ent(D) 的值为0.88。我们想要计算以拥有房产和婚姻来划分得到的信息增益。\n",
    "\n",
    "首先以拥有房产为例，可以看到有两个属性值，令$D^{1}$为拥有房产，$D^{2}$为没有房产。可以知道$D^{1}$含有{1,4,7}三个样本，可以看到这三个样本均能偿还债务，因此$p_1=\\frac{3}{3}$；不能偿还债务的为零，因此$p_2=\\frac{0}{3}$。$D^{2}$为没有房产，可以知道$D^{1}$含有{2,3,5,6,8,9,10}七个样本，可以看到这4个样本能偿还债务，因此$p_1=\\frac{4}{7}$；不能偿还债务的为3，因此$p_2=\\frac{3}{7}$。相应的信息熵为：\n",
    "$$\n",
    "\\large{\n",
    "\\begin{split}\n",
    "Ent(D^{1}) &= -\\left(\\frac{3}{3} * \\log_2\\frac{3}{3} + \\frac{0}{3}*\\log_2\\frac{0}{3}\\right) = 0 \\\\\n",
    "Ent(D^{2}) &= -\\left(\\frac{4}{7} * \\log_2\\frac{3}{3} + \\frac{3}{7}*\\log_2\\frac{3}{7}\\right) = 0.99\n",
    "\\end{split}\n",
    "}\n",
    "$$\n",
    "\n",
    "因此可以算出拥有房产的信息增益：\n",
    "$$\n",
    "\\large{\n",
    "\\begin{split}\n",
    "Gain(D,拥有房产) &= 0.88 -\\left(\\frac{3}{10} * 0 + \\frac{7}{10} * 0.99 \\right) \\\\\n",
    "                &= 0.88 - 0.69 = 0.19\n",
    "\\end{split}\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来结算婚姻的信息增益，令$D^{1} = 单身\\;\\{1,3,8,10\\}, D^{2}=已婚\\;\\{2,4,6,9\\}, D^{2} = 离婚\\;\\{5,7\\}$ 计算各自的信息熵如下：\n",
    "$$\n",
    "\\large{\n",
    "\\begin{split}\n",
    "Ent(D^{1}) &= -\\left(\\frac{2}{4} * \\log_2\\frac{2}{4} + \\frac{2}{4} * \\log_2\\frac{2}{4}\\right) = 1 \\\\\n",
    "Ent(D^{2}) &= -\\left(\\frac{4}{4} * \\log_2\\frac{4}{4} + \\frac{0}{4}*\\log_2\\frac{0}{4}\\right) = 0 \\\\\n",
    "Ent(D^{3}) &= -\\left(\\frac{1}{2} * \\log_2\\frac{1}{2} + \\frac{1}{2} * \\log_2\\frac{1}{2}\\right) = 1\n",
    "\\end{split}\n",
    "}\n",
    "$$\n",
    "\n",
    "那么婚姻的信息增益为：\n",
    "$$\n",
    "\\large{\n",
    "\\begin{split}\n",
    "Gain(D,拥有房产) &= 0.88 -\\left(\\frac{4}{10} * 1 + \\frac{4}{10} * 0 + \\frac{2}{10} * 1\\right) \\\\\n",
    "                &= 0.88 - 0.60 = 0.28\n",
    "\\end{split}\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到婚姻的信息增益比使用是否拥有房产更大，因此使用婚姻进行划分更合理。划分的结果如下图：\n",
    "![image](https://wx4.sinaimg.cn/large/69d4185bly1fwf0ordvt5j20bx04pmxe.jpg)\n",
    "\n",
    "接下来做的就是在每一个分支上重复该过程，知道最终返回。\n",
    "\n",
    "**偿还债务这个例子不是太好，放弃这个例子，使用习西瓜书里面的例子**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 西瓜书的例子"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用西瓜书中的例子来研究信息熵：\n",
    "![image](https://wx2.sinaimg.cn/large/69d4185bly1fwf0zc6ut4j20m00at76c.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们要决定瓜是不是好瓜，以你 y 的值为 2，数据集一共包含了 17 个样本。正例，也就是好瓜是8个，反例是 9 个。因此可以计算出**根节点**的信息熵为：Ent(D) = 0.998\n",
    "\n",
    "然后计算此时的各个属性的信息增益，一色泽为例，此时 D 可以划分为 3 个子集 $D^{1}=青绿$ 包含 6 个样本, $D^{2}=乌黑$包含 6 个样本, $D^{3}=浅白$包含 5 个样本。按照色泽划分之后的每个节点的信息熵值为：\n",
    "$$\n",
    "\\large{\n",
    "\\begin{split}\n",
    "Ent(D^{1}) &= 1.000 \\\\\n",
    "Ent(D^{2}) &= 0.918 \\\\\n",
    "Ent(D^{3}) &= 0.722 \\\\\n",
    "\\end{split}\n",
    "}\n",
    "$$\n",
    "相应的信息增益为: Gain(D, 色泽) = 0.109\n",
    "\n",
    "相应的求出其他几个属性的信息增益：Gain(D, 根蒂) = 0.143、Gain(D, 敲声) = 0.141、Gain(D, 纹理) = 0.381、Gain(D, 脐部) = 0.289、Gain(D, 触感) = 0.006。可以看到使用`纹理`的信息增益最大，于是就选择她来划分属性，如下图的划分结果：\n",
    "![image](https://wx1.sinaimg.cn/large/69d4185bly1fwf1guffolj20f204twev.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后决策树算法对每一个分支点在进行划分。以上图中第一个分支`纹理=清晰`为例，样本集合$D^{1}$包含9个样本，此时可选的属性集合为`{色泽，根蒂，敲声，脐部，触感}`，基于 $D^{1}$ 可计算出各个属性的信息增益为：Gain(D, 色泽) = 0.043、Gain(D, 根蒂) = 0.458、Gain(D, 敲声) = 0.331、Gain(D, 脐部) = 0.458、Gain(D, 触感) = 0.458，可以看到根蒂、脐部、触感的信息增益都是最大的，可以任选一个进行划分。之后不断重复这个过程。最终得到如下的结果图：\n",
    "![image](https://wx2.sinaimg.cn/large/69d4185bly1fwf21hivxlj20h90cfgmr.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 增益率(C4.5)\n",
    "在计算时，我们忽略了编号这一项。如果把编号也作为一个候选属性进行划分，那么可以计算出他的信息增益为 0.998，远远大于其他候选属性。这个很容易理解，因为按照编号划分产生 17 个分支，每个分支节点仅包含一个样本，这样分支点纯度已经达到了最大。然而这样的决策树显然是没有什么泛化能力，无法对西样本进行预测。\n",
    "\n",
    "实际上，**信息增益准则对可取值数目比较多的属性有所偏好**，如纹理、根蒂、色泽都含有三个属性。为了减少这种偏好带来的不理影响，著名的$\\large{\\color{red}{C4.5}}$决策树算法不像 `ID3` 使用信息增益，而是使用$\\large{\\color{red}{增益率（gain\\; ratio）}}$来选择属性划分，增益率的定义如下：\n",
    "$$\n",
    "\\large{\n",
    "Gain_{ratio}(D,a) = \\frac{Gain(D,a)}{IV(a)}\n",
    "} \\tag{3}\n",
    "$$\n",
    "\n",
    "其中：\n",
    "$$\n",
    "\\large{\n",
    "IV(a) = -\\sum_{v=1}^{V}\\frac{|D^{v}|}{|D|}\\log_2\\frac{|D^{v}|}{|D|}\n",
    "} \\tag{4}\n",
    "$$\n",
    "\n",
    "称为属性 a 的$\\large{\\color{red}{固有值(intrinsic\\; value)}}$。属性 a 的取值越多（即 V 值大），则 IV(a) 的值通常也大。如对于西瓜数据集就有，IV(触感)= 0.874（V=2）、IV(色泽)= 1.580（V=3）、IV(编号)= 4.088（V=17）。\n",
    "\n",
    "需要注意的是，**增益率准则对可取数值较少的属性有偏好**，因此 C4.5 算法并不是直接选择增益率最大的属性进行划分，而是使用一个启发式：**先从候选属性中找出信息增益高于平均水平的属性，再从中选择增益率最高的**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基尼指数(CART)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CART（Classification and Regression Tree） 决策树使用**$\\large{\\color{red}{基尼指数（Gini\\; index）}}$** 来选择划分属性，数据集 D 的纯度可以使用基尼值来衡量，CART 既可以用于分类，也可以用于回归。公式如下：\n",
    "$$\n",
    "\\large{\n",
    "\\begin{split}\n",
    "Gini(D) &= \\sum_{k=1}^{y}\\sum_{k' \\neq k}p_kp_{k'} \\\\\n",
    "        &= 1 - \\sum_{k=1}^{y}p_{k}^{2} \n",
    "\\end{split} \\tag{5}\n",
    "}\n",
    "$$\n",
    "\n",
    "直观的来说，Gini(D) 反映了**从数据集随机抽取两个样本，其类别标记不一致的概率**，因此，** Gini(D) 越小，数据的纯度越高。**\n",
    "\n",
    "属性 a 的基尼指数可以定义为：\n",
    "$$\n",
    "\\large{\n",
    "\\begin{split}\n",
    "Gini_index(D,a) &= \\sum_{v=1}^{V}\\frac{|D^{v}|}{|D|}Gini(D^{v})\n",
    "\\end{split} \\tag{6}\n",
    "}\n",
    "$$\n",
    "\n",
    "我们在候选属性集合 A 中，选择基尼指数最小的属性进行划分，即$a_* = arg \\min_{a \\in A}Gini\\_index(D,a)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 剪枝处理\n",
    "剪枝（pruning）是决策树学习算法应对$\\large{\\color{red}{过拟合}}$的主要手段。\n",
    "\n",
    "在决策树学习中，为了尽可能正确分类训练样本，节点划分过程将不断地重复，有时候会造成决策树的分支过多，这时就可能因训练样本学的太好了，以至于把训练样本自身的一些特点当做了所有数据都具有的一般性质而导致过拟合。因此，需要主动去掉一些分支来降低过拟合的风险。\n",
    "\n",
    "决策树剪枝的基本策略有$\\large{\\color{red}{预剪枝（prepruning）}}$ 和 $\\large{\\color{red}{后剪枝}}$：\n",
    "- 预剪枝是指在决策树生成的过程中，对每个节点在划分前先进行估计，若当前节点的划分不能带来决策树泛化能力的提升，则停止划分并将当前节点标记为叶节点\n",
    "- 后剪枝则是先从训练集生成一棵完整的决策树，然后自底向上地对非叶节点进行考察，若将该节点对应的子树替换为叶节点能带来决策树泛化性能的提升，则将该子树替换为叶节点。\n",
    "\n",
    "对于判断性能是否得到了提升，我们可以将数据集划分成两部分，一部分用于训练，另一部分用于验证，对性能进行评估。如我们对上面的西瓜数据集随机分成两部分，上部分是训练集，下部分是测试集：\n",
    "![image](https://wx4.sinaimg.cn/large/69d4185bly1fwfnhszlbvj20mg0bq76j.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预剪枝\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设我们按照信息增益的原则来进行属性的划分，可以得到如下的决策树：\n",
    "![image](https://ws4.sinaimg.cn/large/69d4185bly1fwfoilcec1j20hf0ey0uc.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先，基于信息增益我们会选择“脐部”进行划分，产生 3 个分支，如上图的①。是否进行这个划分呢，这时候就需要对划分前后的性能进行评估。\n",
    "\n",
    "在划分前所有, 的样本集中在根节点。若不进行划分，根据算法该节点被标记为叶节点，类别标记为训练样本数最多的类别（最多的样本的类不唯一时，可以任选其中一类）。假设我们将这个叶节点标记为好瓜（上表训练集中的正例与负例一样多，选择其中好瓜作为标签），用上表中的验证集对这个单点决策树进行评估，那么编号{4,5,8}的样本被正确分类，另外四个样本被错误分类，于是验证集的精度为$\\frac{3}{7}×100\\%=42.9\\%$\n",
    "![image](https://ws2.sinaimg.cn/large/69d4185bly1fwfufcahu6j20el0a0t9v.jpg)\n",
    "\n",
    "在使用属性“脐部”划分之后，上图的②③④节点包含的样本，三个节点被标记为“好瓜”、“好瓜”、“坏瓜”。此时验证集中{4,5,8,11,12}被正确的分类，验证集的精度为$\\frac{5}{7}×100\\%=71.4\\% \\gt 42.9\\%$。因此，用“脐部”进行划分可以确定。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后决策算法要对节点②进行划分，基于信息增益的原则选出属性“色泽”进行划分，可以看到青绿和乌黑的样本被划分为正例，浅白的样本被划分为负例。然而使用色泽进行划分之后，验证集{4,8,11,12}被正确分类，可以看到与上面的相比，中编号为{5}的样本的分类结果由正确变成了错误，使得样本集的精度下降为 57.1% 。因此，预剪枝策略禁止在②节点处划分。\n",
    "\n",
    "对节点③我们使用“根蒂”进行划分，划分后验证集的精度不改变，没有能提升验证集的精度，因此，根据预剪枝的策略，禁止在③几点处进行划分。\n",
    "\n",
    "对于节点④，所有的训练样本已经属于同一类别，因此不需要进行划分。\n",
    "\n",
    "因此最终生成的决策树就是上图，验证集的精度为 71.4% 。这是一棵仅有一层的决策树，亦成为**$\\large{\\color{red}{决策树桩（decision\\; stump）}}$**。\n",
    "\n",
    "对比剪枝前与剪枝后可以发现，预剪枝是的决策树的很多分支都没有“展开”，这不仅降低了过拟合的风险，还显著的降低了训练的时间开销和测试时间开销。  \n",
    "但另一方面，有些分支的当前划分虽然不能提升泛化性能、甚至是导致泛化性能的下降，但在其基础上进行后续的划分却有可能导致性能显著提高；**预剪枝是基于贪心本质禁止这些分支展开，给预剪枝决策树带来了欠拟合的风险**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 后剪枝"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "后剪枝先从训练一棵完整的决策树，如上图的未剪枝的决策树，可以知道此时的验证集精度为 42.9%。\n",
    "![image](https://wx4.sinaimg.cn/large/69d4185bly1fwfvnf225ij20h60f175y.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**后剪枝的流程**：\n",
    "\n",
    "后剪枝策略首先考察上图的⑥节点。将其领衔的分支剪除，相当于把节点⑥标记为叶节点。替换后的叶节点包含编号为{7,15}的训练样本，于是该节点被标记为“好瓜”，此时决策树验证集的精度提升到了57.1%，于是节点⑥进行后剪枝。如下图所示：\n",
    "![image](https://wx1.sinaimg.cn/large/69d4185bly1fwfw1goah8j20jr0bytab.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后考察节点⑤，将其领衔的子树替换为叶节点，替换后的节点包含{6,7,15}的训练样例，叶节点类别标记为好瓜，此时验证集的精度为57.1%，因此不进行剪枝。\n",
    "\n",
    "对节点②，将其领衔的子树替换为叶节点，则替换后的叶节点包含编号为{1,2,3,14}的训练样本，业绩点被标记为“好瓜”。此时验证集的精度为71.4%，因此，记性剪枝的操作。\n",
    "\n",
    "对于节点③和①，若将其领衔的子树替换为叶节点，获得验证集精度分别为71.4%和42.9%，均没有提高，因此不进行剪枝。\n",
    "\n",
    "最终得到的结果就是上图，验证集的精度为 71.4%。\n",
    "\n",
    "对比预剪枝和后剪枝可以看出，后剪枝决策树通常比预剪枝决策树保留了更多的分支。一般情况下，后剪枝的欠拟合风险更小，泛化性能往往优于预剪枝决策树。但后剪枝决策过程是在生产完全决策树之后进行的，并且要自底向上的对树种的所有非叶节点进行逐一考察，因此训练时间开销比未剪枝决策树和预剪枝决策树都要打很多。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 连续值和缺失值\n",
    "## 连续值的处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于连续值的可取数目不再有限，因此，不能直接根据连续属性的可取值来对节点进行划分。这这时候就需要使用$\\large{\\color{red}{连续值离散化}}$技术。最简单的策略就是采用$\\large{\\color{red}{二分法(bi-partition)}}$对连续属性进行处理，这也是$\\large{\\color{red}{C4.5}}$算法采取的策略。\n",
    "\n",
    "给定样本集 D 和连续属性 a，假设 a 在 D 上出现了 n 个不同的值，讲这些值从小到大的顺序排列，记为$\\{a^1,a^2,\\cdots,a^n\\}$.基于划分点 t 可以将数据集 D 划分为子集$D_{t}^{-}$和$D_{t}^{+}$，其中$D_{t}^{-}$包含那些在属性 a 上取值不大于 t 的样本，而$D_{t}^{+}$则包含在属性 a 上取值大于 t 的样本。显然，对相邻的属性取值$a^{i}$ 和 $a^{i+1}$来说，t 在区间$[a^{i}, a^{i+1})$中去任意值产生的划分结果相同。因此，对连续属性$a^{i}$，我们可以考察 n-1 个元素的候选划分点集合：\n",
    "$$\n",
    "\\large{\n",
    "T_a = \\left\\{\\frac{a^i + a^{i+1}}{2} \\;\\vert\\; 1 \\le i \\le n-1 \\right\\}\n",
    "}\\tag{7}\n",
    "$$\n",
    "\n",
    "即把区间 $[a^{i}, a^{i+1})$ 的中位点 $\\frac{a^i + a^{i+1}}{2}$ 作为候选划分点。之后，就可以像离散属性值一样来考察这些划分点，选取最优的划分点进行样本集合的划分。如对(2)式进行改造之后：\n",
    "$$\n",
    "\\large{\n",
    "\\begin{split}\n",
    "Gain(D,a) &= \\max_{t \\in T_a}Gain(D,a,t) \\\\\n",
    "          &= \\max_{t \\in T_a}Ent(D) - \\sum_{\\lambda \\in \\{-,+\\}}\\frac{|D_{t}^{\\lambda}|}{|D|}Ent(D_{t}^{\\lambda})\n",
    "\\end{split}\n",
    "}\\tag{8}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中 Gain(D,a,t) 是样本集 D 基于划分点 t 二分后的信息增益，于是，我们就可选择 Gain(D,a,t) 最大化的划分点。\n",
    "\n",
    "如下的例子，对原有的数据添加了两个连续属性之后的数据集：\n",
    "![image](https://ws2.sinaimg.cn/large/69d4185bly1fwfxkfhcjej20k70ajgo4.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对属性“密度”，在决策树学习开始时，根据节点包含的17个训练样本在该属性上取值均不同，根据式（7），该属性的候选划分点结合包含16个候选值：T(密度) = {0.244, 0.294, 0.352, 0.382, 0.420, 0.459, 0.519, 0.575, 0.601, 0.621, 0.637, 0.648, 0.662, 0.682, 0.708, 0.747}，由式子（8）可以计算出密度的最大信息增益为0.262，对应的划分点为 0.382。\n",
    "\n",
    "对属性“含糖率”，有16个候选值，T(含糖率)={0.050, 0.074, 0.095, 0.101, 0.126, 0.155, 0.180, 0.205, 0.213, 0.226, 0.251, 0.266, 0.293, 0.344, 0.373, 0.418}，最大信息增益为 0.349，对应的划分点为 0.126.\n",
    "\n",
    "再计算出其他属性的相应信息增益：Gain(D, 色泽) = 0.109、Gain(D, 根蒂) = 0.143、Gain(D, 敲声) = 0.141、Gain(D, 纹理) = 0.381、Gain(D, 脐部) = 0.289、Gain(D, 触感) = 0.006、Gain(D,密度)= 0.262、Gain(D,含糖量)= 0.349。于是纹理被选为根节点的划分属性，之后的节点递归此过程，最终得到如下的决策树：\n",
    "![image](https://wx4.sinaimg.cn/large/69d4185bly1fwfy9ge46pj20cl09egmc.jpg)\n",
    "\n",
    "**与离散属性不同，若当前节点划分属性为连续属性，该属性还可以作为后代节点的划分属性**。如在父节点上使用了密度≤0.381，不会禁止在子节点上使用密度≤0.294."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 缺失值处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在现实中常会遇到不完整的样本，即样本的某些属性值缺失。如果简单的放弃不完整样本，仅使用无缺失值的样本进行学习，显然对数据信息极大的浪费。如下给出的含有趋势值的西瓜数据：\n",
    "![image](https://ws1.sinaimg.cn/large/69d4185bly1fwfyose8wfj20mg0aijta.jpg)\n",
    "\n",
    "如果不完整的数据不使用，那么仅有四条数据可供使用，显然需要考虑到有缺失值的数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有两个问题需要解决：\n",
    "- 1.如何在属性有缺失值的情况下进行划分属性的选择？\n",
    "- 2.给定划分属性，若样本在该属性上有缺失值，如何对样本进行划分\n",
    "\n",
    "给定训练集 D 和属性 a，令$\\tilde{D}$表示在 a 属性上没有缺失值的样本子集，如以色泽属性为例，$\\tilde{D}=\\{2,3,4,6,7,8,9,10,11,12,14,15,1617\\}$ 14个样例。\n",
    "\n",
    "对于问题 1 ，我们可以根据 $\\tilde{D}$ 来判断属性 a 的优劣。假定属性 a 有 V 个可取值$a^1,a^2,\\cdots,a^V$（青绿、乌黑、浅白），令$\\tilde{D}^v$表示$\\tilde{D}$中在 a 上取值为$a^v$的样本子集（如$\\tilde{D^1}(青绿)=\\{4,6,10,17\\}$，$\\tilde{D}_k$ 表示$\\tilde{D}$中属于第 k 类（k=1,2,...,y）的样本子集，显然有$\\tilde{D}=\\bigcup_{k=1}^y\\tilde{D}_k$，$\\tilde{D}=\\bigcup_{v=1}^V\\tilde{D}^v$.假定我们为每一个样本$\\boldsymbol{x}$赋予权重$\\omega_{\\boldsymbol{x}}$（学习开始时权重均设置为1），并做如下定义：\n",
    "$$\n",
    "\\large{\n",
    "\\begin{split}\n",
    "\\rho &= \\frac{\\sum_{\\boldsymbol{x} \\in \\tilde{D}} \\omega_{\\boldsymbol{x}}}{\\sum_{\\boldsymbol{x} \\in D} \\omega_{\\boldsymbol{x}}} \\\\\n",
    "\\tilde{p}_k &= \\frac{\\sum_{\\boldsymbol{x} \\in \\tilde{D}_k} \\omega_{\\boldsymbol{x}}}{\\sum_{\\boldsymbol{x} \\in D} \\omega_{\\boldsymbol{x}}} \\quad (1 \\le k \\le y)\\\\\n",
    "\\tilde{\\gamma}_v &= \\frac{\\sum_{\\boldsymbol{x} \\in \\tilde{D}^v} \\omega_{\\boldsymbol{x}}}{\\sum_{\\boldsymbol{x} \\in D} \\omega_{\\boldsymbol{x}}} \\quad (1 \\le v \\le V)\\\\\n",
    "\\end{split}\n",
    "}\\tag{9}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "直观的来看，对属性 a， ρ表示无缺失值样本所占的比例，$\\tilde{p}_k$ 表示无确实样本中第 k 类所占的比例，$\\tilde{\\gamma}_v$表示无缺失值样本中属性 a 上取值为 $a^v$ 的样本所占的比例。$\\tilde{p}_k$与$\\tilde{\\gamma}_v$的求和均为1。\n",
    "\n",
    "根据上面的定义，可以对式子（2）信息增益进行推广为下式：\n",
    "$$\n",
    "\\large{\n",
    "\\begin{split}\n",
    "Gain(D,a) &= \\rho × Gain(\\tilde{D},a) \\\\\n",
    "          &= \\rho × \\left( Ent(\\tilde{D}) - \\sum_{v=1}^{V}\\tilde{\\gamma}_v Ent(\\tilde{D}^v)\\right)\n",
    "\\end{split}\n",
    "}\\tag{10}\n",
    "$$\n",
    "其中：\n",
    "$$\n",
    "\\large{\n",
    "\\begin{split}\n",
    "Ent(\\tilde{D}) = -\\sum_{k=1}^{y}\\tilde{p}_k\\log_2 \\tilde{p}_k\n",
    "\\end{split}\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于问题 2 ，若样本 $\\boldsymbol{x}$ 在划分属性 a 上取值已知，则将 $\\boldsymbol{x}$ 划入与其取值对应的子节点，且样本权值在子节点中保持为$\\omega_\\boldsymbol{x}$ 。若样本 $\\boldsymbol{x}$ 在划分属性 a 上取值未知，则将 $\\boldsymbol{x}$ 同时划入所有的子节点，其样本的权值在与属性 $a^v$ 对应的子节点中调整为 $\\tilde{\\gamma}_v·\\omega_{\\boldsymbol{x}}$;直观的来看就是，**让同一个样本以不同的概率划入不同的子节点中**。\n",
    "\n",
    "**C4.5**算法就是使用的上述解决方案。以上面的含有17个训练样本的有缺失值的样本集为例，以色泽为例，，可知：$\\tilde{D}=\\{2,3,4,6,7,8,9,10,11,12,14,15,1617\\}$ 的 14 个样例，其中含有 6 个正例，8个负例。那么可以计算出 $\\tilde{D}$ 的信息熵：\n",
    "$$\n",
    "\\large{\n",
    "\\begin{split}\n",
    "Ent(\\tilde{D}) &= -\\sum_{k=1}^{2}\\tilde{p}_k\\log_2 \\tilde{p}_k \\\\\n",
    "               &= -\\left(\\frac{6}{14}\\log_2\\frac{6}{14} + \\frac{8}{14}\\log_2\\frac{8}{14} \\right) = 0.985\n",
    "\\end{split}\n",
    "}\n",
    "$$\n",
    "\n",
    "令$\\tilde{D}^1, \\tilde{D}^2, \\tilde{D}^3$ 分别表示为青绿、乌黑、浅白，那么就有：\n",
    "$$\n",
    "\\large{\n",
    "\\begin{split}\n",
    "Ent(\\tilde{D}^1) &= -\\left(\\frac{2}{4}\\log_2\\frac{2}{4} + \\frac{2}{4}\\log_2\\frac{2}{4} \\right) = 1.000 \\\\\n",
    "Ent(\\tilde{D}^2) &= -\\left(\\frac{4}{6}\\log_2\\frac{4}{6} + \\frac{2}{6}\\log_2\\frac{2}{6} \\right) = 0.918 \\\\\n",
    "Ent(\\tilde{D}^3) &= -\\left(\\frac{0}{4}\\log_2\\frac{0}{4} + \\frac{4}{4}\\log_2\\frac{4}{4} \\right) = 0.000 \n",
    "\\end{split}\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "那么样本子集$\\tilde{D}$色泽的信息增益为：\n",
    "$$\n",
    "\\large{\n",
    "\\begin{split}\n",
    "Gain(\\tilde{D},色泽) &= Ent(\\tilde{D}) - \\sum_{v=1}^{3}\\tilde{\\gamma}_v Ent(\\tilde{D}^v) \\\\\n",
    "                  &= 0.985 - \\left(\\frac{4}{14}×1.000 + \\frac{6}{14}×0.918 + \\frac{4}{14}×0.000 +\\right)\\\\\n",
    "                    &= 0.306\n",
    "\\end{split}\n",
    "}\n",
    "$$\n",
    "\n",
    "那么数据集 D 在色泽上的信息增益为:\n",
    "$$\n",
    "\\large{\n",
    "\\begin{split}\n",
    "Gain(D,色泽) = \\rho × Gain(\\tilde{D},色泽) = \\frac{14}{17} × 0.306 = 0.252\n",
    "\\end{split}\n",
    "}\n",
    "$$\n",
    "\n",
    "类似的可以计算出其他属性的信息增益：Gain(D, 色泽) = 0.252、Gain(D, 根蒂) = 0.171、Gain(D, 敲声) = 0.145、Gain(D, 纹理) = 0.424、Gain(D, 脐部) = 0.289、Gain(D, 触感) = 0.006。\n",
    "\n",
    "可以看到纹理的信息增益最大，因此选择他作为划分属性。划分结果为编号{1,2,3,4,5,6,15}的样本进入纹理=清晰的分支，编号为{7,9,13,14,17}的样本进入纹理=稍糊的分支，编号为{11,12,16}的样本进入纹理=模糊的分支，且样本在各个分支点中的权重都为 1 。但是编号为{8}的样本含有缺失值，因此同时进入三个分支中，但在子节点的权重重新调整为$\\frac{7}{15},\\frac{5}{15},\\frac{3}{15}$。编号为{10}的与此类似。\n",
    "\n",
    "重复此过程最终得到如下的决策树：\n",
    "![image](https://ws2.sinaimg.cn/large/69d4185bly1fwg10cg74oj20l40e4wgb.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 多变量决策树"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "若我们把每个属性视为坐标空间中的一个坐标轴，则 d 个属性描述的样本就对应了 d 维空间的一个数据点，对样本分类则意味着在这个坐标空间中寻找不同类样本的分类边界。决策树所形成的分类边界有一个明显的特点：**轴平行**，即她的分类边界由若干个与坐标轴平行的分段组成。以下面的西瓜数据集为例：\n",
    "![image](https://wx2.sinaimg.cn/large/69d4185bly1fwg6p2nhqfj20b90ak3zf.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由上表生成的决策树如下图：\n",
    "![image](https://wx2.sinaimg.cn/large/69d4185bly1fwg89g4si1j20il0axab3.jpg)\n",
    "\n",
    "此时的决策树会非常复杂，由于要进行大量的属性测试，预测时间开销也会非常的大。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "若能使用斜线划分边界，如下图所示：\n",
    "![image](https://ws4.sinaimg.cn/large/69d4185bly1fwg8tjs0r3j207p08a0t0.jpg)\n",
    "\n",
    "则决策树模型将大为简化。$\\large{\\color{red}{多变量决策树（multivariate\\; decision \\; tree）}}$就能实现这样的斜划分，甚至是更复杂的划分决策树。以实现划分的多变量决策树为例，在此类决策树种，非叶节点不再是仅对某个属性，而是对**属性的线性组合**进行测试；换言之每个非叶节点是一个形如$\\sum_{i=1}^{d}\\omega_{i}a_i=t$的线性分类器，其中$\\omega_i$是属性$a_i$的权重，$\\omega_i$和 t 可在该节点所含有的样本和属性集上学得。于是传统的“单变量决策树（univariate decision tree）”不同，在多变量环境学习过程中，不是为每一个非叶节点寻找一个最优划分属性，而是试图建立一个合适的线性分类器。如下图的示例：\n",
    "![image](https://wx3.sinaimg.cn/large/69d4185bly1fwg8fvyp2wj20i709mwfe.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一些技巧\n",
    "【参考】\n",
    "- [cnblods - 决策树学习笔记整理](http://www.cnblogs.com/bourneli/archive/2013/03/15/2961568.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 停止条件\n",
    "决策树的构建过程是一个递归的过程，所以需要确定停止条件，否则过程将不会结束。\n",
    "\n",
    "一种最直观的方式是当每个子节点只有一种类型的记录时停止，但是这样往往会使得树的节点过多，导致过拟合问题（Overfitting）。\n",
    "\n",
    "另一种可行的方法是当前节点中的样本数低于一个最小的阀值，那么就停止分割，将节点中样本最多的类别对应的分类作为当前叶节点的分类。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 过拟合\n",
    "也就是该决策树对训练数据可以得到很低的错误率，但是运用到测试数据上却得到非常高的错误率。过渡拟合的原因有以下几点：\n",
    "- 噪音数据：训练数据中存在噪音数据，决策树的某些节点有噪音数据作为分割标准，导致决策树无法代表真实数据。\n",
    "- 缺少代表性数据：训练数据没有包含所有具有代表性的数据，导致某一类数据无法很好的匹配，这一点可以通过观察混淆矩阵（Confusion Matrix）分析得出。\n",
    "- 多重比较（Mulitple Comparition）：举个列子，股票分析师预测股票涨或跌。假设分析师都是靠随机猜测，也就是他们正确的概率是0.5。每一个人预测10次，那么预测正确的次数在8次或8次以上的概率只有5%左右，比较低。但是如果50个分析师，每个人预测10次，选择至少一个人得到8次或以上的人作为代表，那么概率为 0.9399，概率十分大，随着分析师人数的增加，概率无限接近1。但是，选出来的分析师其实是打酱油的，他对未来的预测不能做任何保证。上面这个例子就是多重比较。这一情况和决策树选取分割点类似，需要在每个变量的每一个值中选取一个作为分割的代表，所以选出一个噪音分割标准的概率是很大的。\n",
    "\n",
    "解决过拟合可以适应前面提到的预剪枝和后剪枝。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KF验证\n",
    "\n",
    "即K-Fold Cross Validation，首先计算出整体的决策树T，叶节点个数记作N，设i属于[1,N]。对每个i，使用 K-Fold Validataion 方法计算决策树，并裁剪到i个节点，计算错误率，最后求出平均错误率。这样可以用具有最小错误率对应的i作为最终决策树的大小，对原始决策树进行裁剪，得到最优决策树。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RF 随机森林\n",
    "\n",
    "Random Forest是用训练数据随机的计算出许多决策树，形成了一个森林。然后用这个森林对未知数据进行预测，选取投票最多的分类。实践证明，此算法的错误率得到了经一步的降低。这种方法背后的原理可以用“三个臭皮匠定一个诸葛亮”这句谚语来概括。一颗树预测正确的概率可能不高，但是集体预测正确的概率却很高。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 总结\n",
    "【参考】\n",
    "- [sklearn - Decision Trees](https://scikit-learn.org/stable/modules/tree.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 优缺点\n",
    "决策树的优点：\n",
    "- 容易理解和解释，树可以被可视化\n",
    "- 只需要很少的数据预处理。其他的技术则需要对数据进行归一化（normalisation）、创建 dummy 变量、移除空值等。\n",
    "- 可以用于处理数字和分类数据。其他技术通常只能用于分析其中的一种数据类型。\n",
    "- 可以处理多输出类问题\n",
    "- 可以使用统计测试来验证模型\n",
    "- 执行性能很好\n",
    "\n",
    "缺点：\n",
    "- 决策树学习器会创建出过于复杂的模型，导致其泛化性能不好。技术上可以通过剪枝、设置叶子节点样本最小数、树的最大深度等来解决\n",
    "- 决策树对数据的变化敏感，数据的一点小的变化可能会生成完全不同的模型。这个可以通过决策树集成来解决\n",
    "- 有些概念决策树很难去学习，如 XOR、奇偶校验（parity）或多路复用器（multiplexer）问题\n",
    "- 如何某些类占据了主导地位，决策树学习器创建的树会有偏差。因此需要平衡数据集来适应决策树。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如下的使用 sklearn 在鸢尾花数据上的可视化：\n",
    "![img](https://ws2.sinaimg.cn/large/69d4185bly1fx7nlkqdtqj20u30nhgpg.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实践中的建议\n",
    "以使用 sklearn 为基础。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 在数据含有大量特征时，决策树倾向于过拟合。获得特征与样本数正确的比例非常重要，因为决策树在少量的高维数据上更容易过拟合；\n",
    "- 考虑执行维度削减（如 PCA、ICA、特征选择），预先给你的树一个更好的找到更具判别性特征的机会；\n",
    "- 训练时通过`export`函数来可视化你的决策树。使用`max_depth=3`来初始化树的深度以获得数怎么样拟合你的数据的感受，之后在增加树的深度；\n",
    "- 在决策树中使用 `min_samples_split` 和 `min_samples_leaf` 来确保多个样本通知每个决策，通过控制哪一个分割会被考虑。一个很小的值通过意味着决策树会过拟合，然而大的值也会阻止决策树从数据中学习。尝试 `min_samples_leaf=5` 作为初始值是一个不错的选择，如果样本量变化比较大，可以设置为浮点值，表示样本所占的百分比。`min_samples_split` 可以设置任意小的值，`min_samples_leaf` 则可以保证每个子叶都有最小的值，从而避免低方差过拟合叶子几点在回归问题中。对于含有很少类别的分类问题，`min_samples_leaf=1` 经常是不错的选择。\n",
    "- 为了防止决策树偏向于具有只被地位的类别，在训练之前需要平衡你的数据集。类平衡可以通过在每一个类别中采样相同数量的样本数来完成，或者优先选择通过将每一个类的样本权重归一化为相同的值。也要注意基于权重的预剪枝标准，如`min_weight_fraction_leaf` ，更少偏向于具有主导的类别比没有考虑权重的标准，如 `min_samples_leaf` 。\n",
    "- 如果样本是加权的，这会更容易优化树的结构，通过基于权重的预剪枝标准，如`min_weight_fraction_leaf`，他确保每个子叶含有至少样本权重总和的一部分。\n",
    "- 所有的决策树在内部使用的是 `np.float32` 数组。如果训练数据不是这种格式，将会复制一份数据集。\n",
    "- 如果输入矩阵 X 是非常稀疏的，推荐在调用 fit 之前将其装换成稀疏的 `csc_matrix`，在预测之前将其装换成稀疏的 `csr_matrix`。当在大多数样本中，特征的值为 0 时，稀疏矩阵输入比密集矩阵输入的训练时间要快一个数量级。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "目录",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "303px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
