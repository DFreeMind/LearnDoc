{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 表情预测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文本分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>author</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1956967341</td>\n",
       "      <td>empty</td>\n",
       "      <td>xoshayzers</td>\n",
       "      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1956967666</td>\n",
       "      <td>sadness</td>\n",
       "      <td>wannamama</td>\n",
       "      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1956967696</td>\n",
       "      <td>sadness</td>\n",
       "      <td>coolfunky</td>\n",
       "      <td>Funeral ceremony...gloomy friday...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1956967789</td>\n",
       "      <td>enthusiasm</td>\n",
       "      <td>czareaquino</td>\n",
       "      <td>wants to hang out with friends SOON!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1956968416</td>\n",
       "      <td>neutral</td>\n",
       "      <td>xkilljoyx</td>\n",
       "      <td>@dannycastillo We want to trade with someone w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     tweet_id   sentiment       author  \\\n",
       "0  1956967341       empty   xoshayzers   \n",
       "1  1956967666     sadness    wannamama   \n",
       "2  1956967696     sadness    coolfunky   \n",
       "3  1956967789  enthusiasm  czareaquino   \n",
       "4  1956968416     neutral    xkilljoyx   \n",
       "\n",
       "                                             content  \n",
       "0  @tiffanylue i know  i was listenin to bad habi...  \n",
       "1  Layin n bed with a headache  ughhhh...waitin o...  \n",
       "2                Funeral ceremony...gloomy friday...  \n",
       "3               wants to hang out with friends SOON!  \n",
       "4  @dannycastillo We want to trade with someone w...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from keras.utils.data_utils import get_file\n",
    "import nb_utils\n",
    "\n",
    "emotion_csv = get_file('text_emotion.csv', \n",
    "                       'https://www.crowdflower.com/wp-content/uploads/2016/07/text_emotion.csv')\n",
    "emotion_df = pd.read_csv(emotion_csv)[:10000]\n",
    "print(len(emotion_df))\n",
    "emotion_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "worry         3115\n",
       "sadness       2216\n",
       "neutral       1857\n",
       "surprise       562\n",
       "hate           535\n",
       "happiness      469\n",
       "love           369\n",
       "relief         227\n",
       "fun            211\n",
       "empty          194\n",
       "enthusiasm     132\n",
       "boredom         69\n",
       "anger           44\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 简单分类器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "VOCAB_SIZE = 50000\n",
    "\n",
    "tfidf_vec = TfidfVectorizer(max_features=VOCAB_SIZE)\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "X = tfidf_vec.fit_transform(emotion_df['content'])\n",
    "y = label_encoder.fit_transform(emotion_df['sentiment'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.31454545454545457"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 贝叶斯方法\n",
    "bayes = MultinomialNB()\n",
    "bayes.fit(X_train, y_train)\n",
    "predictions = bayes.predict(X_test)\n",
    "precision_score(predictions, y_test, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lq/anaconda2/envs/cookbook/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sgd 0.3\n",
      "svm 0.314545454545\n",
      "random_forrest 0.281515151515\n"
     ]
    }
   ],
   "source": [
    "# 多种分类器\n",
    "classifiers = {'sgd': SGDClassifier(loss='hinge'),\n",
    "               'svm': SVC(),\n",
    "               'random_forrest': RandomForestClassifier()}\n",
    "\n",
    "for lbl, clf in classifiers.items():\n",
    "    clf.fit(X_train, y_train)\n",
    "    predictions = clf.predict(X_test)\n",
    "    print(lbl, precision_score(predictions, y_test, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import eye\n",
    "d = eye(len(tfidf_vec.vocabulary_))\n",
    "word_pred = bayes.predict_proba(d)\n",
    "\n",
    "inverse_vocab = {idx: word for word, idx in tfidf_vec.vocabulary_.items()}\n",
    "from collections import Counter, defaultdict\n",
    "by_cls = defaultdict(Counter)\n",
    "for word_idx, pred in enumerate(word_pred):\n",
    "    for class_idx, score in enumerate(pred):\n",
    "        cls = label_encoder.classes_[class_idx]\n",
    "        by_cls[cls][inverse_vocab[word_idx]] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anger : confuzzled fridaaaayyyyy transtelecom aaaaaaaaaaa motherfuck\n",
      "boredom : meanmillies ultra documentation deposits priecing\n",
      "empty : kimbermuffin shakeyourjunk fooled emuhleepee megabyte6\n",
      "enthusiasm : candy tatt tinabojo sarahbellum que\n",
      "fun : universal sexxieluv magners parachute knight\n",
      "happiness : 10th excellent dazzle chillin laughed\n",
      "hate : hate grrrr zomberellamcfox unfair dropped\n",
      "love : love sweetie lovely sayang loved\n",
      "neutral : ogberry rainy nerd plurk natsmith88\n",
      "relief : imagination samwilson1 clothes_w dhughesy allies\n",
      "sadness : past sadly sometimes sad rip\n",
      "surprise : ship wow sunburnt swpave juice\n",
      "worry : problem hope worried throat find\n"
     ]
    }
   ],
   "source": [
    "for k in by_cls:\n",
    "    words = [x[0] for x in by_cls[k].most_common(5)]\n",
    "    print(k, ':', ' '.join(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练深度模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10000, 161, 95)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 训练模型\n",
    "from itertools import chain\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "chars = list(sorted(set(chain(*emotion_df['content']))))\n",
    "char_to_idx = {ch: idx for idx, ch in enumerate(chars)}\n",
    "max_sequence_len = max(len(x) for x in emotion_df['content'])\n",
    "\n",
    "char_vectors = []\n",
    "for txt in emotion_df['content']:\n",
    "    vec = np.zeros((max_sequence_len, len(char_to_idx)))\n",
    "    vec[np.arange(len(txt)), [char_to_idx[ch] for ch in txt]] = 1\n",
    "    char_vectors.append(vec)\n",
    "print(len(char_vectors))\n",
    "char_vectors = np.asarray(char_vectors, dtype=np.float16)\n",
    "char_vectors = pad_sequences(char_vectors)\n",
    "labels = label_encoder.transform(emotion_df['sentiment'])\n",
    "\n",
    "\n",
    "def split(lst):\n",
    "    training_count = int(0.9 * len(char_vectors))\n",
    "    return lst[:training_count], lst[training_count:]\n",
    "\n",
    "training_char_vectors, test_char_vectors = split(char_vectors)\n",
    "training_labels, test_labels = split(labels)\n",
    "\n",
    "char_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           (None, 161, 95)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 156, 128)          73088     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 26, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 21, 256)           196864    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 3, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               98432     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 13)                1677      \n",
      "=================================================================\n",
      "Total params: 370,061\n",
      "Trainable params: 370,061\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense, Dropout, Merge, LSTM\n",
    "from keras.models import Model\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras import regularizers\n",
    "\n",
    "def create_char_cnn_model(num_chars, max_sequence_len, num_labels):\n",
    "    char_input = Input(shape=(max_sequence_len, num_chars), name='input')\n",
    "    \n",
    "    conv_1x = Conv1D(128, 6, activation='relu', padding='valid')(char_input)\n",
    "    max_pool_1x = MaxPooling1D(6)(conv_1x)\n",
    "    conv_2x = Conv1D(256, 6, activation='relu', padding='valid')(max_pool_1x)\n",
    "    max_pool_2x = MaxPooling1D(6)(conv_2x)\n",
    "\n",
    "    flatten = Flatten()(max_pool_2x)\n",
    "    dense = Dense(128, \n",
    "                  activation='relu',\n",
    "                  kernel_regularizer=regularizers.l2(0.01))(flatten)\n",
    "    preds = Dense(num_labels, activation='softmax')(dense)\n",
    "\n",
    "    model = Model(char_input, preds)\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['acc'])\n",
    "    return model\n",
    "\n",
    "char_cnn_model = create_char_cnn_model(len(char_to_idx), char_vectors.shape[1], len(label_encoder.classes_))\n",
    "char_cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "9000/9000 [==============================] - 8s 933us/step - loss: 3.8933 - acc: 0.2714\n",
      "Epoch 2/20\n",
      "9000/9000 [==============================] - 7s 813us/step - loss: 3.1871 - acc: 0.2884\n",
      "Epoch 3/20\n",
      "9000/9000 [==============================] - 7s 815us/step - loss: 2.8152 - acc: 0.3103\n",
      "Epoch 4/20\n",
      "9000/9000 [==============================] - 7s 816us/step - loss: 2.5609 - acc: 0.3150\n",
      "Epoch 5/20\n",
      "9000/9000 [==============================] - 7s 814us/step - loss: 2.3758 - acc: 0.3024\n",
      "Epoch 6/20\n",
      "9000/9000 [==============================] - 7s 819us/step - loss: 2.2400 - acc: 0.3094\n",
      "Epoch 7/20\n",
      "9000/9000 [==============================] - 7s 816us/step - loss: 2.1334 - acc: 0.3210\n",
      "Epoch 8/20\n",
      "9000/9000 [==============================] - 7s 816us/step - loss: 2.0755 - acc: 0.3081\n",
      "Epoch 9/20\n",
      "9000/9000 [==============================] - 7s 818us/step - loss: 2.0430 - acc: 0.3179\n",
      "Epoch 10/20\n",
      "9000/9000 [==============================] - 7s 815us/step - loss: 2.0176 - acc: 0.3051\n",
      "Epoch 11/20\n",
      "9000/9000 [==============================] - 7s 816us/step - loss: 1.9940 - acc: 0.3079\n",
      "Epoch 12/20\n",
      "9000/9000 [==============================] - 7s 816us/step - loss: 1.9836 - acc: 0.3210\n",
      "Epoch 13/20\n",
      "9000/9000 [==============================] - 7s 817us/step - loss: 1.9704 - acc: 0.3168\n",
      "Epoch 14/20\n",
      "9000/9000 [==============================] - 7s 816us/step - loss: 1.9681 - acc: 0.3223\n",
      "Epoch 15/20\n",
      "9000/9000 [==============================] - 7s 816us/step - loss: 1.9509 - acc: 0.3249\n",
      "Epoch 16/20\n",
      "9000/9000 [==============================] - 7s 817us/step - loss: 1.9537 - acc: 0.3239\n",
      "Epoch 17/20\n",
      "9000/9000 [==============================] - 7s 816us/step - loss: 1.9384 - acc: 0.3269\n",
      "Epoch 18/20\n",
      "9000/9000 [==============================] - 7s 819us/step - loss: 1.9340 - acc: 0.3249\n",
      "Epoch 19/20\n",
      "9000/9000 [==============================] - 7s 814us/step - loss: 1.9281 - acc: 0.3214\n",
      "Epoch 20/20\n",
      "9000/9000 [==============================] - 7s 817us/step - loss: 1.9140 - acc: 0.3367\n",
      "1000/1000 [==============================] - 0s 359us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.0439475917816163, 0.28199999999999997]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_cnn_model.fit(training_char_vectors, training_labels, epochs=20, batch_size=1024)\n",
    "char_cnn_model.evaluate(test_char_vectors, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              (None, 161, 95)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 157, 128)     60928       input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 156, 128)     73088       input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 155, 128)     85248       input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 31, 128)      0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 26, 128)      0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, 22, 128)      0           conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 31, 128)      0           max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 26, 128)      0           max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 22, 128)      0           max_pooling1d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 27, 128)      82048       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 21, 128)      98432       dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 16, 128)      114816      dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 5, 128)       0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 3, 128)       0           conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1D)  (None, 2, 128)       0           conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 5, 128)       0           max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 3, 128)       0           max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 2, 128)       0           max_pooling1d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 10, 128)      0           dropout_2[0][0]                  \n",
      "                                                                 dropout_4[0][0]                  \n",
      "                                                                 dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 10, 128)      0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 1280)         0           dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 128)          163968      flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 13)           1677        dense_8[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 680,205\n",
      "Trainable params: 680,205\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense, Dropout, Merge, LSTM\n",
    "from keras.models import Model\n",
    "from keras.layers.merge import Concatenate\n",
    "\n",
    "def create_char_cnn_model(num_chars, max_sequence_len, num_labels):\n",
    "    char_input = Input(shape=(max_sequence_len, num_chars), name='input')\n",
    "    \n",
    "    layers = []\n",
    "    for window in (5, 6, 7):\n",
    "        conv_1x = Conv1D(128, window, activation='relu', padding='valid')(char_input)\n",
    "        max_pool_1x = MaxPooling1D(window)(conv_1x)\n",
    "        dropout_1x = Dropout(0.3)(max_pool_1x)\n",
    "        conv_2x = Conv1D(128, window, activation='relu', padding='valid')(dropout_1x)\n",
    "        max_pool_2x = MaxPooling1D(window)(conv_2x)\n",
    "        dropout_2x = Dropout(0.3)(max_pool_2x)\n",
    "        layers.append(dropout_2x)\n",
    "\n",
    "    if len(layers) > 1:\n",
    "        merged = Concatenate(axis=1)(layers)\n",
    "    else:\n",
    "        merged = layers[0]\n",
    "\n",
    "    dropout = Dropout(0.3)(merged)\n",
    "    \n",
    "    flatten = Flatten()(dropout)\n",
    "    dense = Dense(128, activation='relu')(flatten)\n",
    "    preds = Dense(num_labels, activation='softmax')(dense)\n",
    "\n",
    "    model = Model(char_input, preds)\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['acc'])\n",
    "    return model\n",
    "\n",
    "char_cnn_model = create_char_cnn_model(len(char_to_idx), char_vectors.shape[1], len(label_encoder.classes_))\n",
    "char_cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "9000/9000 [==============================] - 19s 2ms/step - loss: 2.1509 - acc: 0.2611\n",
      "Epoch 2/20\n",
      "9000/9000 [==============================] - 19s 2ms/step - loss: 1.9839 - acc: 0.2949\n",
      "Epoch 3/20\n",
      "9000/9000 [==============================] - 19s 2ms/step - loss: 1.9562 - acc: 0.3138\n",
      "Epoch 4/20\n",
      "9000/9000 [==============================] - 19s 2ms/step - loss: 1.9577 - acc: 0.3017\n",
      "Epoch 5/20\n",
      "9000/9000 [==============================] - 19s 2ms/step - loss: 1.9546 - acc: 0.3053\n",
      "Epoch 6/20\n",
      "9000/9000 [==============================] - 19s 2ms/step - loss: 1.9500 - acc: 0.3166\n",
      "Epoch 7/20\n",
      "9000/9000 [==============================] - 19s 2ms/step - loss: 1.9383 - acc: 0.3153\n",
      "Epoch 8/20\n",
      "9000/9000 [==============================] - 19s 2ms/step - loss: 1.9355 - acc: 0.3190\n",
      "Epoch 9/20\n",
      "9000/9000 [==============================] - 19s 2ms/step - loss: 1.9236 - acc: 0.3194\n",
      "Epoch 10/20\n",
      "9000/9000 [==============================] - 19s 2ms/step - loss: 1.9345 - acc: 0.3164\n",
      "Epoch 11/20\n",
      "9000/9000 [==============================] - 19s 2ms/step - loss: 1.9174 - acc: 0.3266\n",
      "Epoch 12/20\n",
      "9000/9000 [==============================] - 19s 2ms/step - loss: 1.9061 - acc: 0.3261\n",
      "Epoch 13/20\n",
      "9000/9000 [==============================] - 19s 2ms/step - loss: 1.9140 - acc: 0.3272\n",
      "Epoch 14/20\n",
      "9000/9000 [==============================] - 19s 2ms/step - loss: 1.9072 - acc: 0.3306\n",
      "Epoch 15/20\n",
      "9000/9000 [==============================] - 19s 2ms/step - loss: 1.8873 - acc: 0.3310\n",
      "Epoch 16/20\n",
      "9000/9000 [==============================] - 19s 2ms/step - loss: 1.8863 - acc: 0.3462\n",
      "Epoch 17/20\n",
      "9000/9000 [==============================] - 19s 2ms/step - loss: 1.8765 - acc: 0.3412\n",
      "Epoch 18/20\n",
      "9000/9000 [==============================] - 19s 2ms/step - loss: 1.8772 - acc: 0.3486\n",
      "Epoch 19/20\n",
      "9000/9000 [==============================] - 19s 2ms/step - loss: 1.8536 - acc: 0.3566\n",
      "Epoch 20/20\n",
      "9000/9000 [==============================] - 19s 2ms/step - loss: 1.8432 - acc: 0.3612\n",
      "1000/1000 [==============================] - 1s 892us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.9796250667572022, 0.29899999999999999]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_cnn_model.fit(training_char_vectors, training_labels, epochs=20, batch_size=1024)\n",
    "char_cnn_model.evaluate(test_char_vectors, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特征化与数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/lq/.keras/datasets/https_s3.amazonaws.com_dl4j_distribution_GoogleNews_vectors_negative300.bin.gz\n",
      "/home/lq/.keras/datasets/https_s3.amazonaws.com_dl4j_distribution_GoogleNews_vectors_negative300.bin\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import one_hot\n",
    "\n",
    "VOCAB_SIZE = 50000\n",
    "tokenizer = Tokenizer(num_words=VOCAB_SIZE)\n",
    "tokenizer.fit_on_texts(emotion_df['content'])\n",
    "\n",
    "# This may take a while to load\n",
    "w2v, idf = nb_utils.load_w2v(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.texts_to_sequences(emotion_df['content'])\n",
    "tokens = pad_sequences(tokens)\n",
    "\n",
    "training_count = int(0.9 * len(tokens))\n",
    "training_tokens, training_labels = tokens[:training_count], labels[:training_count]\n",
    "test_tokens, test_labels = tokens[training_count:], labels[training_count:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "message (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "message_vec/embedding (Embeddin (None, None, 300)    15000000    message[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "masking_1 (Masking)             (None, None, 300)    0           message_vec/embedding[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "message_idf/embedding (Embeddin (None, None, 1)      50000       message[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "combine_and_sum (Lambda)        (None, 300)          0           masking_1[0][0]                  \n",
      "                                                                 message_idf/embedding[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128)          38528       combine_and_sum[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 13)           1677        dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 15,090,205\n",
      "Trainable params: 40,205\n",
      "Non-trainable params: 15,050,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import layers, models\n",
    "import keras.backend as K\n",
    "\n",
    "\n",
    "def make_embedding(name, vocab_size, embedding_size, weights=None, mask_zero=True):\n",
    "    if weights is not None:\n",
    "        return layers.Embedding(mask_zero=mask_zero, input_dim=vocab_size, \n",
    "                                output_dim=weights.shape[1], \n",
    "                                weights=[weights], trainable=False, \n",
    "                                name='%s/embedding' % name)\n",
    "    else:\n",
    "        return layers.Embedding(mask_zero=mask_zero, input_dim=vocab_size, \n",
    "                                output_dim=embedding_size,\n",
    "                                name='%s/embedding' % name)\n",
    "\n",
    "def create_unigram_model(vocab_size, embedding_size=None, embedding_weights=None, idf_weights=None):\n",
    "    assert not (embedding_size is None and embedding_weights is None)\n",
    "    message = layers.Input(shape=(None,), dtype='int32', name='message')\n",
    "    \n",
    "    embedding = make_embedding('message_vec', vocab_size, embedding_size, embedding_weights)\n",
    "    idf = make_embedding('message_idf', vocab_size, embedding_size, idf_weights)\n",
    "\n",
    "    mask = layers.Masking(mask_value=0)\n",
    "    def _combine_and_sum(args):\n",
    "        embedding, idf = args\n",
    "        return K.sum(embedding * K.abs(idf), axis=1)\n",
    "\n",
    "    sum_layer = layers.Lambda(_combine_and_sum, name='combine_and_sum')\n",
    "    sum_msg = sum_layer([mask(embedding(message)), idf(message)])\n",
    "    fc1 = layers.Dense(units=128, activation='relu')(sum_msg)\n",
    "    categories = layers.Dense(units=len(label_encoder.classes_), activation='softmax')(fc1)\n",
    "    \n",
    "    model = models.Model(\n",
    "        inputs=[message],\n",
    "        outputs=categories,\n",
    "    )\n",
    "    \n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "unigram_model = create_unigram_model(vocab_size=VOCAB_SIZE,\n",
    "                                     embedding_weights=w2v,\n",
    "                                     idf_weights=idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "9000/9000 [==============================] - 1s 164us/step - loss: 2.8315 - acc: 0.2608\n",
      "Epoch 2/10\n",
      "9000/9000 [==============================] - 1s 128us/step - loss: 1.9567 - acc: 0.3322\n",
      "Epoch 3/10\n",
      "9000/9000 [==============================] - 1s 134us/step - loss: 1.8211 - acc: 0.3710\n",
      "Epoch 4/10\n",
      "9000/9000 [==============================] - 1s 132us/step - loss: 1.7184 - acc: 0.4030\n",
      "Epoch 5/10\n",
      "9000/9000 [==============================] - 1s 134us/step - loss: 1.6181 - acc: 0.4393\n",
      "Epoch 6/10\n",
      "9000/9000 [==============================] - 1s 132us/step - loss: 1.5268 - acc: 0.4672\n",
      "Epoch 7/10\n",
      "9000/9000 [==============================] - 1s 133us/step - loss: 1.4438 - acc: 0.4980\n",
      "Epoch 8/10\n",
      "9000/9000 [==============================] - 1s 134us/step - loss: 1.3650 - acc: 0.5241\n",
      "Epoch 9/10\n",
      "9000/9000 [==============================] - 1s 134us/step - loss: 1.2868 - acc: 0.5461\n",
      "Epoch 10/10\n",
      "9000/9000 [==============================] - 1s 134us/step - loss: 1.2224 - acc: 0.5710\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.8229330902099607, 0.30199999999999999]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_model.fit(training_tokens, training_labels, epochs=10)\n",
    "unigram_model.evaluate(test_tokens, test_labels, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 内嵌学习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "message (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "message_vec/embedding (Embeddin (None, None, 25)     1250000     message[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "masking_2 (Masking)             (None, None, 25)     0           message_vec/embedding[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "message_idf/embedding (Embeddin (None, None, 25)     1250000     message[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "combine_and_sum (Lambda)        (None, 25)           0           masking_2[0][0]                  \n",
      "                                                                 message_idf/embedding[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 128)          3328        combine_and_sum[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 13)           1677        dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 2,505,005\n",
      "Trainable params: 2,505,005\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "learned_embeddings_model = create_unigram_model(vocab_size=VOCAB_SIZE, embedding_size=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "9000/9000 [==============================] - 2s 211us/step - loss: 2.1166 - acc: 0.3092\n",
      "Epoch 2/10\n",
      "9000/9000 [==============================] - 2s 218us/step - loss: 1.9381 - acc: 0.3137\n",
      "Epoch 3/10\n",
      "9000/9000 [==============================] - 2s 211us/step - loss: 1.9166 - acc: 0.3154\n",
      "Epoch 4/10\n",
      "9000/9000 [==============================] - 2s 216us/step - loss: 1.8914 - acc: 0.3220\n",
      "Epoch 5/10\n",
      "9000/9000 [==============================] - 2s 209us/step - loss: 1.8607 - acc: 0.3416\n",
      "Epoch 6/10\n",
      "9000/9000 [==============================] - 2s 195us/step - loss: 1.8211 - acc: 0.3556\n",
      "Epoch 7/10\n",
      "9000/9000 [==============================] - 2s 195us/step - loss: 1.7729 - acc: 0.3817\n",
      "Epoch 8/10\n",
      "9000/9000 [==============================] - 2s 195us/step - loss: 1.7149 - acc: 0.4159\n",
      "Epoch 9/10\n",
      "9000/9000 [==============================] - 2s 195us/step - loss: 1.6498 - acc: 0.4463\n",
      "Epoch 10/10\n",
      "9000/9000 [==============================] - 2s 199us/step - loss: 1.5821 - acc: 0.4778\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7efcb594ce80>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learned_embeddings_model.fit(training_tokens, training_labels, epochs=10, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.9363989353179931, 0.32100000000000001]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learned_embeddings_model.evaluate(test_tokens, test_labels, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 更复杂模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_model(vocab_size, embedding_size=None, embedding_weights=None):\n",
    "    message = layers.Input(shape=(None,), dtype='int32', name='title')\n",
    "    \n",
    "    # The convolution layer in keras does not support masking, so we just allow\n",
    "    # the embedding layer to learn an explicit value.\n",
    "    embedding = make_embedding('message_vec', vocab_size, embedding_size, embedding_weights,\n",
    "                              mask_zero=False)\n",
    "\n",
    "    def _combine_sum(v):\n",
    "        return K.sum(v, axis=1)\n",
    "\n",
    "    cnn_1 = layers.Convolution1D(128, 3)\n",
    "    cnn_2 = layers.Convolution1D(128, 3)\n",
    "    cnn_3 = layers.Convolution1D(128, 3)\n",
    "    \n",
    "    global_pool = layers.GlobalMaxPooling1D()\n",
    "    local_pool = layers.MaxPooling1D(strides=1, pool_size=3)\n",
    "\n",
    "    cnn_encoding = global_pool(cnn_3(local_pool(cnn_2(local_pool(cnn_1(embedding(message)))))))\n",
    "    fc1 = layers.Dense(units=128, activation='elu')(cnn_encoding)\n",
    "    categories = layers.Dense(units=len(label_encoder.classes_), activation='softmax')(fc1)\n",
    "    model = models.Model(\n",
    "        inputs=[message],\n",
    "        outputs=[categories],\n",
    "    )\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "title (InputLayer)              (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "message_vec/embedding (Embeddin (None, None, 300)    15000000    title[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, None, 128)    115328      message_vec/embedding[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, None, 128)    0           conv1d_1[0][0]                   \n",
      "                                                                 conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, None, 128)    49280       max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, None, 128)    49280       max_pooling1d_1[1][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 128)          0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 128)          16512       global_max_pooling1d_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 13)           1677        dense_5[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 15,232,077\n",
      "Trainable params: 232,077\n",
      "Non-trainable params: 15,000,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn_model = create_cnn_model(VOCAB_SIZE, embedding_weights=w2v)\n",
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "9000/9000 [==============================] - 4s 453us/step - loss: 1.9469 - acc: 0.3133\n",
      "Epoch 2/10\n",
      "9000/9000 [==============================] - 4s 449us/step - loss: 1.8099 - acc: 0.3550\n",
      "Epoch 3/10\n",
      "9000/9000 [==============================] - 4s 452us/step - loss: 1.6709 - acc: 0.4039\n",
      "Epoch 4/10\n",
      "9000/9000 [==============================] - 4s 451us/step - loss: 1.4565 - acc: 0.4749\n",
      "Epoch 5/10\n",
      "9000/9000 [==============================] - 4s 451us/step - loss: 1.1898 - acc: 0.5810\n",
      "Epoch 6/10\n",
      "9000/9000 [==============================] - 4s 452us/step - loss: 0.9398 - acc: 0.6729\n",
      "Epoch 7/10\n",
      "9000/9000 [==============================] - 4s 451us/step - loss: 0.7320 - acc: 0.7531\n",
      "Epoch 8/10\n",
      "9000/9000 [==============================] - 4s 451us/step - loss: 0.6009 - acc: 0.7974\n",
      "Epoch 9/10\n",
      "9000/9000 [==============================] - 4s 454us/step - loss: 0.5101 - acc: 0.8291\n",
      "Epoch 10/10\n",
      "9000/9000 [==============================] - 4s 452us/step - loss: 0.4566 - acc: 0.8474\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7efbd44f04a8>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_model.fit(training_tokens, training_labels, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 153us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[4.8914266662597656, 0.252]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_model.evaluate(test_tokens, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lstm_model(vocab_size, embedding_size=None, embedding_weights=None):\n",
    "    message = layers.Input(shape=(None,), dtype='int32', name='title')\n",
    "    embedding = make_embedding('message_vec', vocab_size, embedding_size, embedding_weights)(message)\n",
    "\n",
    "    lstm_1 = layers.LSTM(units=128, return_sequences=False)(embedding)\n",
    "#     lstm_2 = layers.LSTM(units=128, return_sequences=False)(lstm_1)\n",
    "    category = layers.Dense(units=len(label_encoder.classes_), activation='softmax')(lstm_1)\n",
    "    \n",
    "    model = models.Model(\n",
    "        inputs=[message],\n",
    "        outputs=[category],\n",
    "    )\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "title (InputLayer)           (None, None)              0         \n",
      "_________________________________________________________________\n",
      "message_vec/embedding (Embed (None, None, 300)         15000000  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               219648    \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 13)                1677      \n",
      "=================================================================\n",
      "Total params: 15,221,325\n",
      "Trainable params: 221,325\n",
      "Non-trainable params: 15,000,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lstm_model = create_lstm_model(VOCAB_SIZE, embedding_weights=w2v)\n",
    "lstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "9000/9000 [==============================] - 5s 560us/step - loss: 1.9808 - acc: 0.3198\n",
      "Epoch 2/10\n",
      "9000/9000 [==============================] - 5s 550us/step - loss: 1.8934 - acc: 0.3411\n",
      "Epoch 3/10\n",
      "9000/9000 [==============================] - 5s 545us/step - loss: 1.8445 - acc: 0.3577\n",
      "Epoch 4/10\n",
      "9000/9000 [==============================] - 5s 547us/step - loss: 1.8098 - acc: 0.3654\n",
      "Epoch 5/10\n",
      "9000/9000 [==============================] - 5s 547us/step - loss: 1.7734 - acc: 0.3734\n",
      "Epoch 6/10\n",
      "9000/9000 [==============================] - 5s 549us/step - loss: 1.7537 - acc: 0.3816\n",
      "Epoch 7/10\n",
      "9000/9000 [==============================] - 5s 548us/step - loss: 1.7216 - acc: 0.3936\n",
      "Epoch 8/10\n",
      "9000/9000 [==============================] - 5s 548us/step - loss: 1.7005 - acc: 0.3976\n",
      "Epoch 9/10\n",
      "9000/9000 [==============================] - 5s 557us/step - loss: 1.6700 - acc: 0.4136\n",
      "Epoch 10/10\n",
      "9000/9000 [==============================] - 5s 548us/step - loss: 1.6445 - acc: 0.4190\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7efcb6463710>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_model.fit(training_tokens, training_labels, epochs=10, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 369us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.9011550903320313, 0.34499999999999997]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_model.evaluate(test_tokens, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型比较"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'char_cnn_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-abd77240cb26>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m predictions = {\n\u001b[1;32m      2\u001b[0m     \u001b[0;34m'lstm'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlstm_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_tokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;34m'char_cnn'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mchar_cnn_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_char_vectors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;34m'cnn'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcnn_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_tokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m'unigram'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0munigram_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_tokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'char_cnn_model' is not defined"
     ]
    }
   ],
   "source": [
    "predictions = {\n",
    "    'lstm': lstm_model.predict(test_tokens[:100]),\n",
    "    'char_cnn': char_cnn_model.predict(test_char_vectors[:100]),\n",
    "    'cnn': cnn_model.predict(test_tokens[:100]),\n",
    "    'unigram': unigram_model.predict(test_tokens[:100]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a dataframe just for test data\n",
    "\n",
    "pd.options.display.max_colwidth = 128\n",
    "test_df = emotion_df[training_count:training_count+100].reset_index()\n",
    "eval_df = pd.DataFrame({\n",
    "    'content': test_df['content'],\n",
    "    'true': test_df['sentiment'],\n",
    "    'lstm': [label_encoder.classes_[np.argmax(x)] for x in predictions['lstm']],\n",
    "    'cnn': [label_encoder.classes_[np.argmax(x)] for x in predictions['cnn']],\n",
    "    'char_cnn': [label_encoder.classes_[np.argmax(x)] for x in predictions['char_cnn']],    \n",
    "    'unigram': [label_encoder.classes_[np.argmax(x)] for x in predictions['unigram']],\n",
    "})\n",
    "eval_df = eval_df[['content', 'true', 'lstm', 'cnn', 'char_cnn', 'unigram']]\n",
    "eval_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df[eval_df['lstm'] != eval_df['true']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twitter 分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import twitter\n",
    "import emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill these in!\n",
    "\n",
    "CONSUMER_KEY = 'xbMuxcJpRTiVGt2C2EYnA'\n",
    "CONSUMER_SECRET = '2DbQTsvIptkPTdaUcos8DDvQH9fzO0hNjJpUT2uVzQ'\n",
    "ACCESS_TOKEN = '7319442-EDm4CPxL7W4KkZcGWRMJNVHp88W5OH9vgblu898fg'\n",
    "ACCESS_SECRET = '5ZxJSbqXhG7uhgXzTFWf9XhkfsxxinlPRXyDTzbA9w'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = twitter.Twitter(\n",
    "    auth=twitter.OAuth(\n",
    "        consumer_key=CONSUMER_KEY,\n",
    "        consumer_secret=CONSUMER_SECRET,\n",
    "        token=ACCESS_TOKEN,\n",
    "        token_secret=ACCESS_SECRET,\n",
    "    ))\n",
    "\n",
    "stream = twitter.TwitterStream(\n",
    "    auth=twitter.OAuth(\n",
    "        consumer_key=CONSUMER_KEY,\n",
    "        consumer_secret=CONSUMER_SECRET,\n",
    "        token=ACCESS_TOKEN,\n",
    "        token_secret=ACCESS_SECRET,\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "def has_emoji(tweet):\n",
    "    if tweet.get('lang') != 'en':\n",
    "        return False\n",
    "    return any(ch for ch in tweet.get('text', '') if ch in emoji.UNICODE_EMOJI)\n",
    "\n",
    "%time st = list(itertools.islice(filter(has_emoji, stream.statuses.sample()), 0, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(st), [t.get('text', None) for t in st][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter 表情分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import twitter\n",
    "import emoji\n",
    "import itertools\n",
    "import pandas as pd\n",
    "from itertools import chain\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import one_hot\n",
    "import keras.callbacks\n",
    "import json\n",
    "\n",
    "import os\n",
    "import nb_utils\n",
    "from keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "from keras.layers import Merge, LSTM, Embedding, GlobalMaxPooling1D\n",
    "from keras.models import Model\n",
    "from keras.layers.merge import Concatenate, Average\n",
    "\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill these in!\n",
    "\n",
    "CONSUMER_KEY = 'xbMuxcJpRTiVGt2C2EYnA'\n",
    "CONSUMER_SECRET = '2DbQTsvIptkPTdaUcos8DDvQH9fzO0hNjJpUT2uVzQ'\n",
    "ACCESS_TOKEN = '7319442-EDm4CPxL7W4KkZcGWRMJNVHp88W5OH9vgblu898fg'\n",
    "ACCESS_SECRET = '5ZxJSbqXhG7uhgXzTFWf9XhkfsxxinlPRXyDTzbA9w'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "URLError",
     "evalue": "<urlopen error [Errno 110] Connection timed out>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda2/envs/cookbook/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36mdo_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1317\u001b[0m                 h.request(req.get_method(), req.selector, req.data, headers,\n\u001b[0;32m-> 1318\u001b[0;31m                           encode_chunked=req.has_header('Transfer-encoding'))\n\u001b[0m\u001b[1;32m   1319\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# timeout error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/cookbook/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1238\u001b[0m         \u001b[0;34m\"\"\"Send a complete request to the server.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/cookbook/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36m_send_request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1284\u001b[0m             \u001b[0mbody\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'body'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1285\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendheaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/cookbook/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mendheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1233\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mCannotSendHeader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1234\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_body\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/cookbook/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36m_send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1025\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/cookbook/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    963\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_open\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 964\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    965\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/cookbook/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1392\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/cookbook/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    935\u001b[0m         self.sock = self._create_connection(\n\u001b[0;32m--> 936\u001b[0;31m             (self.host,self.port), self.timeout, self.source_address)\n\u001b[0m\u001b[1;32m    937\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetsockopt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIPPROTO_TCP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTCP_NODELAY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/cookbook/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address)\u001b[0m\n\u001b[1;32m    721\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0merr\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/cookbook/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address)\u001b[0m\n\u001b[1;32m    712\u001b[0m                 \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_address\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 713\u001b[0;31m             \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msa\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    714\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTimeoutError\u001b[0m: [Errno 110] Connection timed out",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mURLError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-3fe3de2055e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mstatus_stream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtwitter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTwitterStream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatuses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mislice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus_stream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda2/envs/cookbook/lib/python3.6/site-packages/twitter/api.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    332\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_response_with_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_handle_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/cookbook/lib/python3.6/site-packages/twitter/stream.py\u001b[0m in \u001b[0;36m_handle_response\u001b[0;34m(self, req, uri, arg_data, _timeout)\u001b[0m\n\u001b[1;32m    286\u001b[0m                 return handle_stream_response(\n\u001b[1;32m    287\u001b[0m                     \u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m                     _timeout or timeout, heartbeat_timeout)\n\u001b[0m\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m         TwitterCall.__init__(\n",
      "\u001b[0;32m~/anaconda2/envs/cookbook/lib/python3.6/site-packages/twitter/stream.py\u001b[0m in \u001b[0;36mhandle_stream_response\u001b[0;34m(req, uri, arg_data, block, timeout, heartbeat_timeout)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mhandle_stream_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheartbeat_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murllib_request\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0murllib_error\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTwitterHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/cookbook/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/cookbook/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    524\u001b[0m             \u001b[0mreq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    527\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m         \u001b[0;31m# post-process response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/cookbook/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(self, req, data)\u001b[0m\n\u001b[1;32m    542\u001b[0m         \u001b[0mprotocol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m         result = self._call_chain(self.handle_open, protocol, protocol +\n\u001b[0;32m--> 544\u001b[0;31m                                   '_open', req)\n\u001b[0m\u001b[1;32m    545\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/cookbook/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    502\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    505\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/cookbook/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36mhttps_open\u001b[0;34m(self, req)\u001b[0m\n\u001b[1;32m   1359\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mhttps_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m             return self.do_open(http.client.HTTPSConnection, req,\n\u001b[0;32m-> 1361\u001b[0;31m                 context=self._context, check_hostname=self._check_hostname)\n\u001b[0m\u001b[1;32m   1362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m         \u001b[0mhttps_request\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAbstractHTTPHandler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_request_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/cookbook/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36mdo_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1318\u001b[0m                           encode_chunked=req.has_header('Transfer-encoding'))\n\u001b[1;32m   1319\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# timeout error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1320\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mURLError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1321\u001b[0m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mURLError\u001b[0m: <urlopen error [Errno 110] Connection timed out>"
     ]
    }
   ],
   "source": [
    "auth=twitter.OAuth(\n",
    "    consumer_key=CONSUMER_KEY,\n",
    "    consumer_secret=CONSUMER_SECRET,\n",
    "    token=ACCESS_TOKEN,\n",
    "    token_secret=ACCESS_SECRET,\n",
    ")\n",
    "\n",
    "status_stream = twitter.TwitterStream(auth=auth).statuses\n",
    "\n",
    "[x['text'] for x in itertools.islice(status_stream.sample(), 0, 5) if x.get('text')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status_stream = twitter.TwitterStream(auth=auth).statuses\n",
    "\n",
    "def english_has_emoji(tweet):\n",
    "    if tweet.get('lang') != 'en':\n",
    "        return False\n",
    "    return any(ch for ch in tweet.get('text', '') if ch in emoji.UNICODE_EMOJI)\n",
    "\n",
    "%time tweets = list(itertools.islice(filter(english_has_emoji, status_stream.sample()), 0, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stripped = []\n",
    "for tweet in tweets:\n",
    "    text = tweet['text']\n",
    "    emojis = {ch for ch in text if ch in emoji.UNICODE_EMOJI}\n",
    "    if len(emojis) == 1:\n",
    "        emoiji = emojis.pop()\n",
    "        text = ''.join(ch for ch in text if ch != emoiji)\n",
    "        stripped.append((text, emoiji))\n",
    "len(stripped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用 CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tweets = pd.read_csv('data/emojis.csv')\n",
    "all_tweets['emoji'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = all_tweets.groupby('emoji').filter(lambda c:len(c) > 1000)\n",
    "tweets['emoji'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(tweets['text'], key=lambda t:len(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = list(sorted(set(chain(*tweets['text']))))\n",
    "char_to_idx = {ch: idx for idx, ch in enumerate(chars)}\n",
    "max_sequence_len = max(len(x) for x in tweets['text'])\n",
    "\n",
    "emojis = list(sorted(set(tweets['emoji'])))\n",
    "emoji_to_idx = {em: idx for idx, em in enumerate(emojis)}\n",
    "emojis[:10]\n",
    "\n",
    "train_tweets, test_tweets = train_test_split(tweets, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(tweets, batch_size):\n",
    "    while True:\n",
    "        if batch_size is None:\n",
    "            batch = tweets\n",
    "            batch_size = batch.shape[0]\n",
    "        else:\n",
    "            batch = tweets.sample(batch_size)\n",
    "        X = np.zeros((batch_size, max_sequence_len, len(chars)))\n",
    "        y = np.zeros((batch_size,))\n",
    "        for row_idx, (_, row) in enumerate(batch.iterrows()):\n",
    "            y[row_idx] = emoji_to_idx[row['emoji']]\n",
    "            for ch_idx, ch in enumerate(row['text']):\n",
    "                X[row_idx, ch_idx, char_to_idx[ch]] = 1\n",
    "        yield X, y\n",
    "\n",
    "next(data_generator(tweets, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_char_cnn_model(num_chars, max_sequence_len, num_labels):\n",
    "    char_input = Input(shape=(max_sequence_len, num_chars), name='char_cnn_input')\n",
    "    \n",
    "    conv_1x = Conv1D(128, 6, activation='relu', padding='valid')(char_input)\n",
    "    max_pool_1x = MaxPooling1D(4)(conv_1x)\n",
    "    conv_2x = Conv1D(256, 6, activation='relu', padding='valid')(max_pool_1x)\n",
    "    max_pool_2x = MaxPooling1D(4)(conv_2x)\n",
    "\n",
    "    flatten = Flatten()(max_pool_2x)\n",
    "    dense = Dense(128, activation='relu')(flatten)\n",
    "    preds = Dense(num_labels, activation='softmax', name='char_cnn_predictions')(dense)\n",
    "\n",
    "    model = Model(char_input, preds)\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['acc'])\n",
    "    return model\n",
    "\n",
    "char_cnn_model = create_char_cnn_model(len(char_to_idx), max_sequence_len, len(emojis))\n",
    "char_cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early = keras.callbacks.EarlyStopping(monitor='loss',\n",
    "                              min_delta=0.03,\n",
    "                              patience=2,\n",
    "                              verbose=0, mode='auto')\n",
    "\n",
    "BATCH_SIZE = 512\n",
    "char_cnn_model.fit_generator(\n",
    "    data_generator(train_tweets, batch_size=BATCH_SIZE),\n",
    "    epochs=20,\n",
    "    steps_per_epoch=len(train_tweets) / BATCH_SIZE,\n",
    "    verbose=2,\n",
    "    callbacks=[early]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_cnn_model.evaluate_generator(\n",
    "    data_generator(test_tweets, batch_size=BATCH_SIZE),\n",
    "    steps=len(test_tweets) / BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./zoo/07/emoji_chars.json', 'w') as fout:\n",
    "    json.dump({\n",
    "        'emojis': ''.join(emojis),\n",
    "        'char_to_idx': char_to_idx,\n",
    "        'max_sequence_len': max_sequence_len,\n",
    "    }, fout)\n",
    "char_cnn_model.save('./zoo/07/char_cnn_model.h5')\n",
    "char_cnn_model.save_weights('./zoo/07/char_cnn_model_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = 128\n",
    "inspect_tweets = test_tweets.sample(100)\n",
    "predicted = char_cnn_model.predict_generator(data_generator(inspect_tweets, batch_size=None), steps=1)\n",
    "show = pd.DataFrame({\n",
    "    'text': inspect_tweets['text'],\n",
    "    'true': inspect_tweets['emoji'],\n",
    "    'pred': [emojis[np.argmax(x)] for x in predicted],\n",
    "})\n",
    "show = show[['text', 'true', 'pred']]\n",
    "show.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense, Dropout, Merge, LSTM\n",
    "from keras.models import Model\n",
    "from keras.layers.merge import Concatenate\n",
    "\n",
    "def create_char_cnn_model2(num_chars, max_sequence_len, num_labels, drop_out=0.25):\n",
    "    char_input = Input(shape=(max_sequence_len, num_chars), name='char_cnn_input')\n",
    "    \n",
    "    layers = []\n",
    "    for window in (4, 5, 6):\n",
    "        conv_1x = Conv1D(128, window, activation='relu', padding='valid')(char_input)\n",
    "        max_pool_1x = MaxPooling1D(4)(conv_1x)\n",
    "        dropout_1x = Dropout(drop_out)(max_pool_1x)\n",
    "        conv_2x = Conv1D(256, window, activation='relu', padding='valid')(dropout_1x)\n",
    "        max_pool_2x = MaxPooling1D(4)(conv_2x)\n",
    "        dropout_2x = Dropout(drop_out)(max_pool_2x)\n",
    "        layers.append(dropout_2x)\n",
    "\n",
    "    merged = Concatenate(axis=1)(layers)\n",
    "\n",
    "    dropout = Dropout(drop_out)(merged)\n",
    "    \n",
    "    flatten = Flatten()(dropout)\n",
    "    dense = Dense(128, activation='relu')(flatten)\n",
    "    preds = Dense(num_labels, activation='softmax', name='char_cnn_predictions')(dense)\n",
    "\n",
    "    model = Model(char_input, preds)\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['acc'])\n",
    "    return model\n",
    "\n",
    "char_cnn_model2 = create_char_cnn_model2(len(char_to_idx), max_sequence_len, len(emojis))\n",
    "char_cnn_model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 2048\n",
    "char_cnn_model2.fit_generator(\n",
    "    data_generator(train_tweets, batch_size=BATCH_SIZE),\n",
    "    epochs=30,\n",
    "    steps_per_epoch=len(train_tweets) / BATCH_SIZE,\n",
    "    verbose=2,\n",
    "    callbacks=[early]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_cnn_model2.evaluate_generator(\n",
    "    data_generator(test_tweets, batch_size=BATCH_SIZE),\n",
    "    steps=len(test_tweets) / BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**数据预处理**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 50000\n",
    "tokenizer = Tokenizer(num_words=VOCAB_SIZE)\n",
    "tokenizer.fit_on_texts(tweets['text'])\n",
    "\n",
    "training_tokens = tokenizer.texts_to_sequences(train_tweets['text'])\n",
    "test_tokens = tokenizer.texts_to_sequences(test_tweets['text'])\n",
    "max_num_tokens = max(len(x) for x in chain(training_tokens, test_tokens))\n",
    "training_tokens = pad_sequences(training_tokens, maxlen=max_num_tokens)\n",
    "test_tokens = pad_sequences(test_tokens, maxlen=max_num_tokens)\n",
    "\n",
    "training_labels = np.asarray([emoji_to_idx[em] for em in train_tweets['emoji']])\n",
    "test_labels = np.asarray([emoji_to_idx[em] for em in test_tweets['emoji']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_weights(tokenizer):\n",
    "    model = Word2Vec.load('data/twitter_w2v.model')\n",
    "    w2v = np.zeros((tokenizer.num_words, w2v_model.syn0.shape[1]))\n",
    "    for k, v in tokenizer.word_index.items():\n",
    "        if v >= tokenizer.num_words:\n",
    "            continue\n",
    "        if k in w2v_model:\n",
    "            w2v[v] = w2v_model[k]\n",
    "    return w2v\n",
    "\n",
    "# This may take a while to load\n",
    "#w2v = load_weights(tokenizer)\n",
    "#model = Word2Vec.load('data/twitter_w2v.model')\n",
    "w2v = np.zeros((tokenizer.num_words, model.wv.syn0.shape[1]))\n",
    "found = 0\n",
    "for k, v in tokenizer.word_index.items():\n",
    "    if v >= tokenizer.num_words:\n",
    "        continue\n",
    "    if k in model:\n",
    "        w2v[v] = model[k]\n",
    "        found += 1\n",
    "found, tokenizer.num_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 词级别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_model(vocab_size, embedding_size=None, embedding_weights=None, drop_out=0.2):\n",
    "    message = Input(shape=(max_num_tokens,), dtype='int32', name='cnn_input')\n",
    "    \n",
    "    \n",
    "    # The convolution layer in keras does not support masking, so we just allow\n",
    "    # the embedding layer to learn an explicit value.\n",
    "    embedding = Embedding(mask_zero=False, input_dim=vocab_size, \n",
    "                          output_dim=embedding_weights.shape[1], \n",
    "                          weights=[embedding_weights],\n",
    "                          trainable=True,\n",
    "                          name='cnn_embedding')(message)\n",
    "    \n",
    "    global_pools = []\n",
    "    for window in 2, 3:\n",
    "        conv_1x = Conv1D(128, window, activation='relu', padding='valid')(embedding)\n",
    "        max_pool_1x = MaxPooling1D(2)(conv_1x)\n",
    "        conv_2x = Conv1D(256, window, activation='relu', padding='valid')(max_pool_1x)\n",
    "        max_pool_2x = MaxPooling1D(2)(conv_2x)\n",
    "        conv_3x = Conv1D(256, window, activation='relu', padding='valid')(max_pool_2x)\n",
    "\n",
    "        global_pools.append(GlobalMaxPooling1D()(conv_3x))\n",
    "\n",
    "    merged = Concatenate(axis=1)(global_pools)\n",
    "    fc1 = Dense(units=128, activation='elu')(merged)\n",
    "    preds = Dense(units=len(emojis), activation='softmax', name='cnn_predictions')(fc1)\n",
    "    model = Model(\n",
    "        inputs=[message],\n",
    "        outputs=[preds],\n",
    "    )\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "cnn_model = create_cnn_model(VOCAB_SIZE, embedding_weights=w2v)\n",
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model.fit(training_tokens, training_labels, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lstm_model(vocab_size, embedding_size=None, embedding_weights=None):\n",
    "    message = Input(shape=(None,), dtype='int32', name='lstm_input')\n",
    "    embedding = Embedding(mask_zero=False, input_dim=vocab_size, \n",
    "                          output_dim=embedding_weights.shape[1], \n",
    "                          weights=[embedding_weights],\n",
    "                          trainable=True,\n",
    "                          name='lstm_embedding')(message)\n",
    "\n",
    "    lstm_1 = LSTM(units=128, return_sequences=False)(embedding)\n",
    "    preds = Dense(units=len(emojis), activation='softmax', name='lstm_predictions')(lstm_1)\n",
    "    \n",
    "    model = Model(\n",
    "        inputs=[message],\n",
    "        outputs=[preds],\n",
    "    )\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = create_lstm_model(VOCAB_SIZE, embedding_weights=w2v)\n",
    "lstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model.fit(training_tokens, training_labels, epochs=12, batch_size=1024, callbacks=[early])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model.evaluate(test_tokens, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**模型比较**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_char_vectors, _ = next(data_generator(test_tweets, None)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = {\n",
    "    label: [emojis[np.argmax(x)] for x in pred]\n",
    "    for label, pred in (\n",
    "        ('lstm', lstm_model.predict(test_tokens[:100])),\n",
    "        ('char_cnn', char_cnn_model.predict(test_char_vectors[:100])),\n",
    "        ('cnn', cnn_model.predict(test_tokens[:100])),\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a dataframe just for test data\n",
    "pd.options.display.max_colwidth = 128\n",
    "test_df = test_tweets[:100].reset_index()\n",
    "eval_df = pd.DataFrame({\n",
    "    'content': test_df['text'],\n",
    "    'true': test_df['emoji'],\n",
    "    **predictions\n",
    "})\n",
    "eval_df[['content', 'true', 'char_cnn', 'cnn', 'lstm']].head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**定性评估**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df[eval_df['lstm'] != eval_df['true']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_data_generator(tweets, tokens, batch_size):\n",
    "    tweets = tweets.reset_index()\n",
    "    while True:\n",
    "        batch_idx = random.sample(range(len(tweets)), batch_size)\n",
    "        tweet_batch = tweets.iloc[batch_idx]\n",
    "        token_batch = tokens[batch_idx]\n",
    "        char_vec = np.zeros((batch_size, max_sequence_len, len(chars)))\n",
    "        token_vec = np.zeros((batch_size, max_num_tokens))\n",
    "        y = np.zeros((batch_size,))\n",
    "        for row_idx, (token_row, (_, tweet_row)) in enumerate(zip(token_batch, tweet_batch.iterrows())):\n",
    "            y[row_idx] = emoji_to_idx[tweet_row['emoji']]\n",
    "            for ch_idx, ch in enumerate(tweet_row['text']):\n",
    "                char_vec[row_idx, ch_idx, char_to_idx[ch]] = 1\n",
    "            token_vec[row_idx, :] = token_row\n",
    "        yield {'char_cnn_input': char_vec, 'cnn_input': token_vec, 'lstm_input': token_vec}, y\n",
    "\n",
    "d, y = next(combined_data_generator(train_tweets, training_tokens, 5))\n",
    "d['lstm_input'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_layer(model):\n",
    "    layers = [layer for layer in model.layers if layer.name.endswith('_predictions')]\n",
    "    return layers[0].output\n",
    "\n",
    "def create_ensemble(*models):\n",
    "    inputs = [model.input for model in models]\n",
    "    predictions = [prediction_layer(model) for model in models]\n",
    "    merged = Average()(predictions)\n",
    "    model = Model(\n",
    "        inputs=inputs,\n",
    "        outputs=[merged],\n",
    "    )\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "ensemble = create_ensemble(char_cnn_model2, cnn_model, lstm_model)\n",
    "ensemble.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 512\n",
    "ensemble.fit_generator(\n",
    "    combined_data_generator(train_tweets, training_tokens, BATCH_SIZE),\n",
    "    epochs=20,\n",
    "    steps_per_epoch=len(train_tweets) / BATCH_SIZE,\n",
    "    verbose=2,\n",
    "    callbacks=[early]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble.evaluate_generator(\n",
    "    combined_data_generator(test_tweets, test_tokens, BATCH_SIZE),\n",
    "    steps=len(test_tweets) / BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 句子转换"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from gutenberg.acquire import load_etext\n",
    "from gutenberg.cleanup import strip_headers\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "import inflect\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "from gensim.utils import tokenize\n",
    "from itertools import groupby\n",
    "\n",
    "from keras.models import Input, Model\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import LSTM, RepeatVector\n",
    "from keras.layers.wrappers import TimeDistributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67176"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = inflect.engine()\n",
    "\n",
    "pairs = {}\n",
    "for synset in wn.all_synsets('n'):\n",
    "    word = synset.name().split('.', 1)[0]\n",
    "    if not word in pairs:\n",
    "        pairs[word] = p.plural(word)\n",
    "len(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/plurals.txt', 'w') as fout:\n",
    "    for k in sorted(pairs):\n",
    "        if '_' in k or '-' in k:\n",
    "            continue\n",
    "        if k.isdigit():\n",
    "            continue\n",
    "        fout.write('%s\\t%s\\n' % (k, pairs[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'noes'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.plural('no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharacterTable(object):\n",
    "    \"\"\"Given a set of characters:\n",
    "    + Encode them to a one hot integer representation\n",
    "    + Decode the one hot integer representation to their character output\n",
    "    + Decode a vector of probabilities to their character output\n",
    "    \"\"\"\n",
    "    def __init__(self, chars):\n",
    "        \"\"\"Initialize character table.\n",
    "        # Arguments\n",
    "            chars: Characters that can appear in the input.\n",
    "        \"\"\"\n",
    "        self.chars = sorted(set(chars))\n",
    "        self.char_indices = dict((c, i) for i, c in enumerate(self.chars))\n",
    "        self.indices_char = dict((i, c) for i, c in enumerate(self.chars))\n",
    "\n",
    "    def encode(self, C, num_rows):\n",
    "        \"\"\"One hot encode given string C.\n",
    "        # Arguments\n",
    "            num_rows: Number of rows in the returned one hot encoding. This is\n",
    "                used to keep the # of rows for each data the same.\n",
    "        \"\"\"\n",
    "        x = np.zeros((num_rows, len(self.chars)))\n",
    "        for i, c in enumerate(C):\n",
    "            x[i, self.char_indices[c]] = 1\n",
    "        return x\n",
    "\n",
    "    def decode(self, x, calc_argmax=True):\n",
    "        if calc_argmax:\n",
    "            x = x.argmax(axis=-1)\n",
    "        return ''.join(self.indices_char[x] for x in x)\n",
    "    \n",
    "class colors:\n",
    "    ok = '\\033[92m'\n",
    "    fail = '\\033[91m'\n",
    "    close = '\\033[0m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total addition questions: 39929\n"
     ]
    }
   ],
   "source": [
    "# Parameters for the model and dataset.\n",
    "INVERT = True\n",
    "\n",
    "questions = []\n",
    "expected = []\n",
    "seen = set()\n",
    "#with open('data/en_de.txt') as fin:\n",
    "with open('data/plurals.txt') as fin:\n",
    "    for line in fin:\n",
    "        en, de = line.strip().split('\\t')\n",
    "        questions.append(en)\n",
    "        expected.append(de)\n",
    "\n",
    "max_question_len = max(len(q) for q in questions)\n",
    "max_expected_len = max(len(e) for e in expected)\n",
    "questions = [' ' * (max_question_len - len(q)) + q for q in questions]\n",
    "expected = [e + ' ' * (max_expected_len - len(e)) for e in expected]\n",
    "if INVERT:\n",
    "    questions = [q[::-1] for q in questions]\n",
    "\n",
    "print('Total addition questions:', len(questions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = set(ch for k, v in zip(questions, expected) for ch in k + v)\n",
    "ctable = CharacterTable(chars)\n",
    "len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorization...\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "print('Vectorization...')\n",
    "x = np.zeros((len(questions), max_question_len, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(questions), max_expected_len, len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(questions):\n",
    "    x[i] = ctable.encode(sentence, max_question_len)\n",
    "for i, sentence in enumerate(expected):\n",
    "    y[i] = ctable.encode(sentence, max_expected_len)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data:\n",
      "(35937, 31, 40)\n",
      "(35937, 32, 40)\n",
      "Validation Data:\n",
      "(3992, 31, 40)\n",
      "(3992, 32, 40)\n"
     ]
    }
   ],
   "source": [
    "# Shuffle (x, y) in unison as the later parts of x will almost all be larger\n",
    "# digits.\n",
    "indices = np.arange(len(y))\n",
    "np.random.shuffle(indices)\n",
    "x = x[indices]\n",
    "y = y[indices]\n",
    "\n",
    "# Explicitly set apart 10% for validation data that we never train over.\n",
    "split_at = len(x) - len(x) // 10\n",
    "(x_train, x_val) = x[:split_at], x[split_at:]\n",
    "(y_train, y_val) = y[:split_at], y[split_at:]\n",
    "\n",
    "print('Training Data:')\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print('Validation Data:')\n",
    "print(x_val.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 128)               86528     \n",
      "_________________________________________________________________\n",
      "repeat_vector_1 (RepeatVecto (None, 32, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 32, 128)           131584    \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 32, 40)            5160      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 32, 40)            0         \n",
      "=================================================================\n",
      "Total params: 223,272\n",
      "Trainable params: 223,272\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# The below is taken from: https://github.com/keras-team/keras/blob/master/examples/addition_rnn.py\n",
    "RNN = layers.LSTM\n",
    "HIDDEN_SIZE = 128\n",
    "LAYERS = 1\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "# \"Encode\" the input sequence using an RNN, producing an output of HIDDEN_SIZE.\n",
    "# Note: In a situation where your input sequences have a variable length,\n",
    "# use input_shape=(None, num_feature).\n",
    "model.add(RNN(HIDDEN_SIZE, input_shape=(max_question_len, len(chars))))\n",
    "# As the decoder RNN's input, repeatedly provide with the last hidden state of\n",
    "# RNN for each time step. Repeat 'DIGITS + 1' times as that's the maximum\n",
    "# length of output, e.g., when DIGITS=3, max output is 999+999=1998.\n",
    "#model.add(layers.Dropout(DROP_OUT))\n",
    "model.add(layers.RepeatVector(max_expected_len))\n",
    "# The decoder RNN could be multiple layers stacked or a single layer.\n",
    "for _ in range(LAYERS):\n",
    "    # By setting return_sequences to True, return not only the last output but\n",
    "    # all the outputs so far in the form of (num_samples, timesteps,\n",
    "    # output_dim). This is necessary as TimeDistributed in the below expects\n",
    "    # the first dimension to be the timesteps.\n",
    "    model.add(RNN(HIDDEN_SIZE, return_sequences=True))\n",
    "#    model.add(layers.Dropout(DROP_OUT))\n",
    "\n",
    "# Apply a dense layer to the every temporal slice of an input. For each of step\n",
    "# of the output sequence, decide which character should be chosen.\n",
    "model.add(layers.TimeDistributed(layers.Dense(len(chars))))\n",
    "model.add(layers.Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input 0 is incompatible with layer repeat_vector_2: expected ndim=2, found ndim=3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-20a70fbf90c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mseq2seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_seq2seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-20a70fbf90c4>\u001b[0m in \u001b[0;36mcreate_seq2seq\u001b[0;34m(num_nodes, num_layers)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcreate_seq2seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_nodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mquestion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_question_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'question'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mrepeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRepeatVector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_expected_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquestion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/cookbook/lib/python3.6/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    557\u001b[0m                 \u001b[0;31m# Raise exceptions in case the input is not compatible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m                 \u001b[0;31m# with the input_spec specified in the layer constructor.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 559\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_input_compatibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m                 \u001b[0;31m# Collect input shapes to build layer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/cookbook/lib/python3.6/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    456\u001b[0m                                      \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m': expected ndim='\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m                                      \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m', found ndim='\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m                                      str(K.ndim(x)))\n\u001b[0m\u001b[1;32m    459\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_ndim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m                 \u001b[0mndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input 0 is incompatible with layer repeat_vector_2: expected ndim=2, found ndim=3"
     ]
    }
   ],
   "source": [
    "def create_seq2seq(num_nodes, num_layers):\n",
    "    question = Input(shape=(max_question_len, len(chars)), name='question')\n",
    "    repeat = RepeatVector(max_expected_len)(question)\n",
    "    prev = question\n",
    "    for _ in range(num_layers):\n",
    "        lstm = LSTM(num_nodes, return_sequences=True, name='lstm_layer_%d' % (i + 1))(prev)\n",
    "        prev = lstm\n",
    "    dense = TimeDistributed(Dense(num_chars, name='dense', activation='softmax'))(prev)\n",
    "    model = Model(inputs=[input], outputs=[dense])\n",
    "    optimizer = RMSprop(lr=0.01)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "seq2seq = create_seq2seq(128, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 35937 samples, validate on 3992 samples\n",
      "Epoch 1/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.7358 - acc: 0.7745 - val_loss: 0.7497 - val_acc: 0.7713\n",
      "Epoch 2/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.7179 - acc: 0.7793 - val_loss: 0.7123 - val_acc: 0.7800\n",
      "Epoch 3/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.7120 - acc: 0.7815 - val_loss: 0.7077 - val_acc: 0.7818\n",
      "Epoch 4/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.7030 - acc: 0.7851 - val_loss: 0.6990 - val_acc: 0.7873\n",
      "Epoch 5/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.7009 - acc: 0.7862 - val_loss: 0.6992 - val_acc: 0.7876\n",
      "Epoch 6/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.7009 - acc: 0.7861 - val_loss: 0.6984 - val_acc: 0.7870\n",
      "Epoch 7/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.7020 - acc: 0.7856 - val_loss: 0.6978 - val_acc: 0.7881\n",
      "Epoch 8/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.7026 - acc: 0.7851 - val_loss: 0.6982 - val_acc: 0.7876\n",
      "Epoch 9/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6991 - acc: 0.7869 - val_loss: 0.6976 - val_acc: 0.7877\n",
      "Epoch 10/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.7083 - acc: 0.7836 - val_loss: 0.7259 - val_acc: 0.7781\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 1\n",
      "                       monarchy (monarchies                      ) - sareeiisgs                      \n",
      "                          ajuga (ajugas                          ) - saaeas                          \n",
      "                      brushwood (brushwoods                      ) - careeaieas                      \n",
      "                     lutjanidae (lutjanidaes                     ) - careeaiises                     \n",
      "                       centaury (centauries                      ) - sareeiises                      \n",
      "                          print (prints                          ) - saaets                          \n",
      "                     karyolysis (karyolyses                      ) - careeaaiises                    \n",
      "                     commission (commissions                     ) - careeaaieas                     \n",
      "                         lappic (lappics                         ) - sartias                         \n",
      "                kindheartedness (kindheartednesses               ) - cereooooiiaaiisns               \n",
      "Train on 35937 samples, validate on 3992 samples\n",
      "Epoch 1/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.8027 - acc: 0.7599 - val_loss: 0.7334 - val_acc: 0.7765\n",
      "Epoch 2/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.7246 - acc: 0.7774 - val_loss: 0.7104 - val_acc: 0.7829\n",
      "Epoch 3/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.7063 - acc: 0.7845 - val_loss: 0.7002 - val_acc: 0.7869\n",
      "Epoch 4/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.7008 - acc: 0.7865 - val_loss: 0.6978 - val_acc: 0.7878\n",
      "Epoch 5/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6993 - acc: 0.7870 - val_loss: 0.6967 - val_acc: 0.7884\n",
      "Epoch 6/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6984 - acc: 0.7876 - val_loss: 0.6963 - val_acc: 0.7884\n",
      "Epoch 7/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6978 - acc: 0.7878 - val_loss: 0.6961 - val_acc: 0.7882\n",
      "Epoch 8/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6984 - acc: 0.7872 - val_loss: 0.7095 - val_acc: 0.7812\n",
      "Epoch 9/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.7441 - acc: 0.7720 - val_loss: 0.7018 - val_acc: 0.7848\n",
      "Epoch 10/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.7141 - acc: 0.7805 - val_loss: 0.7018 - val_acc: 0.7858\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 2\n",
      "                      palmature (palmatures                      ) - sareeiises                      \n",
      "                        western (westerns                        ) - sartiers                        \n",
      "                        stanton (stantons                        ) - sartiens                        \n",
      "                     famotidine (famotidines                     ) - careeaiises                     \n",
      "                  chrysothamnus (chrysothamnuses                 ) - cereoooiatiises                 \n",
      "                      narcissus (narcissuses                     ) - careeaiises                     \n",
      "                      transport (transports                      ) - sareeiises                      \n",
      "                   eosinophilia (eosinophilias                   ) - ceroooaations                   \n",
      "                    bulimarexia (bulimarexias                    ) - ceroooations                    \n",
      "                    gnatcatcher (gnatcatchers                    ) - cereooatists                    \n",
      "Train on 35937 samples, validate on 3992 samples\n",
      "Epoch 1/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.7008 - acc: 0.7862 - val_loss: 0.6967 - val_acc: 0.7877\n",
      "Epoch 2/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6975 - acc: 0.7877 - val_loss: 0.6951 - val_acc: 0.7889\n",
      "Epoch 3/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6968 - acc: 0.7878 - val_loss: 0.6970 - val_acc: 0.7876\n",
      "Epoch 4/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6985 - acc: 0.7868 - val_loss: 0.6945 - val_acc: 0.7890\n",
      "Epoch 5/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6965 - acc: 0.7878 - val_loss: 0.6940 - val_acc: 0.7886\n",
      "Epoch 6/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.7043 - acc: 0.7842 - val_loss: 0.7010 - val_acc: 0.7855\n",
      "Epoch 7/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6979 - acc: 0.7868 - val_loss: 0.6936 - val_acc: 0.7886\n",
      "Epoch 8/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6953 - acc: 0.7881 - val_loss: 0.6926 - val_acc: 0.7890\n",
      "Epoch 9/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.7081 - acc: 0.7833 - val_loss: 0.6960 - val_acc: 0.7866\n",
      "Epoch 10/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.7055 - acc: 0.7834 - val_loss: 0.6981 - val_acc: 0.7859\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 3\n",
      "                         berith (beriths                         ) - sartias                         \n",
      "                     zimbabwean (zimbabweans                     ) - cereeetiens                     \n",
      "                      chalcedon (chalcedons                      ) - sareeeiens                      \n",
      "                       flatwork (flatworks                       ) - sareeings                       \n",
      "                           item (items                           ) - sants                           \n",
      "                       lecanora (lecanoras                       ) - sareeieas                       \n",
      "                     collembola (collembolas                     ) - cereeetieas                     \n",
      "                   ulemorrhagia (ulemorrhagias                   ) - cereooaations                   \n",
      "                       foredeck (foredecks                       ) - sareeints                       \n",
      "                        avignon (avignons                        ) - sareieas                        \n",
      "Train on 35937 samples, validate on 3992 samples\n",
      "Epoch 1/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6967 - acc: 0.7870 - val_loss: 0.6940 - val_acc: 0.7879\n",
      "Epoch 2/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6953 - acc: 0.7877 - val_loss: 0.6953 - val_acc: 0.7881\n",
      "Epoch 3/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.7244 - acc: 0.7798 - val_loss: 0.9272 - val_acc: 0.7494\n",
      "Epoch 4/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.7563 - acc: 0.7705 - val_loss: 0.7129 - val_acc: 0.7822\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.7079 - acc: 0.7830 - val_loss: 0.7010 - val_acc: 0.7865\n",
      "Epoch 6/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6984 - acc: 0.7868 - val_loss: 0.6937 - val_acc: 0.7886\n",
      "Epoch 7/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6945 - acc: 0.7880 - val_loss: 0.6921 - val_acc: 0.7891\n",
      "Epoch 8/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6936 - acc: 0.7883 - val_loss: 0.6913 - val_acc: 0.7892\n",
      "Epoch 9/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6935 - acc: 0.7883 - val_loss: 0.7081 - val_acc: 0.7815\n",
      "Epoch 10/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.7046 - acc: 0.7836 - val_loss: 0.6925 - val_acc: 0.7886\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 4\n",
      "                         ocelot (ocelots                         ) - sariers                         \n",
      "                       outreach (outreaches                      ) - sareeiiies                      \n",
      "                         eyecup (eyecups                         ) - sarteas                         \n",
      "                       gentiana (gentianas                       ) - sareeioas                       \n",
      "                           utah (utahs                           ) - saale                           \n",
      "                         bowleg (bowlegs                         ) - sarines                         \n",
      "                          kamba (kambas                          ) - sareas                          \n",
      "                         tanakh (tanakhs                         ) - sarteas                         \n",
      "                        cetacea (cetaceas                        ) - sartiens                        \n",
      "                          pipet (pipets                          ) - saaets                          \n",
      "Train on 35937 samples, validate on 3992 samples\n",
      "Epoch 1/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6966 - acc: 0.7868 - val_loss: 0.6920 - val_acc: 0.7884\n",
      "Epoch 2/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6929 - acc: 0.7883 - val_loss: 0.6908 - val_acc: 0.7891\n",
      "Epoch 3/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6920 - acc: 0.7887 - val_loss: 0.6911 - val_acc: 0.7889\n",
      "Epoch 4/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.7648 - acc: 0.7706 - val_loss: 0.7700 - val_acc: 0.7699\n",
      "Epoch 5/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.7277 - acc: 0.7758 - val_loss: 0.7050 - val_acc: 0.7848\n",
      "Epoch 6/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.7014 - acc: 0.7853 - val_loss: 0.6938 - val_acc: 0.7885\n",
      "Epoch 7/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6943 - acc: 0.7881 - val_loss: 0.6908 - val_acc: 0.7895\n",
      "Epoch 8/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6925 - acc: 0.7884 - val_loss: 0.6906 - val_acc: 0.7893\n",
      "Epoch 9/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6927 - acc: 0.7882 - val_loss: 0.6896 - val_acc: 0.7898\n",
      "Epoch 10/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6912 - acc: 0.7889 - val_loss: 0.6890 - val_acc: 0.7895\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 5\n",
      "                         testee (testees                         ) - sariies                         \n",
      "                        orleans (orlean                          ) - sareiias                        \n",
      "                    mesoamerica (mesoamericas                    ) - ceroooatioas                    \n",
      "                      refectory (refectories                     ) - careeaiises                     \n",
      "                       amygdala (amygdalas                       ) - sareeioas                       \n",
      "                      tofieldia (tofieldias                      ) - careeeioas                      \n",
      "                      menshevik (mensheviks                      ) - sareetints                      \n",
      "                  circumduction (circumductions                  ) - cereoooaations                  \n",
      "                     typography (typographies                    ) - cereooaiises                    \n",
      "                          lygus (lyguses                         ) - sariies                         \n",
      "Train on 35937 samples, validate on 3992 samples\n",
      "Epoch 1/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.7008 - acc: 0.7851 - val_loss: 0.7030 - val_acc: 0.7836\n",
      "Epoch 2/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6991 - acc: 0.7853 - val_loss: 0.6932 - val_acc: 0.7875\n",
      "Epoch 3/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6925 - acc: 0.7880 - val_loss: 0.6891 - val_acc: 0.7900\n",
      "Epoch 4/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6906 - acc: 0.7888 - val_loss: 0.6903 - val_acc: 0.7889\n",
      "Epoch 5/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6899 - acc: 0.7890 - val_loss: 0.6877 - val_acc: 0.7897\n",
      "Epoch 6/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.7558 - acc: 0.7712 - val_loss: 0.7508 - val_acc: 0.7732\n",
      "Epoch 7/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.7216 - acc: 0.7779 - val_loss: 0.7133 - val_acc: 0.7807\n",
      "Epoch 8/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.7007 - acc: 0.7850 - val_loss: 0.6943 - val_acc: 0.7874\n",
      "Epoch 9/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6936 - acc: 0.7876 - val_loss: 0.6889 - val_acc: 0.7894\n",
      "Epoch 10/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6904 - acc: 0.7887 - val_loss: 0.6879 - val_acc: 0.7899\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 6\n",
      "                     ambassador (ambassadors                     ) - careeatiens                     \n",
      "                     coastguard (coastguards                     ) - careeatiets                     \n",
      "                       strainer (strainers                       ) - sareeiers                       \n",
      "                         phyllo (phylloes                        ) - sartiies                        \n",
      "                       jeremiad (jeremiads                       ) - sareeiers                       \n",
      "                          feria (ferias                          ) - sareas                          \n",
      "                         actaea (actaeas                         ) - sariens                         \n",
      "                         rescue (rescues                         ) - sariies                         \n",
      "                     regression (regressions                     ) - careeatiens                     \n",
      "                        cutwork (cutworks                        ) - sartints                        \n",
      "Train on 35937 samples, validate on 3992 samples\n",
      "Epoch 1/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6897 - acc: 0.7892 - val_loss: 0.6872 - val_acc: 0.7899\n",
      "Epoch 2/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6904 - acc: 0.7887 - val_loss: 0.6882 - val_acc: 0.7900\n",
      "Epoch 3/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.7325 - acc: 0.7784 - val_loss: 0.7694 - val_acc: 0.7686\n",
      "Epoch 4/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.7602 - acc: 0.7690 - val_loss: 0.7193 - val_acc: 0.7805\n",
      "Epoch 5/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.7093 - acc: 0.7820 - val_loss: 0.7012 - val_acc: 0.7847\n",
      "Epoch 6/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6956 - acc: 0.7869 - val_loss: 0.6902 - val_acc: 0.7895\n",
      "Epoch 7/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6905 - acc: 0.7888 - val_loss: 0.6877 - val_acc: 0.7899\n",
      "Epoch 8/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6890 - acc: 0.7893 - val_loss: 0.6880 - val_acc: 0.7898\n",
      "Epoch 9/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6888 - acc: 0.7894 - val_loss: 0.6872 - val_acc: 0.7902\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6883 - acc: 0.7895 - val_loss: 0.6872 - val_acc: 0.7901\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 7\n",
      "                            pot (pots                            ) - sats                            \n",
      "                        malamud (malamuds                        ) - sartiers                        \n",
      "                        fucales (fucale                          ) - sareiias                        \n",
      "                      eptesicus (eptesicuses                     ) - careeatises                     \n",
      "                      alchemist (alchemists                      ) - sareetints                      \n",
      "                        getaway (getaways                        ) - sareiias                        \n",
      "                        marstan (marstans                        ) - sartiens                        \n",
      "                          dijon (dijons                          ) - sarens                          \n",
      "                   seismography (seismographies                  ) - cereoooaaiises                  \n",
      "                           soul (souls                           ) - saaas                           \n",
      "Train on 35937 samples, validate on 3992 samples\n",
      "Epoch 1/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6957 - acc: 0.7866 - val_loss: 0.7257 - val_acc: 0.7749\n",
      "Epoch 2/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.7567 - acc: 0.7695 - val_loss: 0.7212 - val_acc: 0.7784\n",
      "Epoch 3/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.7147 - acc: 0.7799 - val_loss: 0.7007 - val_acc: 0.7851\n",
      "Epoch 4/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6956 - acc: 0.7866 - val_loss: 0.6889 - val_acc: 0.7901\n",
      "Epoch 5/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6894 - acc: 0.7892 - val_loss: 0.6869 - val_acc: 0.7899\n",
      "Epoch 6/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6882 - acc: 0.7894 - val_loss: 0.6865 - val_acc: 0.7905\n",
      "Epoch 7/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6877 - acc: 0.7895 - val_loss: 0.6856 - val_acc: 0.7906\n",
      "Epoch 8/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6877 - acc: 0.7894 - val_loss: 0.6852 - val_acc: 0.7907\n",
      "Epoch 9/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6871 - acc: 0.7897 - val_loss: 0.6845 - val_acc: 0.7908\n",
      "Epoch 10/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6870 - acc: 0.7895 - val_loss: 0.6860 - val_acc: 0.7906\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 8\n",
      "                        sitting (sittings                        ) - sartints                        \n",
      "                    fascination (fascinations                    ) - cereooations                    \n",
      "                         eupnea (eupneas                         ) - sarteas                         \n",
      "                   underpayment (underpayments                   ) - cereooaatists                   \n",
      "                     piperazine (piperazines                     ) - careeatises                     \n",
      "                    semimonthly (semimonthlies                   ) - ceroooaatises                   \n",
      "                           apar (apars                           ) - saans                           \n",
      "                   aristophanes (aristophane                     ) - cereoooaaiia                    \n",
      "                          daily (dailies                         ) - sariies                         \n",
      "                          hanoi (hanois                          ) - sarias                          \n",
      "Train on 35937 samples, validate on 3992 samples\n",
      "Epoch 1/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6865 - acc: 0.7897 - val_loss: 0.6881 - val_acc: 0.7890\n",
      "Epoch 2/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.7982 - acc: 0.7635 - val_loss: 0.7678 - val_acc: 0.7677\n",
      "Epoch 3/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.7318 - acc: 0.7749 - val_loss: 0.7054 - val_acc: 0.7826\n",
      "Epoch 4/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.7016 - acc: 0.7849 - val_loss: 0.6920 - val_acc: 0.7885\n",
      "Epoch 5/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6911 - acc: 0.7884 - val_loss: 0.6868 - val_acc: 0.7901\n",
      "Epoch 6/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6889 - acc: 0.7891 - val_loss: 0.6854 - val_acc: 0.7905\n",
      "Epoch 7/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6875 - acc: 0.7894 - val_loss: 0.6859 - val_acc: 0.7901\n",
      "Epoch 8/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6865 - acc: 0.7899 - val_loss: 0.6889 - val_acc: 0.7886\n",
      "Epoch 9/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6884 - acc: 0.7891 - val_loss: 0.6856 - val_acc: 0.7902\n",
      "Epoch 10/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6871 - acc: 0.7894 - val_loss: 0.6838 - val_acc: 0.7904\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 9\n",
      "                       vaseline (vaselines                       ) - saretises                       \n",
      "                         almaty (almaties                        ) - sartiies                        \n",
      "                           node (nodes                           ) - sanes                           \n",
      "                       diskette (diskettes                       ) - saretises                       \n",
      "                        tangelo (tangeloes                       ) - saretises                       \n",
      "                       feelings (feeling                         ) - sareeiias                       \n",
      "                     tourmaline (tourmalines                     ) - careeatises                     \n",
      "                  struthiomimus (struthiomimuses                 ) - cereoooiaaiises                 \n",
      "                       simazine (simazines                       ) - saretises                       \n",
      "                   gonorhynchus (gonorhynchuses                  ) - cereoooaaiises                  \n",
      "Train on 35937 samples, validate on 3992 samples\n",
      "Epoch 1/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6853 - acc: 0.7901 - val_loss: 0.6832 - val_acc: 0.7909\n",
      "Epoch 2/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6848 - acc: 0.7903 - val_loss: 0.6836 - val_acc: 0.7914\n",
      "Epoch 3/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.7670 - acc: 0.7711 - val_loss: 0.8207 - val_acc: 0.7624\n",
      "Epoch 4/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.7502 - acc: 0.7702 - val_loss: 0.7064 - val_acc: 0.7809\n",
      "Epoch 5/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.7058 - acc: 0.7826 - val_loss: 0.6934 - val_acc: 0.7885\n",
      "Epoch 6/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6923 - acc: 0.7876 - val_loss: 0.6871 - val_acc: 0.7901\n",
      "Epoch 7/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6869 - acc: 0.7896 - val_loss: 0.6842 - val_acc: 0.7909\n",
      "Epoch 8/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6861 - acc: 0.7898 - val_loss: 0.6840 - val_acc: 0.7906\n",
      "Epoch 9/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6850 - acc: 0.7903 - val_loss: 0.6842 - val_acc: 0.7900\n",
      "Epoch 10/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6844 - acc: 0.7904 - val_loss: 0.6821 - val_acc: 0.7915\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 10\n",
      "                       liposome (liposomes                       ) - sareiises                       \n",
      "                      cartridge (cartridges                      ) - sareeiises                      \n",
      "                     eyewitness (eyewitnesses                    ) - cereooatises                    \n",
      "                           lego (legoes                          ) - saiies                          \n",
      "                    chlorophyta (chlorophytas                    ) - cereooatioas                    \n",
      "                       ranching (ranchings                       ) - sareeints                       \n",
      "                      furniture (furnitures                      ) - sareeiises                      \n",
      "                            eye (eyes                            ) - aaes                            \n",
      "                           chop (chops                           ) - saaas                           \n",
      "                thermochemistry (thermochemistries               ) - ceeeooooiiaaiises               \n",
      "Train on 35937 samples, validate on 3992 samples\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6959 - acc: 0.7865 - val_loss: 0.7091 - val_acc: 0.7811\n",
      "Epoch 2/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.7173 - acc: 0.7799 - val_loss: 0.6855 - val_acc: 0.7900\n",
      "Epoch 3/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6951 - acc: 0.7859 - val_loss: 0.6855 - val_acc: 0.7894\n",
      "Epoch 4/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6941 - acc: 0.7865 - val_loss: 0.6860 - val_acc: 0.7898\n",
      "Epoch 5/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6881 - acc: 0.7886 - val_loss: 0.6866 - val_acc: 0.7899\n",
      "Epoch 6/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6854 - acc: 0.7898 - val_loss: 0.6841 - val_acc: 0.7911\n",
      "Epoch 7/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6848 - acc: 0.7899 - val_loss: 0.6813 - val_acc: 0.7916\n",
      "Epoch 8/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6841 - acc: 0.7902 - val_loss: 0.6822 - val_acc: 0.7910\n",
      "Epoch 9/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6834 - acc: 0.7905 - val_loss: 0.6817 - val_acc: 0.7916\n",
      "Epoch 10/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6840 - acc: 0.7901 - val_loss: 0.6807 - val_acc: 0.7918\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 11\n",
      "                    hypokalemia (hypokalemias                    ) - cereooaaiias                    \n",
      "                         girdle (girdles                         ) - sariies                         \n",
      "                      detriment (detriments                      ) - sareetints                      \n",
      "                     repetition (repetitions                     ) - cereeatiens                     \n",
      "                          lagos (lago                            ) - sariis                          \n",
      "                 psittaciformes (psittaciforme                   ) - cereoooiiatiias                 \n",
      "                    aminopyrine (aminopyrines                    ) - cereoaatises                    \n",
      "                    requirement (requirements                    ) - cereooatints                    \n",
      "                        latchet (latchets                        ) - sartints                        \n",
      "                        tellima (tellimas                        ) - sareieas                        \n",
      "Train on 35937 samples, validate on 3992 samples\n",
      "Epoch 1/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.7069 - acc: 0.7829 - val_loss: 0.8121 - val_acc: 0.7591\n",
      "Epoch 2/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.7495 - acc: 0.7722 - val_loss: 0.7447 - val_acc: 0.7684\n",
      "Epoch 3/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.7071 - acc: 0.7823 - val_loss: 0.6937 - val_acc: 0.7877\n",
      "Epoch 4/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6892 - acc: 0.7885 - val_loss: 0.6833 - val_acc: 0.7913\n",
      "Epoch 5/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6840 - acc: 0.7904 - val_loss: 0.6813 - val_acc: 0.7917\n",
      "Epoch 6/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6825 - acc: 0.7909 - val_loss: 0.6808 - val_acc: 0.7918\n",
      "Epoch 7/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6825 - acc: 0.7908 - val_loss: 0.6806 - val_acc: 0.7916\n",
      "Epoch 8/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6820 - acc: 0.7909 - val_loss: 0.6804 - val_acc: 0.7916\n",
      "Epoch 9/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6810 - acc: 0.7914 - val_loss: 0.6793 - val_acc: 0.7926\n",
      "Epoch 10/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6816 - acc: 0.7911 - val_loss: 0.6783 - val_acc: 0.7929\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 12\n",
      "                  chorizagrotis (chorizagroti                    ) - cerooooiaaiiae                  \n",
      "                    progressive (progressives                    ) - cereoaatises                    \n",
      "                       bedstead (bedsteads                       ) - sareeiers                       \n",
      "                     infidelity (infidelities                    ) - cereoaatises                    \n",
      "                       resurvey (resurveys                       ) - sareeiias                       \n",
      "                   solenostemon (solenostemons                   ) - cerooooations                   \n",
      "               photolithography (photolithographies              ) - cnoooooooiiiiiises              \n",
      "                           jade (jades                           ) - sanes                           \n",
      "                   convalescent (convalescents                   ) - cereooaatists                   \n",
      "                           wart (warts                           ) - sants                           \n",
      "Train on 35937 samples, validate on 3992 samples\n",
      "Epoch 1/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6945 - acc: 0.7862 - val_loss: 0.6855 - val_acc: 0.7902\n",
      "Epoch 2/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6828 - acc: 0.7904 - val_loss: 0.6775 - val_acc: 0.7931\n",
      "Epoch 3/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6791 - acc: 0.7922 - val_loss: 0.6761 - val_acc: 0.7941\n",
      "Epoch 4/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6774 - acc: 0.7933 - val_loss: 0.6770 - val_acc: 0.7937\n",
      "Epoch 5/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.7059 - acc: 0.7833 - val_loss: 0.7753 - val_acc: 0.7665\n",
      "Epoch 6/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.7528 - acc: 0.7723 - val_loss: 0.7165 - val_acc: 0.7797\n",
      "Epoch 7/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6965 - acc: 0.7860 - val_loss: 0.6797 - val_acc: 0.7929\n",
      "Epoch 8/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6799 - acc: 0.7923 - val_loss: 0.6734 - val_acc: 0.7948\n",
      "Epoch 9/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6735 - acc: 0.7945 - val_loss: 0.6706 - val_acc: 0.7956\n",
      "Epoch 10/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6705 - acc: 0.7957 - val_loss: 0.6677 - val_acc: 0.7964\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 13\n",
      "                           loos (loo                             ) - soias                           \n",
      "                      heliogram (heliograms                      ) - cereeeiers                      \n",
      "                         boston (bostons                         ) - sosions                         \n",
      "                    rambouillet (rambouillets                    ) - cereeeatints                    \n",
      "                       cadaster (cadasters                       ) - careeiers                       \n",
      "                         filler (fillers                         ) - sariers                         \n",
      "                        ariomma (ariommas                        ) - caretaas                        \n",
      "                          goose (geese                           ) - soiies                          \n",
      "                           grog (grogs                           ) - sants                           \n",
      "                    zonotrichia (zonotrichias                    ) - cooooooaioas                    \n",
      "Train on 35937 samples, validate on 3992 samples\n",
      "Epoch 1/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6682 - acc: 0.7967 - val_loss: 0.6660 - val_acc: 0.7974\n",
      "Epoch 2/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6667 - acc: 0.7973 - val_loss: 0.6646 - val_acc: 0.7977\n",
      "Epoch 3/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6658 - acc: 0.7977 - val_loss: 0.6641 - val_acc: 0.7979\n",
      "Epoch 4/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6660 - acc: 0.7974 - val_loss: 0.6637 - val_acc: 0.7980\n",
      "Epoch 5/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6673 - acc: 0.7967 - val_loss: 0.6945 - val_acc: 0.7859\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6740 - acc: 0.7935 - val_loss: 0.6689 - val_acc: 0.7954\n",
      "Epoch 7/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6662 - acc: 0.7967 - val_loss: 0.6637 - val_acc: 0.7973\n",
      "Epoch 8/10\n",
      "35937/35937 [==============================] - 4s 112us/step - loss: 0.6635 - acc: 0.7980 - val_loss: 0.6625 - val_acc: 0.7980\n",
      "Epoch 9/10\n",
      "35937/35937 [==============================] - 4s 117us/step - loss: 0.6769 - acc: 0.7923 - val_loss: 0.6703 - val_acc: 0.7951\n",
      "Epoch 10/10\n",
      "35937/35937 [==============================] - 4s 117us/step - loss: 0.6671 - acc: 0.7962 - val_loss: 0.6622 - val_acc: 0.7980\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 14\n",
      "                   refractivity (refractivities                  ) - aereeeeaatiies                  \n",
      "                      greenberg (greenbergs                      ) - aereeeints                      \n",
      "                          grunt (grunts                          ) - saants                          \n",
      "                         bessel (bessels                         ) - solions                         \n",
      "                         crater (craters                         ) - tarters                         \n",
      "                    obliqueness (obliquenesses                   ) - coooooooiises                   \n",
      "                      tablature (tablatures                      ) - aereetiies                      \n",
      "                        guthrie (guthries                        ) - sartines                        \n",
      "                           toea (toeas                           ) - taaas                           \n",
      "                   chlorination (chlorinations                   ) - cooooooations                   \n",
      "Train on 35937 samples, validate on 3992 samples\n",
      "Epoch 1/10\n",
      "35937/35937 [==============================] - 4s 110us/step - loss: 0.6641 - acc: 0.7973 - val_loss: 0.6601 - val_acc: 0.7989\n",
      "Epoch 2/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6620 - acc: 0.7981 - val_loss: 0.6590 - val_acc: 0.7996\n",
      "Epoch 3/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6641 - acc: 0.7971 - val_loss: 0.7193 - val_acc: 0.7788\n",
      "Epoch 4/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6920 - acc: 0.7873 - val_loss: 0.7260 - val_acc: 0.7792\n",
      "Epoch 5/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6965 - acc: 0.7864 - val_loss: 0.6825 - val_acc: 0.7893\n",
      "Epoch 6/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6703 - acc: 0.7945 - val_loss: 0.6628 - val_acc: 0.7984\n",
      "Epoch 7/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6619 - acc: 0.7981 - val_loss: 0.6591 - val_acc: 0.7992\n",
      "Epoch 8/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6593 - acc: 0.7992 - val_loss: 0.6578 - val_acc: 0.8004\n",
      "Epoch 9/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6584 - acc: 0.7996 - val_loss: 0.6578 - val_acc: 0.8004\n",
      "Epoch 10/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6577 - acc: 0.7999 - val_loss: 0.6559 - val_acc: 0.8008\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 15\n",
      "                       mobility (mobilities                      ) - sorioiises                      \n",
      "                         nailer (nailers                         ) - aarters                         \n",
      "                      patagonia (patagonias                      ) - aareeaiias                      \n",
      "                     salvadoran (salvadorans                     ) - aereeatiens                     \n",
      "                  imperiousness (imperiousnesses                 ) - cosoooooonssses                 \n",
      "                       manifest (manifests                       ) - sareiints                       \n",
      "                      pizzicato (pizzicatoes                     ) - aareeaiiaes                     \n",
      "                   chlorination (chlorinations                   ) - corooooations                   \n",
      "                           dame (dames                           ) - tanes                           \n",
      "                        fucales (fucale                          ) - soosoi s                        \n",
      "Train on 35937 samples, validate on 3992 samples\n",
      "Epoch 1/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6576 - acc: 0.7998 - val_loss: 0.6556 - val_acc: 0.8010\n",
      "Epoch 2/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6607 - acc: 0.7980 - val_loss: 0.6564 - val_acc: 0.8004\n",
      "Epoch 3/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6588 - acc: 0.7991 - val_loss: 0.6991 - val_acc: 0.7833\n",
      "Epoch 4/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6756 - acc: 0.7923 - val_loss: 0.6632 - val_acc: 0.7970\n",
      "Epoch 5/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6575 - acc: 0.7998 - val_loss: 0.6549 - val_acc: 0.8010\n",
      "Epoch 6/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6549 - acc: 0.8010 - val_loss: 0.6572 - val_acc: 0.7996\n",
      "Epoch 7/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6570 - acc: 0.7994 - val_loss: 0.6527 - val_acc: 0.8016\n",
      "Epoch 8/10\n",
      "35937/35937 [==============================] - 4s 110us/step - loss: 0.6561 - acc: 0.8000 - val_loss: 0.6547 - val_acc: 0.8007\n",
      "Epoch 9/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6584 - acc: 0.7985 - val_loss: 0.6555 - val_acc: 0.8006\n",
      "Epoch 10/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6543 - acc: 0.8005 - val_loss: 0.6509 - val_acc: 0.8024\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 16\n",
      "                      casuarius (casuariuses                     ) - pooooossses                     \n",
      "                      annapurna (annapurnas                      ) - sareeaiias                      \n",
      "                          faith (faiths                          ) - sarias                          \n",
      "                           wind (winds                           ) - baeds                           \n",
      "                          catch (catches                         ) - sariies                         \n",
      "                    callorhinus (callorhinuses                   ) - poooooooesses                   \n",
      "                        bowline (bowlines                        ) - sartines                        \n",
      "                    biquadratic (biquadratics                    ) - aereeaaaiias                    \n",
      "                          detox (detoxes                         ) - sariies                         \n",
      "                         potion (potions                         ) - sossons                         \n",
      "Train on 35937 samples, validate on 3992 samples\n",
      "Epoch 1/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6535 - acc: 0.8006 - val_loss: 0.6548 - val_acc: 0.8005\n",
      "Epoch 2/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6522 - acc: 0.8011 - val_loss: 0.6499 - val_acc: 0.8024\n",
      "Epoch 3/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6511 - acc: 0.8014 - val_loss: 0.6490 - val_acc: 0.8026\n",
      "Epoch 4/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.7231 - acc: 0.7807 - val_loss: 0.7056 - val_acc: 0.7830\n",
      "Epoch 5/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6800 - acc: 0.7897 - val_loss: 0.6590 - val_acc: 0.7985\n",
      "Epoch 6/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6573 - acc: 0.7987 - val_loss: 0.6516 - val_acc: 0.8014\n",
      "Epoch 7/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6524 - acc: 0.8005 - val_loss: 0.6491 - val_acc: 0.8023\n",
      "Epoch 8/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6499 - acc: 0.8015 - val_loss: 0.6470 - val_acc: 0.8028\n",
      "Epoch 9/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6494 - acc: 0.8014 - val_loss: 0.6521 - val_acc: 0.7993\n",
      "Epoch 10/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6485 - acc: 0.8016 - val_loss: 0.6452 - val_acc: 0.8038\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------\n",
      "Iteration 17\n",
      "                       krakatau (krakataus                       ) - sareaiias                       \n",
      "                          malay (malays                          ) - saraas                          \n",
      "                          swede (swedes                          ) - baaies                          \n",
      "                       romanism (romanisms                       ) - corssisms                       \n",
      "                       yerupaja (yerupajas                       ) - sorooiias                       \n",
      "                           aids (aid                             ) - souhs                           \n",
      "                     percolator (percolators                     ) - careoatiors                     \n",
      "                      crabgrass (crabgrasses                     ) - poroosssses                     \n",
      "                          pagan (pagans                          ) - sarans                          \n",
      "                       ribaldry (ribaldries                      ) - bereaiiies                      \n",
      "Train on 35937 samples, validate on 3992 samples\n",
      "Epoch 1/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6486 - acc: 0.8016 - val_loss: 0.6549 - val_acc: 0.7981\n",
      "Epoch 2/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6501 - acc: 0.8004 - val_loss: 0.6437 - val_acc: 0.8036\n",
      "Epoch 3/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6462 - acc: 0.8020 - val_loss: 0.6454 - val_acc: 0.8032\n",
      "Epoch 4/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6439 - acc: 0.8031 - val_loss: 0.6418 - val_acc: 0.8049\n",
      "Epoch 5/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6436 - acc: 0.8029 - val_loss: 0.6420 - val_acc: 0.8041\n",
      "Epoch 6/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6438 - acc: 0.8026 - val_loss: 0.6398 - val_acc: 0.8052\n",
      "Epoch 7/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6402 - acc: 0.8042 - val_loss: 0.6395 - val_acc: 0.8054\n",
      "Epoch 8/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.7585 - acc: 0.7765 - val_loss: 0.7048 - val_acc: 0.7827\n",
      "Epoch 9/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6843 - acc: 0.7887 - val_loss: 0.6594 - val_acc: 0.7973\n",
      "Epoch 10/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6516 - acc: 0.8001 - val_loss: 0.6442 - val_acc: 0.8041\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 18\n",
      "                           plot (plots                           ) - sonts                           \n",
      "                      gelechiid (gelechiids                      ) - sereeaiers                      \n",
      "                       flatfish (flatfish                        ) - corosii                         \n",
      "                   masterstroke (masterstrokes                   ) - cereooaaiiies                   \n",
      "                   crassulaceae (crassulaceaes                   ) - cereooaaiiaes                   \n",
      "                         testee (testees                         ) - sariies                         \n",
      "                        remount (remounts                        ) - corsints                        \n",
      "                       pouteria (pouterias                       ) - sarooiias                       \n",
      "                      taximeter (taximeters                      ) - sareetiers                      \n",
      "                       carriage (carriages                       ) - sareiiies                       \n",
      "Train on 35937 samples, validate on 3992 samples\n",
      "Epoch 1/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6430 - acc: 0.8035 - val_loss: 0.6397 - val_acc: 0.8051\n",
      "Epoch 2/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6398 - acc: 0.8046 - val_loss: 0.6374 - val_acc: 0.8060\n",
      "Epoch 3/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6385 - acc: 0.8048 - val_loss: 0.6359 - val_acc: 0.8062\n",
      "Epoch 4/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6368 - acc: 0.8052 - val_loss: 0.6345 - val_acc: 0.8066\n",
      "Epoch 5/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6571 - acc: 0.7964 - val_loss: 0.6444 - val_acc: 0.8029\n",
      "Epoch 6/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6460 - acc: 0.8008 - val_loss: 0.6370 - val_acc: 0.8052\n",
      "Epoch 7/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6367 - acc: 0.8049 - val_loss: 0.6349 - val_acc: 0.8066\n",
      "Epoch 8/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6346 - acc: 0.8059 - val_loss: 0.6319 - val_acc: 0.8075\n",
      "Epoch 9/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6363 - acc: 0.8048 - val_loss: 0.7206 - val_acc: 0.7791\n",
      "Epoch 10/10\n",
      "35937/35937 [==============================] - 4s 109us/step - loss: 0.6588 - acc: 0.7962 - val_loss: 0.6347 - val_acc: 0.8062\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 19\n",
      "                  transmigrante (transmigrantes                  ) - seeeeiiaaiines                  \n",
      "                       niobrara (niobraras                       ) - sareaiias                       \n",
      "                     redemption (redemptions                     ) - sareoations                     \n",
      "                        ouranos (ourano                          ) - corssu                          \n",
      "                      semaphore (semaphores                      ) - corootiies                      \n",
      "                      diplomate (diplomates                      ) - corootiies                      \n",
      "                      shortness (shortnesses                     ) - pareeeesses                     \n",
      "                     prosthesis (prostheses                      ) - pereeenes                       \n",
      "                         hadean (hadeans                         ) - saniens                         \n",
      "                   salviniaceae (salviniaceaes                   ) - seeeeeaaiiaes                   \n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 2048\n",
    "\n",
    "# Train the model each generation and show predictions against the validation\n",
    "# dataset.\n",
    "for iteration in range(1, 20):\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=BATCH_SIZE,\n",
    "              epochs=10,\n",
    "              validation_data=(x_val, y_val))\n",
    "    print()\n",
    "    print('-' * 50)\n",
    "    print('Iteration', iteration)\n",
    "    # Select 10 samples from the validation set at random so we can visualize\n",
    "    # errors.\n",
    "    for i in range(10):\n",
    "        ind = np.random.randint(0, len(x_val))\n",
    "        rowx, rowy = x_val[np.array([ind])], y_val[np.array([ind])]\n",
    "        preds = model.predict_classes(rowx, verbose=0)\n",
    "        q = ctable.decode(rowx[0])\n",
    "        correct = ctable.decode(rowy[0])\n",
    "        guess = ctable.decode(preds[0], calc_argmax=False)\n",
    "        print(q[::-1] if INVERT else q, '(%s)' % correct, '-', guess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plays' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-f75574c8dd76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mshakespeare\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstrip_headers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_etext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_lower\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtoken_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m pairs = [(token[i], token[i + 1], token_id) \n",
      "\u001b[0;31mNameError\u001b[0m: name 'plays' is not defined"
     ]
    }
   ],
   "source": [
    "shakespeare = strip_headers(load_etext(100))\n",
    "tokens = [tuple(word) for word in tokenize(plays, to_lower=True)]\n",
    "token_counts = Counter(tokens)\n",
    "\n",
    "pairs = [(token[i], token[i + 1], token_id) \n",
    "         for token_id, token in enumerate(tokens) \n",
    "         for i in range(len(token) - 1)]\n",
    "\n",
    "pairs[10], tokens[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gutenberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter, defaultdict\n",
    "from gutenberg.acquire.text import UnknownDownloadUriException\n",
    "import re\n",
    "from gensim.utils import tokenize\n",
    "import random\n",
    "import nltk\n",
    "from gutenberg.acquire import load_etext\n",
    "from gutenberg.cleanup import strip_headers\n",
    "import os\n",
    "import glob\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/gutenberg_index.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-ef9a3aac07f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data/gutenberg_index.json'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mauthors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mrecent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mauthors\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m'birthdate'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'birthdate'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1830\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'birthdate'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'english_books'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrecent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/gutenberg_index.json'"
     ]
    }
   ],
   "source": [
    "with open('./data/gutenberg_index.json') as fin:\n",
    "    authors = json.load(fin)\n",
    "recent = [x for x in authors if 'birthdate' in x and x['birthdate'] > 1830]\n",
    "[(x['name'], x['birthdate'], x['english_books']) for x in recent[:5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'list_supported_metadatas' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-38586656dfa2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_supported_metadatas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'list_supported_metadatas' is not defined"
     ]
    }
   ],
   "source": [
    "print(list_supported_metadatas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidSchema",
     "evalue": "Missing dependencies for SOCKS support.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidSchema\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-a80200f898dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mconversations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_conversations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstrip_headers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_etext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10008\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconversations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/cookbook/lib/python3.6/site-packages/gutenberg/acquire/text.py\u001b[0m in \u001b[0;36mload_etext\u001b[0;34m(etextno, refresh_cache, mirror, prefer_ascii)\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcached\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcached\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0mdownload_uri\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_format_download_uri\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metextno\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmirror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefer_ascii\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdownload_uri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;31m# Ensure proper UTF-8 saving. There might be instances of ebooks or\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/cookbook/lib/python3.6/site-packages/gutenberg/acquire/text.py\u001b[0m in \u001b[0;36m_format_download_uri\u001b[0;34m(etextno, mirror, prefer_ascii)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0muri_root\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmirror\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_GUTENBERG_MIRROR\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0muri_root\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muri_root\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0m_check_mirror_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri_root\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;31m# Check https://www.gutenberg.org/files/ for details about available\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/cookbook/lib/python3.6/site-packages/gutenberg/_util/decorators.py\u001b[0m in \u001b[0;36mcall_once\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcall_once\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_called\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                 \u001b[0mcall_once\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_called\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/cookbook/lib/python3.6/site-packages/gutenberg/acquire/text.py\u001b[0m in \u001b[0;36m_check_mirror_exists\u001b[0;34m(mirror)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mexecute_only_once\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_check_mirror_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmirror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmirror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mok\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         raise UnknownDownloadUriException(\n",
      "\u001b[0;32m~/anaconda2/envs/cookbook/lib/python3.6/site-packages/requests/api.py\u001b[0m in \u001b[0;36mhead\u001b[0;34m(url, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'allow_redirects'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'head'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/cookbook/lib/python3.6/site-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/cookbook/lib/python3.6/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    516\u001b[0m         }\n\u001b[1;32m    517\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 518\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/cookbook/lib/python3.6/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 639\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/cookbook/lib/python3.6/site-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    401\u001b[0m         \"\"\"\n\u001b[1;32m    402\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m         \u001b[0mconn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcert_verify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/cookbook/lib/python3.6/site-packages/requests/adapters.py\u001b[0m in \u001b[0;36mget_connection\u001b[0;34m(self, url, proxies)\u001b[0m\n\u001b[1;32m    299\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mproxy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m             \u001b[0mproxy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepend_scheme_if_needed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproxy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'http'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m             \u001b[0mproxy_manager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproxy_manager_for\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproxy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m             \u001b[0mconn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproxy_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnection_from_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/cookbook/lib/python3.6/site-packages/requests/adapters.py\u001b[0m in \u001b[0;36mproxy_manager_for\u001b[0;34m(self, proxy, **proxy_kwargs)\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0mmaxsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool_maxsize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m                 \u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool_block\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m                 \u001b[0;34m**\u001b[0m\u001b[0mproxy_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m             )\n\u001b[1;32m    188\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/cookbook/lib/python3.6/site-packages/requests/adapters.py\u001b[0m in \u001b[0;36mSOCKSProxyManager\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mSOCKSProxyManager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mInvalidSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Missing dependencies for SOCKS support.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0mDEFAULT_POOLBLOCK\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidSchema\u001b[0m: Missing dependencies for SOCKS support."
     ]
    }
   ],
   "source": [
    "PARAGRAPH_SPLIT_RE = re.compile(r'\\n *\\n+')\n",
    "\n",
    "def extract_conversations(text, quote='\"'):\n",
    "    paragraphs = PARAGRAPH_SPLIT_RE.split(text.strip())\n",
    "    conversations = [['']]\n",
    "    for paragraph in paragraphs:\n",
    "        chunks = paragraph.replace('\\n', ' ').split(quote)\n",
    "        for i in range((len(chunks) + 1) // 2):\n",
    "            if (len(chunks[i * 2]) > 100 or len(chunks) == 1) and conversations[-1] != ['']:\n",
    "                if conversations[-1][-1] == '':\n",
    "                    del conversations[-1][-1]\n",
    "                conversations.append([''])\n",
    "            if i * 2 + 1 < len(chunks):\n",
    "                chunk = chunks[i * 2 + 1]\n",
    "                if chunk:\n",
    "                    if conversations[-1][-1]:\n",
    "                        if chunk[0] >= 'A' and chunk[0] <= 'Z':\n",
    "                            if conversations[-1][-1].endswith(','):\n",
    "                                conversations[-1][-1] = conversations[-1][-1][:-1]\n",
    "                            conversations[-1][-1] += '.'\n",
    "                        conversations[-1][-1] += ' '\n",
    "                    conversations[-1][-1] += chunk\n",
    "        if conversations[-1][-1]:\n",
    "            conversations[-1].append('')\n",
    "\n",
    "    return [x for x in conversations if len(x) > 1]\n",
    "\n",
    "\n",
    "conversations = extract_conversations(strip_headers(load_etext(10008).strip()))\n",
    "sum(len(x) for x in conversations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LATIN_1_CHARS = (\n",
    "    (u'\\xe2\\x80\\x99', \"'\"),\n",
    "    (u'\\xc3\\xa9', 'e'),\n",
    "    (u'\\xe2\\x80\\x90', '-'),\n",
    "    (u'\\xe2\\x80\\x91', '-'),\n",
    "    (u'\\xe2\\x80\\x92', '-'),\n",
    "    (u'\\xe2\\x80\\x93', '-'),\n",
    "    (u'\\xe2\\x80\\x94', '-'),\n",
    "    (u'\\xe2\\x80\\x94', '-'),\n",
    "    (u'\\xe2\\x80\\x98', \"'\"),\n",
    "    (u'\\xe2\\x80\\x9b', \"'\"),\n",
    "    (u'\\xe2\\x80\\x9c', '\"'),\n",
    "    (u'\\xe2\\x80\\x9c', '\"'),\n",
    "    (u'\\xe2\\x80\\x9d', '\"'),\n",
    "    (u'\\xe2\\x80\\x9e', '\"'),\n",
    "    (u'\\xe2\\x80\\x9f', '\"'),\n",
    "    (u'\\xe2\\x80\\xa6', '...'),\n",
    "    (u'\\xe2\\x80\\xb2', \"'\"),\n",
    "    (u'\\xe2\\x80\\xb3', \"'\"),\n",
    "    (u'\\xe2\\x80\\xb4', \"'\"),\n",
    "    (u'\\xe2\\x80\\xb5', \"'\"),\n",
    "    (u'\\xe2\\x80\\xb6', \"'\"),\n",
    "    (u'\\xe2\\x80\\xb7', \"'\"),\n",
    "    (u'\\xe2\\x81\\xba', \"+\"),\n",
    "    (u'\\xe2\\x81\\xbb', \"-\"),\n",
    "    (u'\\xe2\\x81\\xbc', \"=\"),\n",
    "    (u'\\xe2\\x81\\xbd', \"(\"),\n",
    "    (u'\\xe2\\x81\\xbe', \")\")\n",
    ")\n",
    "\n",
    "books = 0\n",
    "for author in recent[:1000]:\n",
    "    for book in author['books']:\n",
    "        books += 1\n",
    "        try:\n",
    "            txt = strip_headers(load_etext(int(book[0]))).strip()\n",
    "        except UnknownDownloadUriException:\n",
    "            continue\n",
    "        for ch1, ch2 in LATIN_1_CHARS:\n",
    "            txt = txt.replace(ch1, ch2)\n",
    "        conversations += extract_conversations(txt)\n",
    "\n",
    "print(len(conversations), books)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "302px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
