{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 词的表示"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此内嵌常用于 NLP，即把一组词映射为一组向量，前面提到的 one-hot 就是其中的一种，one-hot 的缺点是不能表示词之间的相似性。在任何给定的语料中，我们计算`(cat, dog)(knife, spoon)` 之间的相似度是通过点乘来计算，但 one-hot 计算出的结果总是 0.\n",
    "\n",
    "常见的方法是基于 IR（information retrieval），如 TF-IDF、LSA（latent semantic Analysis）、topic modeling等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**分布式表示**\n",
    "\n",
    "如下示例：\n",
    "```\n",
    "Pairs is the capital of France.\n",
    "Berlin is the capital of Germany.\n",
    "```\n",
    "不需要英语知识我们就可以知道，Pairs 与 Berlin、France 与 Germany 是类似的。而分布式表示即使寻找一个转换函数 $\\phi$ ，来转换每个词与它相关的向量，如下：\n",
    "$$\n",
    "\\large{\n",
    "\\begin{split}\n",
    "\\phi(Pairs) - \\phi(France) \\approx \\phi(Berlin) - \\phi(Germany)\n",
    "\\end{split}\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "常见的架构有两种\n",
    "- Continuous Bag of Words（CBOW）\n",
    "- Skip-gram\n",
    "\n",
    "经过实现 CBOW 更快。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## skip-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "他是基于中心词的，如下：\n",
    "```\n",
    "I love green eggs and ham\n",
    "```\n",
    "假设 窗口的大小为 3 ，那么上面的句子可以切分成如下格式：\n",
    "```\n",
    "([I, green], love)\n",
    "([love, eggs], green)\n",
    "([green, and], eggs)\n",
    "...\n",
    "```\n",
    "根据中心词，我们可以将处理的数据集翻译成 `(input, output)` (input 就是中心词，output 就是上面方括号里面的词)，那么上面的示例就可以写成如下形式：\n",
    "```\n",
    "(love, I), (love, green), (green, love), (green, eggs), (eggs, green), (eggs, and), ...\n",
    "```\n",
    "上面的数据可以当做正例，之后在随机的生成一些负例：\n",
    "```\n",
    "(love, Sam), (love, zebra), (green, thing),...\n",
    "```\n",
    "\n",
    "这样就可以组成我们的训练数据集：\n",
    "```\n",
    "((love, I), 1), ((love, green), 1), ... ((love, Sam), 0), ((love, zebra), 0),..\n",
    "```\n",
    "\n",
    "示意图如下：\n",
    "![image](https://ws4.sinaimg.cn/large/69d4185bly1fyfdaqwks1j20c00bqmxp.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras 可以实现，假设词汇表大小为 5000，词内嵌大小为 300，窗口大小为 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Merge\n",
    "from keras.layers.core import Dense, Reshape\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import Sequential\n",
    "vocab_size = 5000\n",
    "embed_size = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word\n",
    "word_model = Sequential()\n",
    "word_model.add(Embedding (vocab_size, embed_size,\n",
    "                          embeddings_ initializer=\"glorot_uniform\",\n",
    "                          input_length=1) )\n",
    "word_model.add(Reshape( (embed_size, ) ) )\n",
    "\n",
    "# context words\n",
    "context_model = Sequential()\n",
    "context_model.add(Embedding (vocab_size, embed_size,\n",
    "                          embeddings_ initializer=\"glorot_uniform\",\n",
    "                          input_length=1) )\n",
    "context_model.add(Reshape( (embed_size, ) ) )\n",
    "\n",
    "# merge\n",
    "model = Sequential()\n",
    "model.add (Merge([word_ model, context_ model], mode=\"dot\") )\n",
    "model.add(Dense(1, init=\"glorot_uniform\", activation=\"sigmoid\") )\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=\"adam\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras 提供的字符处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56 56\n",
      "(and (5), eggs (4)) -> 1\n",
      "(and (5), green (3)) -> 0\n",
      "(green (3), i (1)) -> 0\n",
      "(eggs (4), and (5)) -> 0\n",
      "(eggs (4), i (1)) -> 0\n",
      "(ham (6), love (2)) -> 1\n",
      "(love (2), green (3)) -> 1\n",
      "(i (1), eggs (4)) -> 1\n",
      "(love (2), eggs (4)) -> 1\n",
      "(i (1), green (3)) -> 0\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import *\n",
    "from keras.preprocessing.sequence import skipgrams\n",
    "text = \"I love green eggs and ham .\"\n",
    "\n",
    "# 字符转换\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([text])\n",
    "\n",
    "# 创建字符表\n",
    "word2id = tokenizer.word_index\n",
    "id2word = {v:k for k,v in word2id.items()}\n",
    "\n",
    "# 提取前10个字符对\n",
    "wids = [word2id[w] for w in text_to_word_sequence(text)]\n",
    "pairs, labels = skipgrams(wids, len(word2id))\n",
    "print(len(pairs), len(labels))\n",
    "\n",
    "for i in range(10):\n",
    "    print(\"({:s} ({:d}), {:s} ({:d})) -> {:d}\".format(\n",
    "        id2word[pairs[i][0]], pairs[i][0],\n",
    "        id2word[pairs[i][1] ], pairs[i][1],\n",
    "        labels[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CBOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "处理的流程如下：\n",
    "![image](https://wx3.sinaimg.cn/large/69d4185bly1fyff3801sej20av0dodgn.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.core import Dense, Lambda\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import Sequential\n",
    "import keras.backend as K\n",
    "vocab_size = 5000\n",
    "embed_size = 300\n",
    "window_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding (input_dim=vocab_ size, output_dim=embed_size,\n",
    "                      embeddings_ initializer='glorot_uniform' ,\n",
    "                      input_length=window_ size*2) )\n",
    "model.add(Lambda (lambda x: K.mean(x, axis=1) , output_shape=(embed_size,)) )\n",
    "model.add(Dense (vocab_size, kernel_initializer='glorot_uniform',activation='softmax') )\n",
    "model.compile(loss='categorical_ crossentropy', optimizer=\"adam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "388px",
    "width": "160px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
