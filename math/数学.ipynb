{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数学"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 线性代数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 概念\n",
    "#### 矩阵基础概念\n",
    "\n",
    "**单位矩阵**：矩阵对角线值都为 1 ，其他元素值都为零的方阵，，记做 $\\mathbf I$，如 3 × 3 的单位矩阵：\n",
    "$ \\mathbf I = \n",
    "\\left[\\begin{matrix} \n",
    " 1 & 0 & 0 \\\\\n",
    " 0 & 1 & 0 \\\\\n",
    " 0 & 0 & 1\n",
    "\\end{matrix} \\right ]\n",
    "$\n",
    "\n",
    "**单位向量（矢量）**：$\\mathbf L^2$ 范数为 $\\mathbf 1$ 的向量，如 $ \n",
    "\\mathbf a = \n",
    "\\left [ \\begin{matrix} \n",
    "\\frac{\\sqrt{2}}{2} \\\\ \n",
    "\\frac{\\sqrt{2}}{2}\n",
    "\\end{matrix} \\right ]\n",
    "$\n",
    "\n",
    "**逆矩阵**：对于n 阶方阵 A，若存在一个 n 阶矩阵 B 使得$\\mathbf{AB = BA = E}$，那么 B 称作 A 的逆矩阵，记做 $\\mathbf{A^{-1}=B}$，称 A 为可逆矩阵，\n",
    "\n",
    "**对角矩阵**：除了对角线之外，其他元素全为零，单位矩阵就是一种特殊的对角矩阵，如 \n",
    "$$\n",
    "\\mathbf D = \n",
    "\\left [\\begin{matrix}\n",
    "1 & 0 & 0 & 0 \\\\\n",
    "0 & 2 & 0 & 0 \\\\\n",
    "0 & 0 & 4 & 0 \\\\\n",
    "0 & 0 & 0 & 7 \n",
    "\\end{matrix} \\right ]\n",
    "$$\n",
    "也可以记做 $diag(\\mathbf v)$，其中 $\\mathbf v$ 为 $[1,2,4,7]^\\mathbf T$\n",
    "当紧当对角线的值不为零时，矩阵存在逆矩阵，即$diag(v)^{-1} = diag(\\left[\\frac{1}{v_1}, \\cdots, \\frac{1}{v_n} \\right]^\\mathbf T) $。在进行计算时，就是将相应的 x 值进行缩放，即 $diag(\\mathbf v) \\mathbf x = \\mathbf v \\bigodot \\mathbf x$\n",
    "\n",
    "\n",
    "**对称矩阵**：转置等于自身，如通过距离函数生成的矩阵，点(i,j)与点(j,i)到远点的距离是相等的，可以表示为，$\\mathbf A = \\mathbf A^\\mathbf T $。若矩阵中所有的值都是实数，那么就称此矩阵为*实对称矩阵*，实对称矩阵有以下几个特性：\n",
    "- 实对称矩阵A的不同特征值对应的特征向量是正交的。\n",
    "- 实对称矩阵A的特征值都是实数，特征向量都是实向量。\n",
    "- n阶实对称矩阵A必可对角化，且相似对角阵上的元素即为矩阵本身特征值。\n",
    "\n",
    "**正交向量**：当 $\\mathbf x^\\mathbf T \\mathbf y = 0$ 时，两向量正交。当两个向量都为非零向量时，两向量夹角为 90°。当两个向量的范数都为 1 的话，就说两向量是标准正交的（正规化）。\n",
    "\n",
    "**正交矩阵**：当矩阵的行互相正交，且列也互相正交，那么就说这个矩阵是一个正交矩阵，即 \n",
    "$\\mathbf A^\\mathbf T \\mathbf A = \\mathbf A \\mathbf A^\\mathbf T = \\mathbf I$，从中可以看到 $\\mathbf A^{-1} = \\mathbf A^{T}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 矩阵特征\n",
    "\n",
    "**特征向量、特征值**：矩阵 $\\mathbf A $ 的特征向量$\\mathbf v $可以表示为 $\\mathbf A \\mathbf v = \\lambda \\mathbf v$，其中 $\\lambda$ 为特征值。\n",
    "- 当一个矩阵的所有特征值都是正数时，称此矩阵是*正定的（positive definite）*，可以保证 $\\mathbf{x^{T}Ax = 0 \\Rightarrow x = 0}$\n",
    "- 当特征值全是正数或者0时，称此矩阵是*半正定的（positive semideﬁnite）*，可以保证$\\mathbf{\\forall x,  x^{T}Ax \\geqslant 0}$\n",
    "\n",
    "同理，当特征值都是负值时称为*负定矩阵（negative deﬁnite）*，当特征值都是负值或者0时称为*半负定矩阵（negative  semideﬁnite）*\n",
    "\n",
    "**特征值分解**：*只能用于方阵*，假设矩阵 $\\mathbf A$ 是由 n 个线性独立的特征值组成 $\\{\\mathbf v^{(1)},\\cdots,\\mathbf v^{(n)} \\}$，其中特征值为 $\\{\\mathbf \\lambda_{1},\\cdots,\\mathbf \\lambda_{n} \\}$，把所有的特征向量连接成矩阵$\\mathbf V=\\{\\mathbf v^{(1)},\\cdots,\\mathbf v^{(n)} \\}$, 所有的特征值连接成$\\mathbf λ = \\{\\mathbf \\lambda_{1},\\cdots,\\mathbf \\lambda_{n} \\}$，那么矩阵 $\\mathbf A$ 的特征值分解可以表示为: $$\\mathbf A = \\mathbf V diag(λ) \\mathbf V^{-1}$$\n",
    "\n",
    "**奇异矩阵**：奇异矩阵是对方阵来说的，当方阵的行列式|A| 或者任何特征值都为 0 时，就是奇异矩阵，不为 0 则为非奇异矩阵。\n",
    "\n",
    "**奇异值分解（SVD Singular Value Decomposition）**：另一种分解矩阵的方式，将矩阵分解成奇异值（singular values）与 奇异向量（singular vectors）。每个实数矩阵都可以进行奇异值分解，但不一定可以进行特征值分解。奇异值分解可以写作$$\\mathbf A = \\mathbf{UDV^{T}}$$\n",
    "若 A 是一个 m×n 的矩阵，那么 U 是一个 m×m 的矩阵，D 是一个 m×n 的矩阵，T 是一个 n×n 的矩阵。其中 U、V 需要时正交矩阵，D 是一个对角矩阵，但可以不是方阵。\n",
    "\n",
    "D 对角线上的元素成为矩阵 A 的奇异值，U 的列成为 A 的左奇异向量（left-singular vectors），V 的列是 A 的右奇异向量（right-singular vectors）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**矩阵的迹（Trace）**：n 阶方阵 A 主对角线上元素的和成为矩阵的迹，记做 $\\mathbf{tr(A) = \\sum\\limits_{i} A_{i,i}}$。标量的迹是她自己$a = tr(a)$。\n",
    "- $\\mathbf{tr(A) = tr(A^{T})}$\n",
    "- 循环特性(矩阵的维度相关)：$\\mathbf{tr(ABC) = tr(CAB) = tr(BCA)}$，更一般的写法：$\\mathbf{tr(\\prod\\limits_{i=1}^{n}F^{(i)}) = tr(F^{(n)}\\prod\\limits_{i=1}^{n-1}F^{(i)})}$\n",
    "- 在维度不相关时一样有：$\\mathbf{tr(AB) = tr(BA)}$\n",
    "\n",
    "**行列式(determinate)** ：记做$\\mathbf{det(A)}$ ，其值等于矩阵所有特征值的乘积。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基础\n",
    "- [deep learning book linear_algebra](http://www.deeplearningbook.org/contents/linear_algebra.html#pf6)\n",
    "- [斯坦福线性代数 - Linear Algebra Review and Reference](https://see.stanford.edu/materials/aimlcs229/cs229-linalg.pdf)\n",
    "- [矩阵参考手册-The Matrix CookBook](https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 求导\n",
    "求导法则：\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial y x}{\\partial x}=y^T \\\\\n",
    "\\frac{\\partial(x^TA x)}{\\partial x}=(A+A^T)x \\\\\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "\n",
    "- [机器学习中的线性代数之矩阵求导](https://blog.csdn.net/u010976453/article/details/54381248)\n",
    "\n",
    "- [向量的L2范数求导](https://www.cnblogs.com/nowgood/p/fanshuqiudao.html)\n",
    "\n",
    "- [wikipedia - Matrix calculus](https://en.wikipedia.org/wiki/Matrix_calculus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 概论\n",
    "- [Probability and InformationTheory](http://www.deeplearningbook.org/contents/prob.html)\n",
    "- [Jaynes Probability Theory](http://www.med.mcgill.ca/epidemiology/hanley/bios601/GaussianModel/JaynesProbabilityTheory.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 相关概念\n",
    "- 置信度（degree of belief）\n",
    "- 频率论概论（frequentist probability）\n",
    "- 贝叶斯概论（Bayesian probability）\n",
    "- 中心极限定理 (central limit theorem)\n",
    "- 潜在变量（Latent Variable）\n",
    "- 高斯混合模型（Gaussian mixture model）\n",
    "- 先验概率（prior probability）\n",
    "- 后验概率（posterior probability）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基础\n",
    "#### 离散变量\n",
    "可以使用离散概率质量函数（probability mass function PMF）描述，通过记做大写的 $P$，每一个 $P$ 都会关联一个随机变量，如 $P(x)$ 不同 $P(y)$.\n",
    "\n",
    "通常 $X=x$ 的概率记做 $P(x)$ ，值为 1 表示 $X=x$ 是确定的，值为 0 表示 $X=x$ 是不可能的。\n",
    "\n",
    "为了消除歧义，以上通常写作 $P(X=x)$。 有时也会先定义变量$X$，然使用 $\\thicksim$ 在后面跟上他符合的分布：$X \\thicksim P(X)$\n",
    "\n",
    "**联合概率分布（joint probabilitydistribution）**：$P(X=x, Y=y)$ 或者简记为 $P(x,y)$\n",
    "\n",
    "**P应满足的条件**：\n",
    "- P的阈值必须是 X 的所有可能值\n",
    "- $\\forall x \\in X , \\qquad 0 \\leq P(x) \\leq 1$\n",
    "- $\\sum_{x \\in X}P(x) = 1$，此特性被称为正规化（normalized）\n",
    "\n",
    "**均匀分布**：让离散随机变量 X 有 k 个不同的状态，在 X 上的均匀分布的 PMF 为: $P(X=x_{i})=\\frac{1}{k} $，可以得到 $\\sum_{i}P(X=x_{i})=\\sum_{i}\\frac{1}{k} = \\frac{k}{k} = 1$，即分布式正规划的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 连续变量\n",
    "连续变量使用概率密度函数（ probability density function PDF）来描述，而不是概率质量函数（PMF）。此时的 $p$ 应该满足一下的条件：\n",
    "- $p$ 的阈值必须是 X 的所有可能值\n",
    "- $\\forall x \\in X , \\qquad p(x) \\geq 0$ ，但不需要 $p(x) \\leq 1$ 的限制\n",
    "- $\\int p(x)dx = 1$\n",
    "\n",
    "概率密度函数并没有直接给出某个状态的概率，而是通过一个极小的区域 $\\delta x$ 与 $p(x)$ 的乘积给出：$p(x)\\delta x$。通过积分可以求出某一点具体的概率，特别是 x 在某个区域上，可通过以下公式求出：$\\int_{[a,b]}p(x)dx$\n",
    "\n",
    "一种特殊的概率密度分布，在实数上间隔的均匀分布 $u(x;a,b)$ ，其中 a、b 是就间隔的端点，且 b > a，\";\" 表示被参数化。为了保证间隔不在端点之外令 $\\forall x \\notin [a,b] \\qquad u(x;a,b)=0$，在 [a,b] 以内有 $u(x;a,b)=\\frac{1}{b-a}$，可以看到其积分值也为 1 。x 在[a,b]范围内服从均匀分布可以记为 $X \\thicksim U(a,b)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 边缘概率(marginal probability)\n",
    "\n",
    "当知道了一组变量的概率分布之后，想要求变量子集的分布，此种分布就成为边缘概率分布（marginal probability distribution）。\n",
    "\n",
    "如对于离散随机变量X、Y，知道他们的联合概率分布 P(X,Y)，那么通过求和法则就可以找到 P(X)，$\\forall x \\in X, P(X=x)=\\sum_{y}P(X=x,Y=y)$，对于连续的则需要改成积分形式 $p(x) = \\int p(x,y)dy$\n",
    "\n",
    "#### 条件概率（Conditional Probability）\n",
    "\n",
    "通过已知事件来求另一些时间的概率叫做条件概率。即在 X=x 的条件下 Y=y 的概率记做：$P(Y=y | X=x)$。可以通过以下的方式计算：$P(Y=y | X=x)=\\frac{P(X=x,Y=y)}{P(X=x)}$\n",
    "\n",
    "#### 条件概率的链式法则\n",
    "很多变量的联合概率分布可以分解为只有一个变量的条件概率，即：$P(X^{(1)},\\cdots,X^{(n)})=P(X^{(1)})\\prod_{i=2}^{n} P(X^{(i)}|X^{(1)},\\cdots,X^{(i-1)})$ 。这个被称为链式法则或者乘法法则。\n",
    "\n",
    "如下示例，使用两次上述法则：\n",
    "$$P(a,b,c)=P(a|b,c)P(b,c) \\tag{1}$$\n",
    "$$P(b,c)=P(b|c)P(c) \\tag{2}$$\n",
    "$$P(a,b,c)=P(a|b,c)P(b|c)P(c) \\tag{3}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 独立与条件独立\n",
    "**独立**：两个随机变量x、y独立可以写成如下形式，$\\forall x \\in X,y \\in Y, P(X=x,Y=y)=P(X=x)P(Y=y)$，可以记为 $X \\perp Y$\n",
    "\n",
    "**条件独立**：在给定的变量 z 的条件下，若随机变量 x、y 可以写成如下形式：\n",
    "$$\\forall x \\in X, y \\in Y, z \\in Z, P(X = x, Y = y | Z = z) = P(X = x | Z = z)P(Y = y | Z = z)$$\n",
    "可以记为 $X \\perp Y | Z$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 期望（Expectation）、方差（Variance）、协方差（Covariance）\n",
    "**期望**\n",
    "\n",
    "关于概率分布 P(X) 的函数 $f(x)$ 的期望是函数 $f$ 作用在来自于 P 的 x，离散情况计算如下：\n",
    "$$\\mathbb{E}_{X \\thicksim P}[f(x)] = \\sum_{x}P(x)f(x)$$\n",
    "连续的变量计算如下：\n",
    "$$\\mathbb{E}_{X \\thicksim P}[f(x)] = \\int p(x)f(x)dx$$\n",
    "\n",
    "当 P 明确时可以简记成如下 $\\mathbb{E}_{x}[f(x)]$，当 X 与 P 都明确的情况下，上述可以简记成如下形式 $\\mathbb{E}[f(x)]$.\n",
    "\n",
    "期望是线性的，当 α与β不宜懒与 x 时，即有如下形式：\n",
    "$$\\mathbb{E}[\\alpha f(x) + \\beta g(x)] = \\alpha \\mathbb{E}[f(x)] + \\beta \\mathbb{E}[g(x)]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**方差**\n",
    "\n",
    "公式如下：\n",
    "$$Var(f(x)) = \\mathbb{E}[(f(x) − \\mathbb{E}[f(x)])^{2}]$$\n",
    "\n",
    "方差越小，f(x) 越接近期望值。方差的平方根称作标准差（standard variance）。\n",
    "\n",
    "**标准差**\n",
    "\n",
    "标准差表示数据波动的情况，标准差越小数据越集中，标准差越大数据波动越大。\n",
    "\n",
    "\n",
    "**协方差**\n",
    "\n",
    "给出了两个值得相关性，可以表示成如下形式：\n",
    "$$Cov(f(x), g(y)) = \\mathbb{E} [(f(x) − \\mathbb{E} [f(x)]) (g(y) − \\mathbb{E} [g(y)])] $$\n",
    "\n",
    "协方差越大表明两个变量的相关性越低。\n",
    "\n",
    "两个独立的变量，协方差为 0 ；当协方差不为 0 时，两个变量相关。\n",
    "\n",
    "**协方差矩阵**\n",
    "\n",
    "对于一个随机向量 $x \\in \\mathbb{R}^{n}$，他的协方差矩阵是一个 n×n 的方阵：$Cov(X)_{i,j}= Cov(x_i, x_j)$。对角线上的元素是方差：$Cov(x_i, x_i) = Var(x_i)$\n",
    "\n",
    "各向同性（isotropic）协方差矩阵表示各个方向的方差相同；对角协方差矩表示可以分别控制轴对齐方向的方差；满秩协方差矩阵表示可以沿着任意方向控制方差，如下图：![image](//wx3.sinaimg.cn/large/69d4185bly1ftkqwuzw1dj20jm0g8jta.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 贝叶斯公式\n",
    "$$P(X|Y)=\\frac{P(X)P(Y|X)}{P(Y)}$$\n",
    "其中 P(Y) 可以通过如下公式计算：\n",
    "$$P(Y) = \\sum_{x} P(Y|x)P(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 常见的概率分布\n",
    "#### 伯努利分布（Bernoulli Distribution）\n",
    "是一个单二进制变量分布（single binary random variable），通过参数 $\\phi \\in [0,1]$ 来控制，具有以下的属性\n",
    "- $P(X=1) = \\phi$\n",
    "- $P(X=0) = 1 - \\phi$\n",
    "- $P(X=x) = \\phi^{x}(1 - \\phi)^{1-x}$\n",
    "- $\\mathbb{E}_x[x] = \\phi$\n",
    "- $Var_x(x) = \\phi(1 − \\phi)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 正态分布（高斯分布）\n",
    "关于实数的分布，也是最常用的分布，公式如下：\n",
    "$$\\mathcal{N(x;\\mu,\\sigma^{2})}=\\sqrt{\\frac{1}{2\\pi\\sigma^{2}}}exp\\left(-\\frac{1}{2\\sigma^2}(x-\\mu)^2\\right)$$\n",
    "\n",
    "其中 $\\mu \\in \\mathbb{R},\\; \\sigma \\in (0, \\infty)$，μ 是中间峰值的高度，同时也是期望值，即 $\\mathbb E[x] = µ$。标准差是 $\\sigma$ ，方差是 $\\sigma^{2}$\n",
    "\n",
    "**多元正态分布**\n",
    "\n",
    "$$\\mathcal{N(x;\\mathbf{\\mu,\\Sigma})}=\\sqrt{\\frac{1}{(2\\pi)^{n}det(\\Sigma)}}exp\\left(-\\frac{1}{2}(x-\\mu)^T\\Sigma^{-1}(x-\\mu)\\right)$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 信息论\n",
    "- [Probability and InformationTheory](http://www.deeplearningbook.org/contents/prob.html)\n",
    "\n",
    "----\n",
    "- [Elements of information theory](http://staff.ustc.edu.cn/~cgong821/Wiley.Interscience.Elements.of.Information.Theory.Jul.2006.eBook-DDU.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从不太可能发生的事情中了解到的信息比可能发生的事情多。\n",
    "\n",
    "定义事件 X=x 的自信息（self-information）为 ：\n",
    "$$I(x) = -logP(x)$$\n",
    "\n",
    "其中 log 是以 e 为底的自然对数，I 的单位为 纳特（nats），一纳特的信息量是通过观察的概率 $\\frac{1}{e} 得到$。当以 2 为底数时，I 的单位为比特（bits）或者香农（shannons）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数值计算\n",
    "- [deep learning book - numerical](http://www.deeplearningbook.org/contents/numerical.html)\n",
    "- [矩阵的条件数（condition number）](https://blog.csdn.net/lanchunhui/article/details/51372831)\n",
    "\n",
    "\n",
    "### 概念\n",
    "- 偏导数（partial derivatives）\n",
    "- 牛顿法（Newton’s method）\n",
    "- 李普希茨连续函数（Lipschitz continuous）\n",
    "- 数值梯度（numerical gradient）【近似但简单，速度慢】\n",
    "- 分析梯度（analytic gradient）【速度快，容易出错】\n",
    "- 中心差分公式（centered difference formula）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 导数\n",
    "**反向传播**\n",
    "\n",
    "- [Backpropagation, Intuitions](http://cs231n.github.io/optimization-2/)\n",
    "- [Vector, Matrix, and Tensor Derivatives - Erik Learned-Miller](http://cs231n.stanford.edu/vecDerivs.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 偏导数\n",
    "常用于多变量求导，$\\frac{\\partial}{\\partial x_i}f(\\mathbf x)$ 表示在 $\\mathbf x$ 点的变量 $x_i$ 增加是 $f$ 改变了多少。对于向量，f 的导数也是向量，其中包含了所有的偏导，可以记做 $\\nabla_x f(x)$。\n",
    "\n",
    "在高维时，临界点（critical points）是每个元素的梯度都为零的点。\n",
    "\n",
    "在 $u$ 方向导数（directional derivate）是 f 在 $u$ 方向上的斜率。也就是说，方向导数是关于$\\alpha$的函数$f(x + \\alpha u)$的方向导数。通过链式法则对$\\alpha$求导$\\frac{\\partial}{\\partial \\alpha}f(x + \\alpha u)$ 得到 $u^{T}\\nabla_x f(x)$ 在 $\\alpha = 0$时得到最小值。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 雅各比（Jacobian）与黑赛（Hessian）矩阵\n",
    "- [百科-雅可比矩阵](https://baike.baidu.com/item/%E9%9B%85%E5%8F%AF%E6%AF%94%E7%9F%A9%E9%98%B5)\n",
    "- [百科-海赛矩阵](https://baike.baidu.com/item/%E9%BB%91%E5%A1%9E%E7%9F%A9%E9%98%B5/2248782)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在向量分析中，雅可比矩阵是函数的一阶偏导数以一定方式排列成的矩阵，其行列式称为雅可比行列式。\n",
    "\n",
    "如果我们有一个函数：$f:\\mathbb R^m \\to \\mathbb R^n$ 那么雅各比矩阵就是$\\mathbf J \\in \\mathbb R^{n × m}$ 其定义形式如下：${J}_{i,j} = \\frac{\\partial}{\\partial x_j}f(\\mathbf x)_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**二阶导数**\n",
    "\n",
    "即导数的导数，可以记为$\\frac{\\partial^2}{\\partial x_i \\partial x_j}f$，在一维的情况下可以记为：$\\frac{d^2}{dx^2}f$ 或者 $f''(x)$。\n",
    "\n",
    "他告诉了我们，当改变输入的时候一阶导数如何变化。\n",
    "\n",
    "二阶偏导操作具有交换特性：\n",
    "$$\\frac{\\partial^2}{\\partial x_i \\partial x_j}f(\\mathbf x) = \\frac{\\partial^2}{\\partial x_j \\partial x_i}f(\\mathbf x)$$\n",
    "\n",
    "**海赛矩阵**\n",
    "\n",
    "海赛矩阵可以用来表示多维输入二阶导数，其形式如下：\n",
    "$$\\mathbf{H}(f)(\\mathbf x)_{i,j} = \\frac{\\partial^2}{\\partial x_i \\partial x_j}f(\\mathbf x)$$\n",
    "可知，海赛矩阵是雅各比矩阵的梯度。由二阶导出操作具有可交换性，可知 $H_{i,j} = H_{j,i}$，因此海赛矩阵在此点具有对称性。\n",
    "![image](//ws1.sinaimg.cn/large/69d4185bly1ftlwwmdoahj20jh0d878d.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](//ws1.sinaimg.cn/large/69d4185bly1ftlx4dkvdqj20jb0g90w2.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "294px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
