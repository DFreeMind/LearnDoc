{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 监督学习\n",
    "\n",
    "与或非在数学中的表示：\n",
    "- 与： A^B\n",
    "- 或： AvB\n",
    "- 非： ¬A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 神经网络迷你项目\n",
    "### 创建一个感知器（perceptron）\n",
    "参考文章：http://blog.csdn.net/Dream_angel_Z/article/details/48915561"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# ----------\n",
    "# \n",
    "# In this exercise, you will add in code that decides whether a perceptron will fire based\n",
    "# on the threshold. Your code will go in lines 32 and 34. \n",
    "#\n",
    "# ----------\n",
    "import numpy as np\n",
    "\n",
    "class Perceptron:\n",
    "    \"\"\"\n",
    "    This class models an artificial neuron with step activation function.\n",
    "    \"\"\"\n",
    "    def __init__(self, weights = np.array([1]), threshold = 0):\n",
    "        \"\"\"\n",
    "        Initialize weights and threshold based on input arguments. Note that no\n",
    "        type-checking is being performed here for simplicity.\n",
    "        \"\"\"\n",
    "        self.weights = weights\n",
    "        self.threshold = threshold\n",
    "    \n",
    "    def activate(self,inputs):\n",
    "        \"\"\"\n",
    "        Takes in @param inputs, a list of numbers equal to length of weights.\n",
    "        @return the output of a threshold perceptron with given inputs based on\n",
    "        perceptron weights and threshold.\n",
    "        \"\"\" \n",
    "\n",
    "        # The strength with which the perceptron fires.\n",
    "        # 求加权之久的和\n",
    "        strength = np.dot(self.weights, inputs)\n",
    "        print 'strength', strength\n",
    "\n",
    "        # TODO: return 0 or 1 based on the threshold\n",
    "        # 小于阈值（Threshold）为 0，大于阈值（Threshold）为 1\n",
    "        if strength <= self.threshold :\n",
    "            self.result = 0 # TODO\n",
    "        else:\n",
    "            self.result = 1 # TODO    \n",
    "        return self.result\n",
    "\n",
    "\n",
    "def test():\n",
    "    \"\"\"\n",
    "    A few tests to make sure that the perceptron class performs as expected.\n",
    "    Nothing should show up in the output if all the assertions pass.\n",
    "    \"\"\"\n",
    "    p1 = Perceptron(np.array([1, 2]), 0.)\n",
    "    assert p1.activate(np.array([ 1,-1])) == 0 # < threshold --> 0\n",
    "    assert p1.activate(np.array([-1, 1])) == 1 # > threshold --> 1\n",
    "    assert p1.activate(np.array([ 2,-1])) == 0 # on threshold --> 0\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 感知更新规则"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction value： 0\n",
      "weight update： [ 0.2  0.  -0.3]\n",
      "final weight value： [ 1.2  1.   0.7]\n",
      "prediction value： 1\n",
      "weight update： [-0.3 -0.2 -0.1]\n",
      "prediction value： 0\n",
      "weight update： [ 0.  0. -0.]\n",
      "final weight value： [ 0.7  1.8  2.9]\n",
      "prediction value： 1\n",
      "weight update： [-0.2  0.2 -0.4]\n",
      "prediction value： 0\n",
      "weight update： [-0.1 -0.3  0.2]\n",
      "prediction value： 1\n",
      "weight update： [-0.  -0.2 -0.1]\n",
      "final weight value： [ 2.7 -0.3  1.7]\n"
     ]
    }
   ],
   "source": [
    "# ----------\n",
    "#\n",
    "# In this exercise, you will update the perceptron class so that it can update\n",
    "# its weights.\n",
    "#\n",
    "# Finish writing the update() method so that it updates the weights according\n",
    "# to the perceptron update rule. Updates should be performed online, revising\n",
    "# the weights after each data point.\n",
    "#\n",
    "# YOUR CODE WILL GO IN LINES 51 AND 59.\n",
    "# ----------\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class Perceptron:\n",
    "    \"\"\"\n",
    "    This class models an artificial neuron with step activation function.\n",
    "    \"\"\"\n",
    "    def __init__(self, weights = np.array([1]), threshold = 0):\n",
    "        \"\"\"\n",
    "        Initialize weights and threshold based on input arguments. Note that no\n",
    "        type-checking is being performed here for simplicity.\n",
    "        \"\"\"\n",
    "        self.weights = weights.astype(float) \n",
    "        self.threshold = threshold\n",
    "\n",
    "\n",
    "    def activate(self, values):\n",
    "        \"\"\"\n",
    "        Takes in @param values, a list of numbers equal to length of weights.\n",
    "        @return the output of a threshold perceptron with given inputs based on\n",
    "        perceptron weights and threshold.\n",
    "        \"\"\"\n",
    "        # First calculate the strength with which the perceptron fires\n",
    "        strength = np.dot(values,self.weights)\n",
    "        # Then return 0 or 1 depending on strength compared to threshold  \n",
    "        # 小于阈值（Threshold）为 0，大于阈值（Threshold）为 1\n",
    "        return int(strength > self.threshold)\n",
    "\n",
    "\n",
    "    def update(self, values, train, eta=.1):\n",
    "        \"\"\"\n",
    "        Takes in a 2D array @param values consisting of a LIST of inputs and a\n",
    "        1D array @param train, consisting of a corresponding list of expected\n",
    "        outputs. Updates internal weights according to the perceptron training\n",
    "        rule using these values and an optional learning rate, @param eta.\n",
    "        \"\"\"\n",
    "\n",
    "        # For each data point:\n",
    "        for data_point in xrange(len(values)):\n",
    "            # TODO: Obtain the neuron's prediction for the data_point --> values[data_point]\n",
    "            prediction = self.activate(values[data_point])# TODO\n",
    "            print 'prediction value：',prediction\n",
    "            # Get the prediction accuracy calculated as (expected value - predicted value)\n",
    "            # expected value = train[data_point], predicted value = prediction\n",
    "            error = train[data_point] - prediction\n",
    "            # TODO: update self.weights based on the multiplication of:\n",
    "            # - prediction accuracy(error)\n",
    "            # - learning rate(eta)\n",
    "            # - input value(values[data_point])\n",
    "            weight_update = error * eta * values[data_point]# TODO\n",
    "            print 'weight update：', weight_update\n",
    "            self.weights += weight_update\n",
    "        print 'final weight value：', self.weights\n",
    "\n",
    "def test():\n",
    "    \"\"\"\n",
    "    A few tests to make sure that the perceptron class performs as expected.\n",
    "    Nothing should show up in the output if all the assertions pass.\n",
    "    \"\"\"\n",
    "    def sum_almost_equal(array1, array2, tol = 1e-6):\n",
    "        return sum(abs(array1 - array2)) < tol\n",
    "\n",
    "    p1 = Perceptron(np.array([1,1,1]),0)\n",
    "    p1.update(np.array([[2,0,-3]]), np.array([1]))\n",
    "    assert sum_almost_equal(p1.weights, np.array([1.2, 1, 0.7]))\n",
    "\n",
    "    p2 = Perceptron(np.array([1,2,3]),0)\n",
    "    p2.update(np.array([[3,2,1],[4,0,-1]]),np.array([0,0]))\n",
    "    assert sum_almost_equal(p2.weights, np.array([0.7, 1.8, 2.9]))\n",
    "\n",
    "    p3 = Perceptron(np.array([3,0,2]),0)\n",
    "    p3.update(np.array([[2,-2,4],[-1,-3,2],[0,2,1]]),np.array([0,1,0]))\n",
    "    assert sum_almost_equal(p3.weights, np.array([2.7, -0.3, 1.7]))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-12\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(np.dot([1,2,3],[1,1,-5]))\n",
    "print(np.dot([1,2,3],[3,-4,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-25\n"
     ]
    }
   ],
   "source": [
    "print(np.dot([-12,1],[2,-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建 XOR 网络\n",
    "关于 XOR 的资料可以参考：\n",
    "- [WHY  DO  NEURONS  MAKE  NETWORKS](http://toritris.weebly.com/perceptron-5-xor-how--why-neurons-work-together.html)\n",
    "\n",
    "A xor B = (AvB)^¬(A^B)\n",
    "\n",
    "在画直线进行切分时，斜率以及截距可以按照如下方式计算：![img](http://wx4.sinaimg.cn/large/69d4185bly1fnqcizkr2kj208y07mdfv.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 XOR 0 = 0?: 0\n",
      "0 XOR 1 = 1?: 1\n",
      "1 XOR 0 = 1?: 1\n",
      "1 XOR 1 = 0?: 0\n"
     ]
    }
   ],
   "source": [
    "# ----------\n",
    "#\n",
    "# In this exercise, you will create a network of perceptrons that can represent\n",
    "# the XOR function, using a network structure like those shown in the previous\n",
    "# quizzes.\n",
    "#\n",
    "# You will need to do two things:\n",
    "# First, create a network of perceptrons with the correct weights\n",
    "# Second, define a procedure EvalNetwork() which takes in a list of inputs and\n",
    "# outputs the value of this network.\n",
    "#\n",
    "# ----------\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class Perceptron:\n",
    "    \"\"\"\n",
    "    This class models an artificial neuron with step activation function.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, weights = np.array([1]), threshold = 0):\n",
    "        \"\"\"\n",
    "        Initialize weights and threshold based on input arguments. Note that no\n",
    "        type-checking is being performed here for simplicity.\n",
    "        \"\"\"\n",
    "        self.weights = weights\n",
    "        self.threshold = threshold\n",
    "\n",
    "\n",
    "    def activate(self, values):\n",
    "        \"\"\"\n",
    "        Takes in @param values, a list of numbers equal to length of weights.\n",
    "        @return the output of a threshold perceptron with given inputs based on\n",
    "        perceptron weights and threshold.\n",
    "        \"\"\"\n",
    "               \n",
    "        # First calculate the strength with which the perceptron fires\n",
    "        strength = np.dot(values,self.weights)\n",
    "        \n",
    "        # Then return 0 or 1 depending on strength compared to threshold  \n",
    "        return int(strength > self.threshold)\n",
    "\n",
    "            \n",
    "# Part 1: Set up the perceptron network\n",
    "Network = [\n",
    "    # input layer, declare input layer perceptrons here\n",
    "    [ Perceptron([0.6, 0.6], 1) , Perceptron([1.1, 1.1], 1) ], \\\n",
    "    # output node, declare output layer perceptron here\n",
    "    [ Perceptron([-2, 1.1], 1) ]\n",
    "]\n",
    "\n",
    "# Part 2: Define a procedure to compute the output of the network, given inputs\n",
    "def EvalNetwork(inputValues, Network):\n",
    "    \"\"\"\n",
    "    Takes in @param inputValues, a list of input values, and @param Network\n",
    "    that specifies a perceptron network. @return the output of the Network for\n",
    "    the given set of inputs.\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    # 计算第一层\n",
    "    first_layer = Network[0]\n",
    "    first_per1 = first_layer[0]\n",
    "    first_per2 = first_layer[1]\n",
    "    first_layer_value1 = (1 if np.dot(inputValues, first_per1.weights) > first_per1.threshold else 0)\n",
    "    first_layer_value2 = (1 if np.dot(inputValues, first_per2.weights) > first_per2.threshold else 0)\n",
    "    \n",
    "    # 用于存储第二层输入的值（从第一层计算得来）\n",
    "    second_input = [first_layer_value1, first_layer_value2]\n",
    "    \n",
    "    # 计算第二层\n",
    "    second_layer = Network[1]\n",
    "    second_per = second_layer[0]\n",
    "    \n",
    "    OutputValue = (1 if np.dot(second_input, second_per.weights) > second_per.threshold else 0)\n",
    "    \n",
    "    # Be sure your output value is a single number\n",
    "    return OutputValue\n",
    "\n",
    "\n",
    "def test():\n",
    "    \"\"\"\n",
    "    A few tests to make sure that the perceptron class performs as expected.\n",
    "    \"\"\"\n",
    "    print \"0 XOR 0 = 0?:\", EvalNetwork(np.array([0,0]), Network)\n",
    "    print \"0 XOR 1 = 1?:\", EvalNetwork(np.array([0,1]), Network)\n",
    "    print \"1 XOR 0 = 1?:\", EvalNetwork(np.array([1,0]), Network)\n",
    "    print \"1 XOR 1 = 0?:\", EvalNetwork(np.array([1,1]), Network)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 激活函数沙盒"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# ----------\n",
    "# \n",
    "# Python Neural Networks code originally by Szabo Roland and used with\n",
    "# permission\n",
    "#\n",
    "# Modifications, comments, and exercise breakdowns by Mitchell Owen,\n",
    "# (c) Udacity\n",
    "#\n",
    "# Retrieved originally from http://rolisz.ro/2013/04/18/neural-networks-in-python/\n",
    "#\n",
    "#\n",
    "# Neural Network Sandbox\n",
    "#\n",
    "# Define an activation function activate(), which takes in a number and\n",
    "# returns a number.\n",
    "# Using test run you can see the performance of a neural network running with\n",
    "# that activation function, where the inputs are 8x8 images of digits (0-9) and\n",
    "# the outputs are digit predictions made by the network.\n",
    "#\n",
    "# ----------\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def activate(strength):\n",
    "    # Try out different functions here. Input strength will be a number, with\n",
    "    # another number as output.\n",
    "    return np.power(strength,2)\n",
    "    np.sign()\n",
    "def activation_derivative(activate, strength):\n",
    "    #numerically approximate\n",
    "    return (activate(strength+1e-5)-activate(strength-1e-5))/(2e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "运行代码可得到如下结果：\n",
    "```\n",
    "Random snapshot from epoch 201:\n",
    "Our weights [[ -9.15412386e+16  -3.17340236e+09  -6.52835751e+10  -8.44266623e+10\n",
    "   -9.87309840e+07  -7.23846086e+08  -2.22877650e+09  -4.51944048e+05\n",
    "   -1.46888966e+06  -2.41556726e+08]\n",
    " [ -4.22017886e+16  -1.46298277e+09  -3.00966392e+10  -3.89218696e+10\n",
    "   -4.55163616e+07  -3.33703148e+08  -1.02749707e+09  -2.08352617e+05\n",
    "   -6.77178688e+05  -1.11361022e+08]\n",
    " [ -1.73857613e+14  -6.02701206e+06  -1.23988343e+08  -1.60345416e+08\n",
    "   -1.87512511e+05  -1.37474828e+06  -4.23295317e+06  -8.58415825e+02\n",
    "   -2.78982301e+03  -4.58771183e+05]\n",
    " [ -3.19174104e+15  -1.10646073e+08  -2.27622292e+09  -2.94367923e+09\n",
    "   -3.44242349e+06  -2.52381252e+07  -7.77100847e+07  -1.57576991e+04\n",
    "   -5.12154032e+04  -8.42228633e+06]\n",
    " [ -2.79121062e+14  -9.67611388e+06  -1.99058054e+08  -2.57427800e+08\n",
    "   -3.01043350e+05  -2.20710009e+06  -6.79582737e+06  -1.37803435e+03\n",
    "   -4.47892975e+03  -7.36537457e+05]\n",
    " [ -1.02696000e+14  -3.56009745e+06  -7.32387082e+07  -9.47144770e+07\n",
    "   -1.10761869e+05  -8.12050123e+05  -2.50036413e+06  -5.07175025e+02\n",
    "   -1.64796543e+03  -2.70991825e+05]\n",
    " [ -1.09952001e+18  -3.81163665e+10  -7.84133996e+11  -1.01406542e+12\n",
    "   -1.18587747e+09  -8.69425921e+09  -2.67702775e+10  -5.42839053e+06\n",
    "   -1.76431264e+07  -2.90138585e+09]\n",
    " [ -6.63976933e+14  -2.30176692e+07  -4.73521973e+08  -6.12372709e+08\n",
    "   -7.16126474e+05  -5.25027961e+06  -1.61660054e+07  -3.27807808e+03\n",
    "   -1.06541895e+04  -1.75208567e+06]\n",
    " [ -1.88069903e+18  -6.51970066e+10  -1.34123984e+12  -1.73453128e+12\n",
    "   -2.02841111e+09  -1.48712936e+10  -4.57898305e+10  -9.28511459e+06\n",
    "   -3.01780871e+07  -4.96274147e+09]\n",
    " [ -1.47496930e+15  -5.11318302e+07  -1.05188951e+09  -1.36033483e+09\n",
    "   -1.59081495e+06  -1.16630579e+07  -3.59114316e+07  -7.28177579e+03\n",
    "   -2.36676408e+04  -3.89211187e+06]\n",
    " [ -7.79936521e+15  -2.70375673e+08  -5.56219745e+09  -7.19319929e+09\n",
    "   -8.41193539e+06  -6.16720950e+07  -1.89893016e+08  -3.85056572e+04\n",
    "   -1.25150242e+05  -2.05807696e+07]\n",
    " [ -4.09342625e+18  -1.41904225e+11  -2.91926900e+12  -3.77528555e+12\n",
    "   -4.41492825e+09  -3.23680410e+10  -9.96636310e+10  -2.02094702e+07\n",
    "   -6.56839679e+07  -1.08016306e+10]\n",
    " [ -1.68293077e+18  -5.83410991e+10  -1.20019937e+12  -1.55213355e+12\n",
    "   -1.81510992e+09  -1.33074762e+10  -4.09747192e+10  -8.30872159e+06\n",
    "   -2.70046566e+07  -4.44087554e+09]\n",
    " [ -9.82386268e+16  -3.40557649e+09  -7.00598863e+10  -9.06035301e+10\n",
    "   -1.05954392e+08  -7.76804494e+08  -2.39183942e+09  -4.85009743e+05\n",
    "   -1.57635722e+06  -2.59229626e+08]\n",
    " [ -3.05815878e+15  -1.06015261e+08  -2.18095736e+09  -2.82047897e+09\n",
    "   -3.29834962e+06  -2.41818475e+07  -7.44577254e+07  -1.50982674e+04\n",
    "   -4.90719651e+04  -8.06979294e+06]\n",
    " [ -1.08049513e+15  -3.74568428e+07  -7.70566207e+08  -9.96519153e+08\n",
    "   -1.16535842e+06  -8.54382317e+06  -2.63070743e+07  -5.33423413e+03\n",
    "   -1.73378516e+04  -2.85118335e+06]]\n",
    "\n",
    " are being modified with deltas [[-0. -0. -0. -0. -0. -0. -0. -0. -0. -0.]]\n",
    "\n",
    " using the results matrix [[  9.73359541e+37   3.79693929e+36   4.15680126e+30   1.23646194e+32\n",
    "    4.00387542e+31   2.72682070e+29   7.20902374e+39   1.26295109e+31\n",
    "    6.53768438e+40   2.46017869e+31   1.21468016e+34   3.87694376e+41\n",
    "    3.19507500e+40   1.82431892e+37   3.73760357e+33   7.76605073e+31]]\n",
    "\n",
    "Confusion matrix: rows indicate true labels, columns indicate predictions.\n",
    "[[54  0  0  0  0  0  0  0  0  0]\n",
    " [41  0  0  0  0  0  0  0  0  0]\n",
    " [44  0  0  0  0  0  0  0  0  0]\n",
    " [43  0  0  0  0  0  0  0  0  0]\n",
    " [44  0  0  0  0  0  0  0  0  0]\n",
    " [43  0  0  0  0  0  0  0  0  0]\n",
    " [47  0  0  0  0  0  0  0  0  0]\n",
    " [42  0  0  0  0  0  0  0  0  0]\n",
    " [45  0  0  0  0  0  0  0  0  0]\n",
    " [47  0  0  0  0  0  0  0  0  0]]\n",
    "\n",
    "Classification report for above confusion matrix:\n",
    "             precision    recall  f1-score   support\n",
    "\n",
    "          0       0.12      1.00      0.21        54\n",
    "          1       0.00      0.00      0.00        41\n",
    "          2       0.00      0.00      0.00        44\n",
    "          3       0.00      0.00      0.00        43\n",
    "          4       0.00      0.00      0.00        44\n",
    "          5       0.00      0.00      0.00        43\n",
    "          6       0.00      0.00      0.00        47\n",
    "          7       0.00      0.00      0.00        42\n",
    "          8       0.00      0.00      0.00        45\n",
    "          9       0.00      0.00      0.00        47\n",
    "\n",
    "avg / total       0.01      0.12      0.03       450\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid 编程练习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]] [0]\n",
      "[[-3 -1  2]\n",
      " [ 2  1  2]] [1 0]\n"
     ]
    }
   ],
   "source": [
    "# ----------\n",
    "# \n",
    "# As with the previous perceptron exercises, you will complete some of the core\n",
    "# methods of a sigmoid unit class.\n",
    "#\n",
    "# There are two functions for you to finish:\n",
    "# First, in activate(), write the sigmoid activation function.\n",
    "# Second, in update(), write the gradient descent update rule. Updates should be\n",
    "#   performed online, revising the weights after each data point.\n",
    "# \n",
    "# ----------\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Sigmoid:\n",
    "    \"\"\"\n",
    "    This class models an artificial neuron with sigmoid activation function.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, weights = np.array([1])):\n",
    "        \"\"\"\n",
    "        Initialize weights based on input arguments. Note that no type-checking\n",
    "        is being performed here for simplicity of code.\n",
    "        \"\"\"\n",
    "        self.weights = weights\n",
    "\n",
    "        # NOTE: You do not need to worry about these two attribues for this\n",
    "        # programming quiz, but these will be useful for if you want to create\n",
    "        # a network out of these sigmoid units!\n",
    "        self.last_input = 0 # strength of last input\n",
    "        self.delta      = 0 # error signal\n",
    "\n",
    "    def activate(self, values):\n",
    "        \"\"\"\n",
    "        Takes in @param values, a list of numbers equal to length of weights.\n",
    "        @return the output of a sigmoid unit with given inputs based on unit\n",
    "        weights.\n",
    "        \"\"\"\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        # First calculate the strength of the input signal.\n",
    "        strength = np.dot(values, self.weights)\n",
    "        self.last_input = strength\n",
    "        \n",
    "        # TODO: Modify strength using the sigmoid activation function and\n",
    "        # return as output signal.\n",
    "        # HINT: You may want to create a helper function to compute the\n",
    "        #   logistic function since you will need it for the update function.\n",
    "        result = 1.0/(1 + np.exp(-strength))\n",
    "#         print values, self.weights, strength, result\n",
    "        return result\n",
    "    \n",
    "    def logistic(self, strength):\n",
    "        return 1.0/(1 + np.exp(-strength))\n",
    "        \n",
    "    def update(self, values, train, eta=.1):\n",
    "        \"\"\"\n",
    "        Takes in a 2D array @param values consisting of a LIST of inputs and a\n",
    "        1D array @param train, consisting of a corresponding list of expected\n",
    "        outputs. Updates internal weights according to gradient descent using\n",
    "        these values and an optional learning rate, @param eta.\n",
    "        \"\"\"\n",
    "        print values, train\n",
    "        # TODO: for each data point...\n",
    "        for X, y_true in zip(values, train):\n",
    "            # obtain the output signal for that point\n",
    "            y_pred = self.activate(X)\n",
    "\n",
    "            # YOUR CODE HERE\n",
    "\n",
    "            # TODO: compute derivative of logistic function at input strength\n",
    "            # Recall: d/dx logistic(x) = logistic(x)*(1-logistic(x))\n",
    "            dx = self.logistic(self.last_input) * (1 - self.logistic(self.last_input))\n",
    "            # TODO: update self.weights based on learning rate, signal accuracy,\n",
    "            # function slope (derivative) and input value\n",
    "            self.weights = self.weights + eta * (y_true - y_pred) * dx * X\n",
    "\n",
    "def test():\n",
    "    \"\"\"\n",
    "    A few tests to make sure that the perceptron class performs as expected.\n",
    "    Nothing should show up in the output if all the assertions pass.\n",
    "    \"\"\"\n",
    "    def sum_almost_equal(array1, array2, tol = 1e-5):\n",
    "        return sum(abs(array1 - array2)) < tol\n",
    "\n",
    "    u1 = Sigmoid(weights=[3,-2,1])\n",
    "    assert abs(u1.activate(np.array([1,2,3])) - 0.880797) < 1e-5\n",
    "    \n",
    "    u1.update(np.array([[1,2,3]]),np.array([0]))\n",
    "    assert sum_almost_equal(u1.weights, np.array([2.990752, -2.018496, 0.972257]))\n",
    "\n",
    "    u2 = Sigmoid(weights=[0,3,-1])\n",
    "    u2.update(np.array([[-3,-1,2],[2,1,2]]),np.array([1,0]))\n",
    "    assert sum_almost_equal(u2.weights, np.array([-0.030739, 2.984961, -1.027437]))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 神经网络模式识别"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如下图：![img](https://ws1.sinaimg.cn/large/69d4185bly1fnqdn6khlkj20bx09ht8w.jpg)\n",
    "\n",
    "每一层都在识别前一层所拥有的模式。\n",
    "- The red spots are the low-level patterns (edges, diagonal lines, curves, etc) that are detected in the pattern of activation of the retina.  \n",
    "- The green spots are the cells that pick up certain patterns of red spots.  For example; corner + curve + corner = wheel arch.\n",
    "- The blue spots are the cells that pick up certain patterns in the green spots.  For example; wheel arch + door + bonnet + ....  = car.\n",
    "\n",
    "如下图的细分图：![img](https://ws1.sinaimg.cn/large/69d4185bly1fnqdovgh8wj207u098dfu.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "复杂的神经网络也是类似的处理，如下四层神经网络进行模式识别：![img](https://ws1.sinaimg.cn/large/69d4185bly1fnqeqczvquj20l40ekdh0.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基于实例的学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 几个概念\n",
    "\n",
    "- 欧氏距离\n",
    "- 曼哈顿距离\n",
    "\n",
    "\n",
    "- 饥饿式学习\n",
    "- 懒惰式学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 集成 B&B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 朴素贝叶斯项目2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 最大可能性假设"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'need': 4}\n",
      "{'and': 6}\n",
      "{'be': 1, 'just': 1}\n",
      "{'terrific,': 1, 'here': 1, 'great,': 1}\n"
     ]
    }
   ],
   "source": [
    "sample_memo = '''\n",
    "Milt, we're gonna need to go ahead and move you downstairs into storage B. We have some new people coming in, \n",
    "and we need all the space we can get. So if you could just go ahead and pack up your stuff and move it down there, that would be terrific, OK?\n",
    "Oh, and remember: next Friday... is Hawaiian shirt day. So, you know, if you want to, go ahead and wear a Hawaiian shirt and jeans.\n",
    "Oh, oh, and I almost forgot. Ahh, I'm also gonna need you to go ahead and come in on Sunday, too...\n",
    "Hello Peter, whats happening? Ummm, I'm gonna need you to go ahead and come in tomorrow. \n",
    "So if you could be here around 9 that would be great, mmmk... oh oh! and I almost forgot ahh, \n",
    "I'm also gonna need you to go ahead and come in on Sunday too, kay. We ahh lost some people this week and ah, \n",
    "we sorta need to play catch up.\n",
    "'''\n",
    "\n",
    "#\n",
    "#   Maximum Likelihood Hypothesis\n",
    "#\n",
    "#\n",
    "#   In this quiz we will find the maximum likelihood word based on the preceding word\n",
    "#\n",
    "#   Fill in the NextWordProbability procedure so that it takes in sample text and a word,\n",
    "#   and returns a dictionary with keys the set of words that come after, whose values are\n",
    "#   the number of times the key comes after that word.\n",
    "#   \n",
    "#   Just use .split() to split the sample_memo text into words separated by spaces.\n",
    "\n",
    "def NextWordProbability(sampletext,word):\n",
    "    sample = sampletext.split(\" \")\n",
    "    result = {}\n",
    "    \n",
    "    for index, item in enumerate(sample):\n",
    "        if item == word:\n",
    "            if sample[index + 1] not in result.keys():\n",
    "                result[sample[index + 1]] = 1\n",
    "            else:\n",
    "                result[sample[index + 1]] += 1\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(NextWordProbability(sample_memo,\"gonna\"))\n",
    "print(NextWordProbability(sample_memo,\"ahead\"))\n",
    "print(NextWordProbability(sample_memo,\"could\"))\n",
    "print(NextWordProbability(sample_memo,\"be\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {
    "height": "769px",
    "left": "0px",
    "right": "1110px",
    "top": "111px",
    "width": "406px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
